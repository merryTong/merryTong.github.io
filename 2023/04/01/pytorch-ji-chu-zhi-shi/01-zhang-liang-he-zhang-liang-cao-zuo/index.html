<!DOCTYPE HTML>
<html lang="zh-CN">


<head><meta name="generator" content="Hexo 3.9.0">
    <meta charset="utf-8">
    <meta name="keywords" content="Pytorch|张量和张量操作, AI, 算法, 深度学习, 机器学习，编程">
    <meta name="description" content=" 张量
张量是基于向量和矩阵的推广，其本质就是多维数组。
 创建张量
 常规方式创建
常见的初始化有torch.tensor和torch.Tensor，其中的区别是

tensor()：接收现有数据，通过numpy 或 list 的现有数据">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="referrer" content="no-referrer-when-downgrade">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>Pytorch|张量和张量操作 | Merry的博客</title>
    <link rel="icon" type="image/png" href="/medias/logo.png">
    
    <style>
        body{
            background-image: url(https://cdn.jsdelivr.net/gh/Tokisaki-Galaxy/res/site/medias/background.jpg);
            background-repeat:no-repeat;
            background-size: 100% 100%;
            background-attachment:fixed;
        }
    </style>



    <!-- bg-cover style     -->



<link rel="stylesheet" type="text/css" href="/libs/awesome/css/all.min.css">
<link rel="stylesheet" type="text/css" href="/libs/materialize/materialize.min.css">
<link rel="stylesheet" type="text/css" href="/libs/aos/aos.css">
<link rel="stylesheet" type="text/css" href="/libs/animate/animate.min.css">
<link rel="stylesheet" type="text/css" href="/libs/lightGallery/css/lightgallery.min.css">
<link rel="stylesheet" type="text/css" href="/css/matery.css">
<link rel="stylesheet" type="text/css" href="/css/my.css">
<link rel="stylesheet" type="text/css" href="/css/dark.css" media="none" onload="if(media!='all')media='all'">




    <link rel="stylesheet" href="/libs/tocbot/tocbot.css">
    <link rel="stylesheet" href="/css/post.css">




    
        <link rel="stylesheet" type="text/css" href="/css/reward.css">
    



    <script src="/libs/jquery/jquery-3.6.0.min.js"></script>

<link rel="stylesheet" href="/css/prism-tomorrow.css" type="text/css">
<link rel="stylesheet" href="/css/prism-line-numbers.css" type="text/css"></head>


<body>
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/" class="waves-effect waves-light">
                    
                    <img src="/medias/logo.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Merry的博客</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/" class="waves-effect waves-light">
      
      <i class="fas fa-home" style="zoom: 0.6;"></i>
      
      <span>首页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/tags" class="waves-effect waves-light">
      
      <i class="fas fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>归档</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/about" class="waves-effect waves-light">
      
      <i class="fas fa-user-circle" style="zoom: 0.6;"></i>
      
      <span>关于</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/contact" class="waves-effect waves-light">
      
      <i class="fas fa-comments" style="zoom: 0.6;"></i>
      
      <span>留言板</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/friends" class="waves-effect waves-light">
      
      <i class="fas fa-address-book" style="zoom: 0.6;"></i>
      
      <span>友情链接</span>
    </a>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
  <li>
    <a href="javascript:;" class="waves-effect waves-light" onclick="switchNightMode()" title="深色/浅色模式" >
      <i id="sum-moon-icon" class="fas fa-sun" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/medias/logo.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Merry的博客</div>
        <div class="logo-desc">
            
            算法工程师 | 人工智能 | 黑客精神
            
        </div>
    </div>

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-home"></i>
			
			首页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			归档
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/about" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-user-circle"></i>
			
			关于
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/contact" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-comments"></i>
			
			留言板
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/friends" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-address-book"></i>
			
			友情链接
		</a>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/merryTong/merryTong.github.io" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/merryTong/merryTong.github.io" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('/medias/featureimages/architect/10.jpg')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">Pytorch|张量和张量操作</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/tags/基础知识/">
                                <span class="chip bg-color">基础知识</span>
                            </a>
                        
                            <a href="/tags/Pytorch/">
                                <span class="chip bg-color">Pytorch</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/categories/Pytorch/" class="post-category">
                                Pytorch
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2023-04-01
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2023-12-09
                </div>
                

                

                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                    <!-- <div class="info-break-policy"> -->
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>

                    </div>
                <!-- </div> -->
				
            </div>
        </div>
        <hr class="clearfix">

        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <h1 id="张量"><a class="markdownIt-Anchor" href="#张量"></a> 张量</h1>
<p>张量是基于向量和矩阵的推广，其本质就是多维数组。</p>
<h2 id="创建张量"><a class="markdownIt-Anchor" href="#创建张量"></a> 创建张量</h2>
<h3 id="常规方式创建"><a class="markdownIt-Anchor" href="#常规方式创建"></a> 常规方式创建</h3>
<p>常见的初始化有<code>torch.tensor</code>和<code>torch.Tensor</code>，其中的区别是</p>
<ul>
<li>tensor()：接收现有数据，通过numpy 或 list 的现有数据初始化</li>
<li>Tensor()：1. 接收数据的维度(, )生成随机数张量；2. 接收现有的数据[, ]生成指定数据张量</li>
</ul>
<pre class="highlight"><code class="python">tensor=torch.tensor([<span class="hljs-number">2</span>,<span class="hljs-number">3</span>,<span class="hljs-number">4</span>])

tensor=torch.Tensor([<span class="hljs-number">2</span>,<span class="hljs-number">3</span>,<span class="hljs-number">4</span>])
</code></pre>
<p>常见的创建方式还有<code>torch.as_tensor()</code>，<code>torch.from_numpy()</code>。这两种方式都是接收现有数据的，而且生成的张量与原有数据是<strong>内存共享</strong>的。</p>
<pre class="highlight"><code class="python">a = np.array([<span class="hljs-number">1</span>,<span class="hljs-number">2</span>,<span class="hljs-number">3</span>])

tensor = torch.from_numpy(a)
tensor = torch.as_tensor(a)
</code></pre>
<p>在不考虑性能方面，一般情况下使用torch.tensor()方法居多，那么如果要考虑性能方面，首先肯定是要从<strong>torch.as_tensor()和torch.from_numpy()两种方法中选择，因为在创建tensor的过程中，它俩是共享内存的，不需要额外创建一份数据</strong>。<br>
两者的区别是torch.from_numpy()只能接收numpy数组，而torch.as_tensor()不仅可以接收numpy数组，还可以接收python的list类型数据。</p>
<h3 id="序列生成"><a class="markdownIt-Anchor" href="#序列生成"></a> 序列生成</h3>
<ul>
<li><code>torch.arange()</code>：接收参数：a,b,step，输出[a,b)范围内step步长的等差序列组成的tensor，数据类型为int。</li>
<li><code>torc.range()</code>：接收参数：a,b,step，输出[a,b]范围内step步长的等差序列组成的tensor，数据类型为float。</li>
</ul>
<p>arrange()方法和range()方法的区别在于：range()方法可以输出结果包含区间右侧b这个数值，且range()方法的数据类型为float。</p>
<pre class="highlight"><code class="python">a = torch.arange(<span class="hljs-number">0</span>,<span class="hljs-number">10</span>,<span class="hljs-number">2</span>)
<span class="hljs-built_in">print</span>(a)
<span class="hljs-built_in">print</span>(a.<span class="hljs-built_in">type</span>())

b = torch.<span class="hljs-built_in">range</span>(<span class="hljs-number">0</span>,<span class="hljs-number">10</span>,<span class="hljs-number">2</span>)
<span class="hljs-built_in">print</span>(b)
<span class="hljs-built_in">print</span>(b.<span class="hljs-built_in">type</span>())

&gt;&gt;&gt;Output:
tensor([<span class="hljs-number">0</span>, <span class="hljs-number">2</span>, <span class="hljs-number">4</span>, <span class="hljs-number">6</span>, <span class="hljs-number">8</span>])
torch.LongTensor

tensor([ <span class="hljs-number">0.</span>,  <span class="hljs-number">2.</span>,  <span class="hljs-number">4.</span>,  <span class="hljs-number">6.</span>,  <span class="hljs-number">8.</span>, <span class="hljs-number">10.</span>])
torch.FloatTensor
</code></pre>
<p>生成一个0到n-1的n-1个整数的随机排列</p>
<pre class="highlight"><code class="python">a = torch.randperm(<span class="hljs-number">10</span>)
<span class="hljs-built_in">print</span>(a)

&gt;&gt;&gt;Output:
tensor([<span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">4</span>, <span class="hljs-number">7</span>, <span class="hljs-number">9</span>, <span class="hljs-number">8</span>, <span class="hljs-number">6</span>, <span class="hljs-number">3</span>, <span class="hljs-number">2</span>, <span class="hljs-number">5</span>])
</code></pre>
<h3 id="使用随机数据创建"><a class="markdownIt-Anchor" href="#使用随机数据创建"></a> 使用随机数据创建</h3>
<ul>
<li><code>torch.rand()</code>：输入参数为一个shape，创建指定形状大小的tensor，数据为float32类型的随机数。产生[0,1]均匀分布的数据。</li>
<li><code>torch.randint()</code>：指定数据范围为[a, b)的随机tensor创建。输入参数为一个a,b,(x,y,…)，创建(x,y,…)大小的tensor，数据的范围为[a,b)，数据类型为整数值。</li>
<li><code>torch.rand_like()</code>：输入参数为一个<strong>浮点型</strong>的tensor，创建一个与输入tensor数据同大小的矩阵，数据为<strong>与原始tensor相同类型</strong>的<strong>浮点型</strong>随机数。产生[0,1]均匀分布的数据。</li>
</ul>
<pre class="highlight"><code class="python"><span class="hljs-comment"># 等价于 a = torch.rand(2,3)</span>
a = torch.rand([<span class="hljs-number">2</span>,<span class="hljs-number">3</span>])
<span class="hljs-built_in">print</span>(a)

c = torch.rand_like(a)
<span class="hljs-built_in">print</span>(c)
<span class="hljs-built_in">print</span>(c.<span class="hljs-built_in">type</span>())

b = torch.randint(<span class="hljs-number">1</span>,<span class="hljs-number">10</span>,(<span class="hljs-number">2</span>,<span class="hljs-number">3</span>))
<span class="hljs-built_in">print</span>(b)
<span class="hljs-built_in">print</span>(b.<span class="hljs-built_in">type</span>())

&gt;&gt;&gt;Output:
tensor([[<span class="hljs-number">0.8703</span>, <span class="hljs-number">0.8061</span>, <span class="hljs-number">0.5126</span>],
        [<span class="hljs-number">0.6069</span>, <span class="hljs-number">0.5985</span>, <span class="hljs-number">0.4657</span>]])

tensor([[<span class="hljs-number">0.0937</span>, <span class="hljs-number">0.1968</span>, <span class="hljs-number">0.2269</span>],
        [<span class="hljs-number">0.3653</span>, <span class="hljs-number">0.9386</span>, <span class="hljs-number">0.9892</span>]])
torch.FloatTensor

tensor([[<span class="hljs-number">9</span>, <span class="hljs-number">3</span>, <span class="hljs-number">6</span>],
        [<span class="hljs-number">4</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>]])
torch.LongTensor
</code></pre>
<ul>
<li><code>torch.randn()</code>：接受参数为shape，输出一个数据满足标准正态分布<code>N(0,1)</code>的随机数tensor。</li>
<li><code>torch.normal()</code>：接受参数为：mean, std, shape，分别为所创建数据的均值，标准差和形状，输出一个满足上述参数的广义的正态分布tensor。</li>
</ul>
<pre class="highlight"><code class="python">a = torch.randn(<span class="hljs-number">3</span>,<span class="hljs-number">3</span>)

b = torch.normal(mean = torch.zeros(<span class="hljs-number">3</span>,<span class="hljs-number">3</span>), std = torch.ones(<span class="hljs-number">3</span>,<span class="hljs-number">3</span>))

&gt;&gt;&gt;Output:
tensor([[-<span class="hljs-number">0.6225</span>, -<span class="hljs-number">0.1253</span>, -<span class="hljs-number">0.1083</span>],
        [-<span class="hljs-number">0.3199</span>, -<span class="hljs-number">0.5670</span>,  <span class="hljs-number">0.2898</span>],
        [-<span class="hljs-number">0.6500</span>,  <span class="hljs-number">0.9275</span>,  <span class="hljs-number">1.0377</span>]])

tensor([[ <span class="hljs-number">0.5507</span>,  <span class="hljs-number">0.2704</span>,  <span class="hljs-number">0.6472</span>],
        [ <span class="hljs-number">0.2490</span>, -<span class="hljs-number">0.3354</span>,  <span class="hljs-number">0.4564</span>],
        [-<span class="hljs-number">0.6255</span>,  <span class="hljs-number">0.4539</span>, -<span class="hljs-number">1.3740</span>]])
</code></pre>
<p>使用<code>torch.Tensor()</code>创建随机数张量。</p>
<pre class="highlight"><code class="python">a = torch.IntTensor(<span class="hljs-number">3</span>,<span class="hljs-number">4</span>)
<span class="hljs-built_in">print</span>(a)

&gt;&gt;&gt;Output:
tensor([[ <span class="hljs-number">241405024</span>,      <span class="hljs-number">32635</span>, <span class="hljs-number">1567730800</span>],
        [     <span class="hljs-number">22007</span>,         <span class="hljs-number">32</span>,          <span class="hljs-number">0</span>]], dtype=torch.int32)
</code></pre>
<h3 id="特殊矩阵的创建"><a class="markdownIt-Anchor" href="#特殊矩阵的创建"></a> 特殊矩阵的创建</h3>
<p>在数学计算中会经常使用到全0矩阵、全1矩阵、单位矩阵。其创建方法如下：</p>
<ul>
<li>torch.zeros()：接收参数为shape，输出一个shape大小的全0 Tensor。torch.zeros_like(input, dtype)</li>
<li>torch.ones()：接收参数为shape，输出一个shape大小的全1 Tensor。torch.ones_like(input, dtype)</li>
<li>torch.eye()：接收参数为shape，输出一个shape大小的单位矩阵Tensor。</li>
</ul>
<pre class="highlight"><code class="python">a = torch.zeros(<span class="hljs-number">3</span>,<span class="hljs-number">3</span>)
<span class="hljs-built_in">print</span>(a)
<span class="hljs-built_in">print</span>(a.<span class="hljs-built_in">type</span>())

b = torch.ones(<span class="hljs-number">3</span>,<span class="hljs-number">3</span>)
<span class="hljs-built_in">print</span>(b)
<span class="hljs-built_in">print</span>(b.<span class="hljs-built_in">type</span>())

c = torch.eye(<span class="hljs-number">3</span>,<span class="hljs-number">3</span>)
<span class="hljs-built_in">print</span>(c)
<span class="hljs-built_in">print</span>(c.<span class="hljs-built_in">type</span>())

<span class="hljs-built_in">print</span>(torch.eye(<span class="hljs-number">2</span>))

&gt;&gt;&gt;Output:
tensor([[<span class="hljs-number">0.</span>, <span class="hljs-number">0.</span>, <span class="hljs-number">0.</span>],
        [<span class="hljs-number">0.</span>, <span class="hljs-number">0.</span>, <span class="hljs-number">0.</span>],
        [<span class="hljs-number">0.</span>, <span class="hljs-number">0.</span>, <span class="hljs-number">0.</span>]])
torch.FloatTensor

tensor([[<span class="hljs-number">1.</span>, <span class="hljs-number">1.</span>, <span class="hljs-number">1.</span>],
        [<span class="hljs-number">1.</span>, <span class="hljs-number">1.</span>, <span class="hljs-number">1.</span>],
        [<span class="hljs-number">1.</span>, <span class="hljs-number">1.</span>, <span class="hljs-number">1.</span>]])
torch.FloatTensor

tensor([[<span class="hljs-number">1.</span>, <span class="hljs-number">0.</span>, <span class="hljs-number">0.</span>],
        [<span class="hljs-number">0.</span>, <span class="hljs-number">1.</span>, <span class="hljs-number">0.</span>],
        [<span class="hljs-number">0.</span>, <span class="hljs-number">0.</span>, <span class="hljs-number">1.</span>]])
torch.FloatTensor

tensor([[<span class="hljs-number">1.</span>, <span class="hljs-number">0.</span>],
        [<span class="hljs-number">0.</span>, <span class="hljs-number">1.</span>]])
</code></pre>
<p><code>torch.full()</code>：接收参数为shape, x，输出一个shape大小的元素全为x的tensor。</p>
<pre class="highlight"><code class="python">a = torch.full((<span class="hljs-number">2</span>,<span class="hljs-number">3</span>),<span class="hljs-number">1</span>)
<span class="hljs-built_in">print</span>(a)
<span class="hljs-built_in">print</span>(a.<span class="hljs-built_in">type</span>())

a = torch.ones((<span class="hljs-number">3</span>,<span class="hljs-number">3</span>), dtype = torch.<span class="hljs-built_in">int</span>)
torch.fill_(a,<span class="hljs-number">5</span>) 
<span class="hljs-built_in">print</span>(a)

&gt;&gt;&gt;Output:
tensor([[<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>],
        [<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>]])
torch.LongTensor

tensor([[<span class="hljs-number">5.</span>, <span class="hljs-number">5.</span>, <span class="hljs-number">5.</span>],
        [<span class="hljs-number">5.</span>, <span class="hljs-number">5.</span>, <span class="hljs-number">5.</span>],
        [<span class="hljs-number">5.</span>, <span class="hljs-number">5.</span>, <span class="hljs-number">5.</span>]])
</code></pre>
<p><code>torch.empty()</code>接收参数为shape，表示创建一个<strong>未初始化</strong>的张量。torch.empty_like(input, dtype)</p>
<pre class="highlight"><code class="python">a = torch.empty(<span class="hljs-number">2</span>,<span class="hljs-number">3</span>)
<span class="hljs-built_in">print</span>(a)

<span class="hljs-comment"># 等价于 a = torch.empty([2,3])</span>

&gt;&gt;&gt;Output:
tensor([[<span class="hljs-number">8.8319e+17</span>, <span class="hljs-number">3.0838e-41</span>, <span class="hljs-number">9.5811e+17</span>],
        [<span class="hljs-number">3.0838e-41</span>, <span class="hljs-number">9.1995e-41</span>, <span class="hljs-number">4.5989e-40</span>]])
</code></pre>
<h2 id="张量的数据类型"><a class="markdownIt-Anchor" href="#张量的数据类型"></a> 张量的数据类型</h2>
<p>Pytorch的基本数据结构是张量Tensor。张量即多维数组。Pytorch的张量和numpy中的array很类似。</p>
<table>
<thead>
<tr>
<th>Data type</th>
<th>dtype</th>
<th>CPU tensor</th>
<th>GPU tensor</th>
</tr>
</thead>
<tbody>
<tr>
<td>16-bit floating point</td>
<td>torch.float16 or torch.half</td>
<td>torch.HalfTensor</td>
<td>torch.cuda.HalfTensor</td>
</tr>
<tr>
<td>32-bit floating point</td>
<td>torch.float32 or torch.float</td>
<td>torch.FloatTensor</td>
<td>torch.cuda.FloatTensor</td>
</tr>
<tr>
<td>64-bit floating point</td>
<td>torch.float64 or torch.double</td>
<td>torch.DoubleTensor</td>
<td>torch.cuda.DoubleTensor</td>
</tr>
<tr>
<td>8-bit integer (signed)</td>
<td>torch.int8</td>
<td>torch.CharTensor</td>
<td>torch.cuda.CharTensor</td>
</tr>
<tr>
<td>16-bit integer (signed)</td>
<td>torch.int16 or torch.short</td>
<td>torch.ShortTensor</td>
<td>torch.cuda.ShortTensor</td>
</tr>
<tr>
<td>32-bit integer (signed)</td>
<td>torch.int32 or torch.int</td>
<td>torch.IntTensor</td>
<td>torch.cuda.IntTensor</td>
</tr>
<tr>
<td>64-bit integer (signed)</td>
<td>torch.int64 or torch.long</td>
<td>torch.LongTensor</td>
<td>torch.cuda.LongTensor</td>
</tr>
<tr>
<td>8-bit integer (unsigned)</td>
<td>torch.uint8</td>
<td>torch.ByteTensor</td>
<td>torch.cuda.ByteTensor</td>
</tr>
</tbody>
</table>
<p>张量的数据类型和numpy.array基本一一对应，但是不支持str类型。包括:</p>
<ul>
<li>torch.float64(torch.double),</li>
<li><strong>torch.float32(torch.float)</strong>,</li>
<li>torch.float16,</li>
<li>torch.int64(torch.long),</li>
<li>torch.int32(torch.int),</li>
<li>torch.int16,</li>
<li>torch.int8,</li>
<li>torch.uint8,</li>
<li>torch.bool</li>
</ul>
<p>一般神经网络建模使用的都是torch.float32类型。</p>
<pre class="highlight"><code class="python"><span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np
<span class="hljs-keyword">import</span> torch 

<span class="hljs-comment"># 自动推断数据类型</span>
i = torch.tensor(<span class="hljs-number">1</span>);<span class="hljs-built_in">print</span>(i, i.dtype)
x = torch.tensor(<span class="hljs-number">2.0</span>);<span class="hljs-built_in">print</span>(x, x.dtype)
b = torch.tensor(<span class="hljs-literal">True</span>);<span class="hljs-built_in">print</span>(b, b.dtype)

&gt;&gt;&gt;Output:
tensor(<span class="hljs-number">1</span>) torch.int64
tensor(<span class="hljs-number">2.</span>) torch.float32
tensor(<span class="hljs-literal">True</span>) torch.<span class="hljs-built_in">bool</span>
</code></pre>
<pre class="highlight"><code class="python"><span class="hljs-comment"># 指定数据类型</span>
i = torch.tensor(<span class="hljs-number">1</span>, dtype = torch.int32);<span class="hljs-built_in">print</span>(i,i.dtype)
x = torch.tensor(<span class="hljs-number">2.0</span>, dtype = torch.double);<span class="hljs-built_in">print</span>(x,x.dtype)

&gt;&gt;&gt;Output:
tensor(<span class="hljs-number">1</span>, dtype=torch.int32) torch.int32
tensor(<span class="hljs-number">2.</span>, dtype=torch.float64) torch.float64
</code></pre>
<pre class="highlight"><code class="python"><span class="hljs-comment"># 使用特定类型构造函数</span>
i = torch.IntTensor(<span class="hljs-number">1</span>);<span class="hljs-built_in">print</span>(i,i.dtype)
<span class="hljs-comment"># 等价于torch.FloatTensor</span>
x = torch.Tensor(np.array(<span class="hljs-number">2.0</span>));<span class="hljs-built_in">print</span>(x,x.dtype) 
b = torch.BoolTensor(np.array([<span class="hljs-number">1</span>,<span class="hljs-number">0</span>,<span class="hljs-number">2</span>,<span class="hljs-number">0</span>])); <span class="hljs-built_in">print</span>(b,b.dtype)

&gt;&gt;&gt;Output:
tensor([<span class="hljs-number">5</span>], dtype=torch.int32) torch.int32
tensor(<span class="hljs-number">2.</span>) torch.float32
tensor([ <span class="hljs-literal">True</span>, <span class="hljs-literal">False</span>,  <span class="hljs-literal">True</span>, <span class="hljs-literal">False</span>]) torch.<span class="hljs-built_in">bool</span>
</code></pre>
<pre class="highlight"><code class="python"><span class="hljs-comment"># 不同类型进行转换</span>
i = torch.tensor(<span class="hljs-number">1</span>); <span class="hljs-built_in">print</span>(i,i.dtype)

<span class="hljs-comment"># 调用 float方法转换成浮点类型</span>
x = i.<span class="hljs-built_in">float</span>(); <span class="hljs-built_in">print</span>(x,x.dtype) 
<span class="hljs-comment"># 使用type函数转换成浮点类型</span>
y = i.<span class="hljs-built_in">type</span>(torch.<span class="hljs-built_in">float</span>); <span class="hljs-built_in">print</span>(y,y.dtype) 
<span class="hljs-comment"># 使用type_as方法转换成某个Tensor相同类型</span>
z = i.type_as(x);<span class="hljs-built_in">print</span>(z,z.dtype) 

&gt;&gt;&gt;Output:
tensor(<span class="hljs-number">1</span>) torch.int64
tensor(<span class="hljs-number">1.</span>) torch.float32
tensor(<span class="hljs-number">1.</span>) torch.float32
tensor(<span class="hljs-number">1.</span>) torch.float32
</code></pre>
<h2 id="张量的维度"><a class="markdownIt-Anchor" href="#张量的维度"></a> 张量的维度</h2>
<p>不同类型的数据可以用不同维度(dimension)的张量来表示。标量为0维张量，向量为1维张量，矩阵为2维张量。彩色图像有rgb三个通道，可以表示为3维张量。视频还有时间维，可以表示为4维张量。<br>
可以简单地总结为：有几层中括号，就是多少维的张量。</p>
<pre class="highlight"><code class="python">scalar = torch.tensor(<span class="hljs-literal">True</span>)
<span class="hljs-built_in">print</span>(scalar)
<span class="hljs-built_in">print</span>(scalar.dim())  <span class="hljs-comment"># 标量，0维张量</span>

&gt;&gt;&gt;Output:
tensor(<span class="hljs-literal">True</span>)
<span class="hljs-number">0</span>
</code></pre>
<pre class="highlight"><code class="python">vector = torch.tensor([<span class="hljs-number">1.0</span>,<span class="hljs-number">2.0</span>,<span class="hljs-number">3.0</span>,<span class="hljs-number">4.0</span>]) <span class="hljs-comment">#向量，1维张量</span>
<span class="hljs-built_in">print</span>(vector)
<span class="hljs-built_in">print</span>(vector.dim())

&gt;&gt;&gt;Output:
tensor([<span class="hljs-number">1.</span>, <span class="hljs-number">2.</span>, <span class="hljs-number">3.</span>, <span class="hljs-number">4.</span>])
<span class="hljs-number">1</span>
</code></pre>
<pre class="highlight"><code class="python">matrix = torch.tensor([[<span class="hljs-number">1.0</span>,<span class="hljs-number">2.0</span>],[<span class="hljs-number">3.0</span>,<span class="hljs-number">4.0</span>]]) <span class="hljs-comment">#矩阵, 2维张量</span>
<span class="hljs-built_in">print</span>(matrix)
<span class="hljs-built_in">print</span>(matrix.dim())

&gt;&gt;&gt;Output:
tensor([[<span class="hljs-number">1.</span>, <span class="hljs-number">2.</span>],
        [<span class="hljs-number">3.</span>, <span class="hljs-number">4.</span>]])
<span class="hljs-number">2</span>
</code></pre>
<pre class="highlight"><code class="python">tensor3 = torch.tensor([[[<span class="hljs-number">1.0</span>,<span class="hljs-number">2.0</span>],[<span class="hljs-number">3.0</span>,<span class="hljs-number">4.0</span>]],[[<span class="hljs-number">5.0</span>,<span class="hljs-number">6.0</span>],[<span class="hljs-number">7.0</span>,<span class="hljs-number">8.0</span>]]])  <span class="hljs-comment"># 3维张量</span>
<span class="hljs-built_in">print</span>(tensor3)
<span class="hljs-built_in">print</span>(tensor3.dim())

&gt;&gt;&gt;Output:
tensor([[[<span class="hljs-number">1.</span>, <span class="hljs-number">2.</span>],
         [<span class="hljs-number">3.</span>, <span class="hljs-number">4.</span>]],

        [[<span class="hljs-number">5.</span>, <span class="hljs-number">6.</span>],
         [<span class="hljs-number">7.</span>, <span class="hljs-number">8.</span>]]])
<span class="hljs-number">3</span>
</code></pre>
<pre class="highlight"><code class="python">tensor4 = torch.tensor([[[[<span class="hljs-number">1.0</span>,<span class="hljs-number">1.0</span>],[<span class="hljs-number">2.0</span>,<span class="hljs-number">2.0</span>]],[[<span class="hljs-number">3.0</span>,<span class="hljs-number">3.0</span>],[<span class="hljs-number">4.0</span>,<span class="hljs-number">4.0</span>]]],
                        [[[<span class="hljs-number">5.0</span>,<span class="hljs-number">5.0</span>],[<span class="hljs-number">6.0</span>,<span class="hljs-number">6.0</span>]],[[<span class="hljs-number">7.0</span>,<span class="hljs-number">7.0</span>],[<span class="hljs-number">8.0</span>,<span class="hljs-number">8.0</span>]]]])  <span class="hljs-comment"># 4维张量</span>
<span class="hljs-built_in">print</span>(tensor4)
<span class="hljs-built_in">print</span>(tensor4.dim())

&gt;&gt;&gt;Output:
tensor([[[[<span class="hljs-number">1.</span>, <span class="hljs-number">1.</span>],
          [<span class="hljs-number">2.</span>, <span class="hljs-number">2.</span>]],

         [[<span class="hljs-number">3.</span>, <span class="hljs-number">3.</span>],
          [<span class="hljs-number">4.</span>, <span class="hljs-number">4.</span>]]],


        [[[<span class="hljs-number">5.</span>, <span class="hljs-number">5.</span>],
          [<span class="hljs-number">6.</span>, <span class="hljs-number">6.</span>]],

         [[<span class="hljs-number">7.</span>, <span class="hljs-number">7.</span>],
          [<span class="hljs-number">8.</span>, <span class="hljs-number">8.</span>]]]])
<span class="hljs-number">4</span>
</code></pre>
<h2 id="张量的尺寸"><a class="markdownIt-Anchor" href="#张量的尺寸"></a> 张量的尺寸</h2>
<p>可以使用<code>**shape**</code><strong>属性</strong>或者<code>**size()**</code><strong>方法</strong>查看张量在每一维的长度。<br>
可以使用<code>view()</code>方法改变张量的尺寸。如果<code>view()</code>方法改变尺寸失败，可以使用<code>reshape()</code>方法。</p>
<pre class="highlight"><code class="python">scalar = torch.tensor(<span class="hljs-literal">True</span>)
<span class="hljs-built_in">print</span>(scalar.size())
<span class="hljs-built_in">print</span>(scalar.shape)

&gt;&gt;&gt;Output:
torch.Size([])
torch.Size([])
</code></pre>
<pre class="highlight"><code class="python">vector = torch.tensor([<span class="hljs-number">1.0</span>, <span class="hljs-number">2.0</span>, <span class="hljs-number">3.0</span>, <span class="hljs-number">4.0</span>])
<span class="hljs-built_in">print</span>(vector.size())
<span class="hljs-built_in">print</span>(vector.shape)

&gt;&gt;&gt;Output:
torch.Size([<span class="hljs-number">4</span>])
torch.Size([<span class="hljs-number">4</span>])
</code></pre>
<pre class="highlight"><code class="python">matrix = torch.tensor([[<span class="hljs-number">1.0</span>,<span class="hljs-number">2.0</span>],[<span class="hljs-number">3.0</span>,<span class="hljs-number">4.0</span>]])
<span class="hljs-built_in">print</span>(matrix.size())
<span class="hljs-built_in">print</span>(matrix.shape)

&gt;&gt;&gt;Output:
torch.Size([<span class="hljs-number">2</span>, <span class="hljs-number">2</span>])
torch.Size([<span class="hljs-number">2</span>, <span class="hljs-number">2</span>])
</code></pre>
<p>使用view可以改变张量尺寸</p>
<pre class="highlight"><code class="python">vector = torch.arange(<span class="hljs-number">0</span>,<span class="hljs-number">12</span>)
<span class="hljs-built_in">print</span>(vector)
<span class="hljs-built_in">print</span>(vector.shape)

matrix34 = vector.view(<span class="hljs-number">3</span>,<span class="hljs-number">4</span>)
<span class="hljs-built_in">print</span>(matrix34)
<span class="hljs-built_in">print</span>(matrix34.shape)

matrix43 = vector.view(<span class="hljs-number">4</span>,-<span class="hljs-number">1</span>) <span class="hljs-comment">#-1表示该位置长度由程序自动推断</span>
<span class="hljs-built_in">print</span>(matrix43)
<span class="hljs-built_in">print</span>(matrix43.shape)

&gt;&gt;&gt;Output:
tensor([ <span class="hljs-number">0</span>,  <span class="hljs-number">1</span>,  <span class="hljs-number">2</span>,  <span class="hljs-number">3</span>,  <span class="hljs-number">4</span>,  <span class="hljs-number">5</span>,  <span class="hljs-number">6</span>,  <span class="hljs-number">7</span>,  <span class="hljs-number">8</span>,  <span class="hljs-number">9</span>, <span class="hljs-number">10</span>, <span class="hljs-number">11</span>])
torch.Size([<span class="hljs-number">12</span>])
tensor([[ <span class="hljs-number">0</span>,  <span class="hljs-number">1</span>,  <span class="hljs-number">2</span>,  <span class="hljs-number">3</span>],
        [ <span class="hljs-number">4</span>,  <span class="hljs-number">5</span>,  <span class="hljs-number">6</span>,  <span class="hljs-number">7</span>],
        [ <span class="hljs-number">8</span>,  <span class="hljs-number">9</span>, <span class="hljs-number">10</span>, <span class="hljs-number">11</span>]])
torch.Size([<span class="hljs-number">3</span>, <span class="hljs-number">4</span>])
tensor([[ <span class="hljs-number">0</span>,  <span class="hljs-number">1</span>,  <span class="hljs-number">2</span>],
        [ <span class="hljs-number">3</span>,  <span class="hljs-number">4</span>,  <span class="hljs-number">5</span>],
        [ <span class="hljs-number">6</span>,  <span class="hljs-number">7</span>,  <span class="hljs-number">8</span>],
        [ <span class="hljs-number">9</span>, <span class="hljs-number">10</span>, <span class="hljs-number">11</span>]])
torch.Size([<span class="hljs-number">4</span>, <span class="hljs-number">3</span>])
</code></pre>
<p>有些操作会让张量存储结构扭曲，直接使用view会失败，可以用reshape方法。</p>
<pre class="highlight"><code class="python">matrix26 = torch.arange(<span class="hljs-number">0</span>,<span class="hljs-number">12</span>).view(<span class="hljs-number">2</span>,<span class="hljs-number">6</span>)
<span class="hljs-built_in">print</span>(matrix26)
<span class="hljs-built_in">print</span>(matrix26.shape)

<span class="hljs-comment"># 转置操作让张量存储结构扭曲</span>
matrix62 = matrix26.t()
<span class="hljs-built_in">print</span>(matrix62.is_contiguous())

<span class="hljs-comment"># 直接使用view方法会失败，可以使用reshape方法</span>
<span class="hljs-comment"># matrix34 = matrix62.view(3,4) # error!</span>

<span class="hljs-comment"># 等价于matrix34 = matrix62.contiguous().view(3,4)。</span>
<span class="hljs-comment"># contiguous()函数，把tensor变成在内存中连续分布的形式。</span>
matrix34 = matrix62.reshape(<span class="hljs-number">3</span>,<span class="hljs-number">4</span>) 
<span class="hljs-built_in">print</span>(matrix34)

&gt;&gt;&gt;Output:
tensor([[ <span class="hljs-number">0</span>,  <span class="hljs-number">1</span>,  <span class="hljs-number">2</span>,  <span class="hljs-number">3</span>,  <span class="hljs-number">4</span>,  <span class="hljs-number">5</span>],
        [ <span class="hljs-number">6</span>,  <span class="hljs-number">7</span>,  <span class="hljs-number">8</span>,  <span class="hljs-number">9</span>, <span class="hljs-number">10</span>, <span class="hljs-number">11</span>]])
torch.Size([<span class="hljs-number">2</span>, <span class="hljs-number">6</span>])
<span class="hljs-literal">False</span>
tensor([[ <span class="hljs-number">0</span>,  <span class="hljs-number">6</span>,  <span class="hljs-number">1</span>,  <span class="hljs-number">7</span>],
        [ <span class="hljs-number">2</span>,  <span class="hljs-number">8</span>,  <span class="hljs-number">3</span>,  <span class="hljs-number">9</span>],
        [ <span class="hljs-number">4</span>, <span class="hljs-number">10</span>,  <span class="hljs-number">5</span>, <span class="hljs-number">11</span>]])
</code></pre>
<h2 id="张量和numpy数组"><a class="markdownIt-Anchor" href="#张量和numpy数组"></a> 张量和numpy数组</h2>
<p>可以用<code>numpy()</code>方法从Tensor得到numpy数组，也可以用<code>torch.from_numpy()</code>从numpy数组得到Tensor。这两种方法关联的Tensor和numpy数组是<strong>共享数据内存</strong>的。如果改变其中一个，另外一个的值也会发生改变。<br>
如果有需要，可以用张量的<code>clone()</code>方法拷贝张量，中断这种关联。<br>
此外，还可以使用<code>item()</code>方法从标量张量得到对应的Python数值。使用<code>tolist()</code>方法从张量得到对应的Python数值列表。</p>
<pre class="highlight"><code class="python"><span class="hljs-comment">#torch.from_numpy函数从numpy数组得到Tensor</span>

arr = np.zeros(<span class="hljs-number">3</span>)
tensor = torch.from_numpy(arr)
<span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;before add 1:&quot;</span>)
<span class="hljs-built_in">print</span>(arr)
<span class="hljs-built_in">print</span>(tensor)

<span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;\nafter add 1:&quot;</span>)
np.add(arr,<span class="hljs-number">1</span>, out = arr) <span class="hljs-comment"># 给 arr增加1，tensor也随之改变</span>
<span class="hljs-built_in">print</span>(arr)
<span class="hljs-built_in">print</span>(tensor)

&gt;&gt;&gt;Output:
before add <span class="hljs-number">1</span>:
[<span class="hljs-number">0.</span> <span class="hljs-number">0.</span> <span class="hljs-number">0.</span>]
tensor([<span class="hljs-number">0.</span>, <span class="hljs-number">0.</span>, <span class="hljs-number">0.</span>], dtype=torch.float64)

after add <span class="hljs-number">1</span>:
[<span class="hljs-number">1.</span> <span class="hljs-number">1.</span> <span class="hljs-number">1.</span>]
tensor([<span class="hljs-number">1.</span>, <span class="hljs-number">1.</span>, <span class="hljs-number">1.</span>], dtype=torch.float64)
</code></pre>
<pre class="highlight"><code class="python"><span class="hljs-comment"># numpy方法从Tensor得到numpy数组</span>

tensor = torch.zeros(<span class="hljs-number">3</span>)
arr = tensor.numpy()
<span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;before add 1:&quot;</span>)
<span class="hljs-built_in">print</span>(tensor)
<span class="hljs-built_in">print</span>(arr)

<span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;\nafter add 1:&quot;</span>)

<span class="hljs-comment"># 使用带下划线的方法表示计算结果会返回给调用 张量</span>
tensor.add_(<span class="hljs-number">1</span>) <span class="hljs-comment">#给 tensor增加1，arr也随之改变 </span>
<span class="hljs-comment"># 或： torch.add(tensor,1,out = tensor)</span>
<span class="hljs-built_in">print</span>(tensor)
<span class="hljs-built_in">print</span>(arr)

&gt;&gt;&gt;Output:
before add <span class="hljs-number">1</span>:
tensor([<span class="hljs-number">0.</span>, <span class="hljs-number">0.</span>, <span class="hljs-number">0.</span>])
[<span class="hljs-number">0.</span> <span class="hljs-number">0.</span> <span class="hljs-number">0.</span>]

after add <span class="hljs-number">1</span>:
tensor([<span class="hljs-number">1.</span>, <span class="hljs-number">1.</span>, <span class="hljs-number">1.</span>])
[<span class="hljs-number">1.</span> <span class="hljs-number">1.</span> <span class="hljs-number">1.</span>]
</code></pre>
<pre class="highlight"><code class="python"><span class="hljs-comment"># 可以用clone() 方法拷贝张量，中断这种关联</span>

tensor = torch.zeros(<span class="hljs-number">3</span>)

<span class="hljs-comment"># 使用clone方法拷贝张量, 拷贝后的张量和原始张量内存独立</span>
arr = tensor.clone().numpy() <span class="hljs-comment"># 也可以使用tensor.data.numpy()</span>
<span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;before add 1:&quot;</span>)
<span class="hljs-built_in">print</span>(tensor)
<span class="hljs-built_in">print</span>(arr)

<span class="hljs-built_in">print</span>(<span class="hljs-string">&quot;\nafter add 1:&quot;</span>)

<span class="hljs-comment">#使用 带下划线的方法表示计算结果会返回给调用 张量</span>
tensor.add_(<span class="hljs-number">1</span>) <span class="hljs-comment">#给 tensor增加1，arr不再随之改变</span>
<span class="hljs-built_in">print</span>(tensor)
<span class="hljs-built_in">print</span>(arr)

&gt;&gt;&gt;Output:
before add <span class="hljs-number">1</span>:
tensor([<span class="hljs-number">0.</span>, <span class="hljs-number">0.</span>, <span class="hljs-number">0.</span>])
[<span class="hljs-number">0.</span> <span class="hljs-number">0.</span> <span class="hljs-number">0.</span>]

after add <span class="hljs-number">1</span>:
tensor([<span class="hljs-number">1.</span>, <span class="hljs-number">1.</span>, <span class="hljs-number">1.</span>])
[<span class="hljs-number">0.</span> <span class="hljs-number">0.</span> <span class="hljs-number">0.</span>]
</code></pre>
<pre class="highlight"><code class="python"><span class="hljs-comment"># item方法和tolist方法可以将张量转换成Python数值和数值列表</span>
scalar = torch.tensor(<span class="hljs-number">1.0</span>)
<span class="hljs-built_in">print</span>(scalar)
s = scalar.item()
<span class="hljs-built_in">print</span>(s)
<span class="hljs-built_in">print</span>(<span class="hljs-built_in">type</span>(s))

tensor = torch.rand(<span class="hljs-number">2</span>,<span class="hljs-number">2</span>)
t = tensor.tolist()
<span class="hljs-built_in">print</span>(t)
<span class="hljs-built_in">print</span>(<span class="hljs-built_in">type</span>(t))

&gt;&gt;&gt;Output:
tensor(<span class="hljs-number">1.</span>)
<span class="hljs-number">1.0</span>
&lt;<span class="hljs-keyword">class</span> <span class="hljs-string">&#x27;float&#x27;</span>&gt;
[[<span class="hljs-number">0.8211846351623535</span>, <span class="hljs-number">0.20020723342895508</span>], [<span class="hljs-number">0.011571824550628662</span>, <span class="hljs-number">0.2906131148338318</span>]]
&lt;<span class="hljs-keyword">class</span> <span class="hljs-string">&#x27;list&#x27;</span>&gt;
</code></pre>
<h2 id="张量的操作"><a class="markdownIt-Anchor" href="#张量的操作"></a> 张量的操作</h2>
<p>张量的操作主要包括张量的结构操作和张量的数学运算。</p>
<ul>
<li>张量结构操作诸如：张量创建，索引切片，维度变换，合并分割。</li>
<li>张量数学运算主要有：标量运算，向量运算，矩阵运算，张量运算的广播机制。</li>
</ul>
<h3 id="张量的结构操作"><a class="markdownIt-Anchor" href="#张量的结构操作"></a> 张量的结构操作</h3>
<h4 id="张量创建"><a class="markdownIt-Anchor" href="#张量创建"></a> 张量创建</h4>
<p>张量创建的许多方法和numpy中创建array的方法很像。�也可参考【创建张量】</p>
<pre class="highlight"><code class="python">a = torch.tensor([<span class="hljs-number">1</span>,<span class="hljs-number">2</span>,<span class="hljs-number">3</span>],dtype = torch.<span class="hljs-built_in">float</span>)
<span class="hljs-built_in">print</span>(a)

b = torch.arange(<span class="hljs-number">1</span>,<span class="hljs-number">10</span>,step = <span class="hljs-number">2</span>)
<span class="hljs-built_in">print</span>(b)

c = torch.linspace(<span class="hljs-number">0.0</span>,<span class="hljs-number">2</span>*<span class="hljs-number">3.14</span>,<span class="hljs-number">10</span>)
<span class="hljs-built_in">print</span>(c)

d = torch.zeros((<span class="hljs-number">3</span>,<span class="hljs-number">3</span>))
<span class="hljs-built_in">print</span>(d)

&gt;&gt;&gt;Output:
tensor([<span class="hljs-number">1.</span>, <span class="hljs-number">2.</span>, <span class="hljs-number">3.</span>])
tensor([<span class="hljs-number">1</span>, <span class="hljs-number">3</span>, <span class="hljs-number">5</span>, <span class="hljs-number">7</span>, <span class="hljs-number">9</span>])
tensor([<span class="hljs-number">0.0000</span>, <span class="hljs-number">0.6978</span>, <span class="hljs-number">1.3956</span>, <span class="hljs-number">2.0933</span>, <span class="hljs-number">2.7911</span>, <span class="hljs-number">3.4889</span>, <span class="hljs-number">4.1867</span>, <span class="hljs-number">4.8844</span>, <span class="hljs-number">5.5822</span>,
        <span class="hljs-number">6.2800</span>])
tensor([[<span class="hljs-number">0.</span>, <span class="hljs-number">0.</span>, <span class="hljs-number">0.</span>],
        [<span class="hljs-number">0.</span>, <span class="hljs-number">0.</span>, <span class="hljs-number">0.</span>],
        [<span class="hljs-number">0.</span>, <span class="hljs-number">0.</span>, <span class="hljs-number">0.</span>]])
</code></pre>
<pre class="highlight"><code class="python">a = torch.ones((<span class="hljs-number">3</span>,<span class="hljs-number">3</span>), dtype = torch.<span class="hljs-built_in">int</span>)
b = torch.zeros_like(a, dtype = torch.<span class="hljs-built_in">float</span>)
<span class="hljs-built_in">print</span>(a)
<span class="hljs-built_in">print</span>(b)

<span class="hljs-comment"># 等价于 b.fill_(5)</span>
torch.fill_(b,<span class="hljs-number">5</span>) 
<span class="hljs-built_in">print</span>(b)

&gt;&gt;&gt;Output:
tensor([[<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>],
        [<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>],
        [<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>]], dtype=torch.int32)
tensor([[<span class="hljs-number">0.</span>, <span class="hljs-number">0.</span>, <span class="hljs-number">0.</span>],
        [<span class="hljs-number">0.</span>, <span class="hljs-number">0.</span>, <span class="hljs-number">0.</span>],
        [<span class="hljs-number">0.</span>, <span class="hljs-number">0.</span>, <span class="hljs-number">0.</span>]])
tensor([[<span class="hljs-number">5.</span>, <span class="hljs-number">5.</span>, <span class="hljs-number">5.</span>],
        [<span class="hljs-number">5.</span>, <span class="hljs-number">5.</span>, <span class="hljs-number">5.</span>],
        [<span class="hljs-number">5.</span>, <span class="hljs-number">5.</span>, <span class="hljs-number">5.</span>]])
</code></pre>
<pre class="highlight"><code class="python"><span class="hljs-comment"># 均匀随机分布</span>
minval,maxval = <span class="hljs-number">0</span>,<span class="hljs-number">10</span>
a = minval + (maxval-minval)*torch.rand([<span class="hljs-number">5</span>])
<span class="hljs-built_in">print</span>(a)

<span class="hljs-comment"># 正态分布随机</span>
b = torch.normal(mean = torch.zeros(<span class="hljs-number">3</span>,<span class="hljs-number">3</span>), std = torch.ones(<span class="hljs-number">3</span>,<span class="hljs-number">3</span>))
<span class="hljs-built_in">print</span>(b)

<span class="hljs-comment"># 正态分布随机</span>
mean,std = <span class="hljs-number">2</span>,<span class="hljs-number">5</span>
c = std*torch.randn((<span class="hljs-number">3</span>,<span class="hljs-number">3</span>))+mean
<span class="hljs-built_in">print</span>(c)

<span class="hljs-comment"># 整数随机排列</span>
d = torch.randperm(<span class="hljs-number">20</span>)
<span class="hljs-built_in">print</span>(d)

&gt;&gt;&gt;Output:
tensor([<span class="hljs-number">4.9626</span>, <span class="hljs-number">7.6822</span>, <span class="hljs-number">0.8848</span>, <span class="hljs-number">1.3203</span>, <span class="hljs-number">3.0742</span>])
tensor([[ <span class="hljs-number">0.5507</span>,  <span class="hljs-number">0.2704</span>,  <span class="hljs-number">0.6472</span>],
        [ <span class="hljs-number">0.2490</span>, -<span class="hljs-number">0.3354</span>,  <span class="hljs-number">0.4564</span>],
        [-<span class="hljs-number">0.6255</span>,  <span class="hljs-number">0.4539</span>, -<span class="hljs-number">1.3740</span>]])
tensor([[<span class="hljs-number">16.2371</span>, -<span class="hljs-number">1.6612</span>,  <span class="hljs-number">3.9163</span>],
        [ <span class="hljs-number">7.4999</span>,  <span class="hljs-number">1.5616</span>,  <span class="hljs-number">4.0768</span>],
        [ <span class="hljs-number">5.2128</span>, -<span class="hljs-number">8.9407</span>,  <span class="hljs-number">6.4601</span>]])
tensor([ <span class="hljs-number">3</span>, <span class="hljs-number">17</span>,  <span class="hljs-number">9</span>, <span class="hljs-number">19</span>,  <span class="hljs-number">1</span>, <span class="hljs-number">18</span>,  <span class="hljs-number">4</span>, <span class="hljs-number">13</span>, <span class="hljs-number">15</span>, <span class="hljs-number">12</span>,  <span class="hljs-number">0</span>, <span class="hljs-number">16</span>,  <span class="hljs-number">7</span>, <span class="hljs-number">11</span>,  <span class="hljs-number">2</span>,  <span class="hljs-number">5</span>,  <span class="hljs-number">8</span>, <span class="hljs-number">10</span>,
         <span class="hljs-number">6</span>, <span class="hljs-number">14</span>])
</code></pre>
<pre class="highlight"><code class="python">I = torch.eye(<span class="hljs-number">3</span>,<span class="hljs-number">3</span>) <span class="hljs-comment">#单位矩阵</span>
<span class="hljs-built_in">print</span>(I)
t = torch.diag(torch.tensor([<span class="hljs-number">1</span>,<span class="hljs-number">2</span>,<span class="hljs-number">3</span>])) <span class="hljs-comment">#对角矩阵</span>
<span class="hljs-built_in">print</span>(t)

&gt;&gt;&gt;Output:
tensor([[<span class="hljs-number">1.</span>, <span class="hljs-number">0.</span>, <span class="hljs-number">0.</span>],
        [<span class="hljs-number">0.</span>, <span class="hljs-number">1.</span>, <span class="hljs-number">0.</span>],
        [<span class="hljs-number">0.</span>, <span class="hljs-number">0.</span>, <span class="hljs-number">1.</span>]])
tensor([[<span class="hljs-number">1</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>],
        [<span class="hljs-number">0</span>, <span class="hljs-number">2</span>, <span class="hljs-number">0</span>],
        [<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">3</span>]])
</code></pre>
<h4 id="索引切片"><a class="markdownIt-Anchor" href="#索引切片"></a> 索引切片</h4>
<p>张量的索引切片方式和numpy几乎是一样的。切片时支持缺省参数和省略号。可以通过索引和切片对部分元素进行修改。<br>
此外，对于不规则的切片提取，可以使用<code>torch.index_select</code>，<code>torch.masked_select</code>，<code>torch.take</code>。<br>
如果要通过修改张量的某些元素得到新的张量，可以使用<code>torch.where</code>，<code>torch.masked_fill</code>，<code>torch.index_fill</code>。</p>
<pre class="highlight"><code class="python"><span class="hljs-comment"># 均匀随机分布</span>
torch.manual_seed(<span class="hljs-number">0</span>)
minval,maxval = <span class="hljs-number">0</span>,<span class="hljs-number">10</span>
t = torch.floor(minval + (maxval-minval)*torch.rand([<span class="hljs-number">5</span>,<span class="hljs-number">5</span>])).<span class="hljs-built_in">int</span>()
<span class="hljs-built_in">print</span>(t)

<span class="hljs-comment">#第1行</span>
<span class="hljs-built_in">print</span>(t[<span class="hljs-number">0</span>])
<span class="hljs-comment">#倒数第一行</span>
<span class="hljs-built_in">print</span>(t[-<span class="hljs-number">1</span>])
<span class="hljs-comment">#第2行第4列</span>
<span class="hljs-built_in">print</span>(t[<span class="hljs-number">1</span>,<span class="hljs-number">3</span>])
<span class="hljs-built_in">print</span>(t[<span class="hljs-number">1</span>][<span class="hljs-number">3</span>])

<span class="hljs-comment">#第1行至第3行</span>
<span class="hljs-built_in">print</span>(t[<span class="hljs-number">1</span>:<span class="hljs-number">4</span>,:])
<span class="hljs-comment">#第1行至最后一行，第0列到最后一列每隔两列取一列</span>
<span class="hljs-built_in">print</span>(t[<span class="hljs-number">1</span>:<span class="hljs-number">4</span>,:<span class="hljs-number">4</span>:<span class="hljs-number">2</span>])

&gt;&gt;&gt;Output:
tensor([[<span class="hljs-number">4</span>, <span class="hljs-number">7</span>, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">3</span>],
        [<span class="hljs-number">6</span>, <span class="hljs-number">4</span>, <span class="hljs-number">8</span>, <span class="hljs-number">4</span>, <span class="hljs-number">6</span>],
        [<span class="hljs-number">3</span>, <span class="hljs-number">4</span>, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">2</span>],
        [<span class="hljs-number">5</span>, <span class="hljs-number">6</span>, <span class="hljs-number">8</span>, <span class="hljs-number">1</span>, <span class="hljs-number">2</span>],
        [<span class="hljs-number">6</span>, <span class="hljs-number">9</span>, <span class="hljs-number">3</span>, <span class="hljs-number">8</span>, <span class="hljs-number">4</span>]], dtype=torch.int32)

tensor([<span class="hljs-number">4</span>, <span class="hljs-number">7</span>, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">3</span>], dtype=torch.int32)
tensor([<span class="hljs-number">6</span>, <span class="hljs-number">9</span>, <span class="hljs-number">3</span>, <span class="hljs-number">8</span>, <span class="hljs-number">4</span>], dtype=torch.int32)
tensor(<span class="hljs-number">4</span>, dtype=torch.int32)
tensor(<span class="hljs-number">4</span>, dtype=torch.int32)

tensor([[<span class="hljs-number">6</span>, <span class="hljs-number">4</span>, <span class="hljs-number">8</span>, <span class="hljs-number">4</span>, <span class="hljs-number">6</span>],
        [<span class="hljs-number">3</span>, <span class="hljs-number">4</span>, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">2</span>],
        [<span class="hljs-number">5</span>, <span class="hljs-number">6</span>, <span class="hljs-number">8</span>, <span class="hljs-number">1</span>, <span class="hljs-number">2</span>]], dtype=torch.int32)
tensor([[<span class="hljs-number">6</span>, <span class="hljs-number">8</span>],
        [<span class="hljs-number">3</span>, <span class="hljs-number">0</span>],
        [<span class="hljs-number">5</span>, <span class="hljs-number">8</span>]], dtype=torch.int32)
</code></pre>
<pre class="highlight"><code class="python"><span class="hljs-comment">#可以使用索引和切片修改部分元素</span>
x = torch.tensor([[<span class="hljs-number">1</span>,<span class="hljs-number">2</span>],[<span class="hljs-number">3</span>,<span class="hljs-number">4</span>]],dtype = torch.float32,requires_grad=<span class="hljs-literal">True</span>)
x.data[<span class="hljs-number">1</span>,:] = torch.tensor([<span class="hljs-number">0.0</span>,<span class="hljs-number">0.0</span>])
<span class="hljs-built_in">print</span>(x)

&gt;&gt;&gt;Output:
tensor([[<span class="hljs-number">1.</span>, <span class="hljs-number">2.</span>],
        [<span class="hljs-number">0.</span>, <span class="hljs-number">0.</span>]], requires_grad=<span class="hljs-literal">True</span>)
</code></pre>
<pre class="highlight"><code class="python">a = torch.arange(<span class="hljs-number">27</span>).view(<span class="hljs-number">3</span>,<span class="hljs-number">3</span>,<span class="hljs-number">3</span>)
<span class="hljs-built_in">print</span>(a)

<span class="hljs-comment">#省略号可以表示多个冒号</span>
<span class="hljs-built_in">print</span>(a[...,<span class="hljs-number">1</span>])

&gt;&gt;&gt;Output:
tensor([[[ <span class="hljs-number">0</span>,  <span class="hljs-number">1</span>,  <span class="hljs-number">2</span>],
         [ <span class="hljs-number">3</span>,  <span class="hljs-number">4</span>,  <span class="hljs-number">5</span>],
         [ <span class="hljs-number">6</span>,  <span class="hljs-number">7</span>,  <span class="hljs-number">8</span>]],

        [[ <span class="hljs-number">9</span>, <span class="hljs-number">10</span>, <span class="hljs-number">11</span>],
         [<span class="hljs-number">12</span>, <span class="hljs-number">13</span>, <span class="hljs-number">14</span>],
         [<span class="hljs-number">15</span>, <span class="hljs-number">16</span>, <span class="hljs-number">17</span>]],

        [[<span class="hljs-number">18</span>, <span class="hljs-number">19</span>, <span class="hljs-number">20</span>],
         [<span class="hljs-number">21</span>, <span class="hljs-number">22</span>, <span class="hljs-number">23</span>],
         [<span class="hljs-number">24</span>, <span class="hljs-number">25</span>, <span class="hljs-number">26</span>]]])
tensor([[ <span class="hljs-number">1</span>,  <span class="hljs-number">4</span>,  <span class="hljs-number">7</span>],
        [<span class="hljs-number">10</span>, <span class="hljs-number">13</span>, <span class="hljs-number">16</span>],
        [<span class="hljs-number">19</span>, <span class="hljs-number">22</span>, <span class="hljs-number">25</span>]])
</code></pre>
<p>以上切片方式相对规则，对于不规则的切片提取，可以使用<code>torch.index_select</code>，<code>torch.masked_select</code>，<code>torch.take</code>，<code>torch.gather</code>。<br>
以班级成绩册为例子，有4个班级，每个班级10个学生，每个学生7门科目成绩。可以用一个4×10×7的张量来表示。</p>
<pre class="highlight"><code class="python">minval=<span class="hljs-number">0</span>
maxval=<span class="hljs-number">100</span>
scores = torch.floor(minval + (maxval-minval)*torch.rand([<span class="hljs-number">4</span>,<span class="hljs-number">10</span>,<span class="hljs-number">7</span>])).<span class="hljs-built_in">int</span>()
<span class="hljs-built_in">print</span>(scores)

&gt;&gt;&gt;Output:
tensor([[[<span class="hljs-number">55</span>, <span class="hljs-number">95</span>,  <span class="hljs-number">3</span>, <span class="hljs-number">18</span>, <span class="hljs-number">37</span>, <span class="hljs-number">30</span>, <span class="hljs-number">93</span>],
         [<span class="hljs-number">17</span>, <span class="hljs-number">26</span>, <span class="hljs-number">15</span>,  <span class="hljs-number">3</span>, <span class="hljs-number">20</span>, <span class="hljs-number">92</span>, <span class="hljs-number">72</span>],
         [<span class="hljs-number">74</span>, <span class="hljs-number">52</span>, <span class="hljs-number">24</span>, <span class="hljs-number">58</span>,  <span class="hljs-number">3</span>, <span class="hljs-number">13</span>, <span class="hljs-number">24</span>],
         [<span class="hljs-number">81</span>, <span class="hljs-number">79</span>, <span class="hljs-number">27</span>, <span class="hljs-number">48</span>, <span class="hljs-number">81</span>, <span class="hljs-number">99</span>, <span class="hljs-number">69</span>],
         [<span class="hljs-number">56</span>, <span class="hljs-number">83</span>, <span class="hljs-number">20</span>, <span class="hljs-number">59</span>, <span class="hljs-number">11</span>, <span class="hljs-number">15</span>, <span class="hljs-number">24</span>],
         [<span class="hljs-number">72</span>, <span class="hljs-number">70</span>, <span class="hljs-number">20</span>, <span class="hljs-number">65</span>, <span class="hljs-number">77</span>, <span class="hljs-number">43</span>, <span class="hljs-number">51</span>],
         [<span class="hljs-number">61</span>, <span class="hljs-number">81</span>, <span class="hljs-number">98</span>, <span class="hljs-number">11</span>, <span class="hljs-number">31</span>, <span class="hljs-number">69</span>, <span class="hljs-number">91</span>],
         [<span class="hljs-number">93</span>, <span class="hljs-number">94</span>, <span class="hljs-number">59</span>,  <span class="hljs-number">6</span>, <span class="hljs-number">54</span>, <span class="hljs-number">18</span>,  <span class="hljs-number">3</span>],
         [<span class="hljs-number">94</span>, <span class="hljs-number">88</span>,  <span class="hljs-number">0</span>, <span class="hljs-number">59</span>, <span class="hljs-number">41</span>, <span class="hljs-number">41</span>, <span class="hljs-number">27</span>],
         [<span class="hljs-number">69</span>, <span class="hljs-number">20</span>, <span class="hljs-number">68</span>, <span class="hljs-number">75</span>, <span class="hljs-number">85</span>, <span class="hljs-number">68</span>,  <span class="hljs-number">0</span>]],

        [[<span class="hljs-number">17</span>, <span class="hljs-number">74</span>, <span class="hljs-number">60</span>, <span class="hljs-number">10</span>, <span class="hljs-number">21</span>, <span class="hljs-number">97</span>, <span class="hljs-number">83</span>],
         [<span class="hljs-number">28</span>, <span class="hljs-number">37</span>,  <span class="hljs-number">2</span>, <span class="hljs-number">49</span>, <span class="hljs-number">12</span>, <span class="hljs-number">11</span>, <span class="hljs-number">47</span>],
         [<span class="hljs-number">57</span>, <span class="hljs-number">29</span>, <span class="hljs-number">79</span>, <span class="hljs-number">19</span>, <span class="hljs-number">95</span>, <span class="hljs-number">84</span>,  <span class="hljs-number">7</span>],
         [<span class="hljs-number">37</span>, <span class="hljs-number">52</span>, <span class="hljs-number">57</span>, <span class="hljs-number">61</span>, <span class="hljs-number">69</span>, <span class="hljs-number">52</span>, <span class="hljs-number">25</span>],
         [<span class="hljs-number">73</span>,  <span class="hljs-number">2</span>, <span class="hljs-number">20</span>, <span class="hljs-number">37</span>, <span class="hljs-number">25</span>, <span class="hljs-number">32</span>,  <span class="hljs-number">9</span>],
         [<span class="hljs-number">39</span>, <span class="hljs-number">60</span>, <span class="hljs-number">17</span>, <span class="hljs-number">47</span>, <span class="hljs-number">85</span>, <span class="hljs-number">44</span>, <span class="hljs-number">51</span>],
         [<span class="hljs-number">45</span>, <span class="hljs-number">60</span>, <span class="hljs-number">81</span>, <span class="hljs-number">97</span>, <span class="hljs-number">81</span>, <span class="hljs-number">97</span>, <span class="hljs-number">46</span>],
         [ <span class="hljs-number">5</span>, <span class="hljs-number">26</span>, <span class="hljs-number">84</span>, <span class="hljs-number">49</span>, <span class="hljs-number">25</span>, <span class="hljs-number">11</span>,  <span class="hljs-number">3</span>],
         [ <span class="hljs-number">7</span>, <span class="hljs-number">39</span>, <span class="hljs-number">77</span>, <span class="hljs-number">77</span>,  <span class="hljs-number">1</span>, <span class="hljs-number">81</span>, <span class="hljs-number">10</span>],
         [<span class="hljs-number">39</span>, <span class="hljs-number">29</span>, <span class="hljs-number">40</span>, <span class="hljs-number">40</span>,  <span class="hljs-number">5</span>,  <span class="hljs-number">6</span>, <span class="hljs-number">42</span>]],

        [[<span class="hljs-number">50</span>, <span class="hljs-number">27</span>, <span class="hljs-number">68</span>,  <span class="hljs-number">4</span>, <span class="hljs-number">46</span>, <span class="hljs-number">93</span>, <span class="hljs-number">29</span>],
         [<span class="hljs-number">95</span>, <span class="hljs-number">68</span>,  <span class="hljs-number">4</span>, <span class="hljs-number">81</span>, <span class="hljs-number">44</span>, <span class="hljs-number">27</span>, <span class="hljs-number">89</span>],
         [ <span class="hljs-number">9</span>, <span class="hljs-number">55</span>, <span class="hljs-number">39</span>, <span class="hljs-number">85</span>, <span class="hljs-number">63</span>, <span class="hljs-number">74</span>, <span class="hljs-number">67</span>],
         [<span class="hljs-number">37</span>, <span class="hljs-number">39</span>,  <span class="hljs-number">8</span>, <span class="hljs-number">77</span>, <span class="hljs-number">89</span>, <span class="hljs-number">84</span>, <span class="hljs-number">14</span>],
         [<span class="hljs-number">52</span>, <span class="hljs-number">14</span>, <span class="hljs-number">22</span>, <span class="hljs-number">20</span>, <span class="hljs-number">67</span>, <span class="hljs-number">20</span>, <span class="hljs-number">48</span>],
         [<span class="hljs-number">52</span>, <span class="hljs-number">82</span>, <span class="hljs-number">12</span>, <span class="hljs-number">15</span>, <span class="hljs-number">20</span>, <span class="hljs-number">84</span>, <span class="hljs-number">32</span>],
         [<span class="hljs-number">92</span>, <span class="hljs-number">68</span>, <span class="hljs-number">56</span>, <span class="hljs-number">49</span>, <span class="hljs-number">40</span>, <span class="hljs-number">56</span>, <span class="hljs-number">38</span>],
         [<span class="hljs-number">49</span>, <span class="hljs-number">56</span>, <span class="hljs-number">10</span>, <span class="hljs-number">23</span>, <span class="hljs-number">90</span>,  <span class="hljs-number">9</span>, <span class="hljs-number">46</span>],
         [<span class="hljs-number">99</span>, <span class="hljs-number">68</span>, <span class="hljs-number">51</span>,  <span class="hljs-number">6</span>, <span class="hljs-number">74</span>, <span class="hljs-number">14</span>, <span class="hljs-number">35</span>],
         [<span class="hljs-number">33</span>, <span class="hljs-number">42</span>, <span class="hljs-number">50</span>, <span class="hljs-number">91</span>, <span class="hljs-number">56</span>, <span class="hljs-number">94</span>, <span class="hljs-number">80</span>]],

        [[<span class="hljs-number">18</span>, <span class="hljs-number">72</span>, <span class="hljs-number">14</span>, <span class="hljs-number">28</span>, <span class="hljs-number">64</span>, <span class="hljs-number">66</span>, <span class="hljs-number">87</span>],
         [<span class="hljs-number">33</span>, <span class="hljs-number">50</span>, <span class="hljs-number">75</span>,  <span class="hljs-number">1</span>, <span class="hljs-number">86</span>,  <span class="hljs-number">8</span>, <span class="hljs-number">50</span>],
         [<span class="hljs-number">41</span>, <span class="hljs-number">23</span>, <span class="hljs-number">56</span>, <span class="hljs-number">91</span>, <span class="hljs-number">35</span>, <span class="hljs-number">20</span>, <span class="hljs-number">31</span>],
         [ <span class="hljs-number">0</span>, <span class="hljs-number">72</span>, <span class="hljs-number">25</span>, <span class="hljs-number">16</span>, <span class="hljs-number">21</span>, <span class="hljs-number">78</span>, <span class="hljs-number">76</span>],
         [<span class="hljs-number">88</span>, <span class="hljs-number">68</span>, <span class="hljs-number">33</span>, <span class="hljs-number">36</span>, <span class="hljs-number">64</span>, <span class="hljs-number">91</span>, <span class="hljs-number">63</span>],
         [<span class="hljs-number">26</span>, <span class="hljs-number">26</span>,  <span class="hljs-number">2</span>, <span class="hljs-number">60</span>, <span class="hljs-number">21</span>,  <span class="hljs-number">5</span>, <span class="hljs-number">93</span>],
         [<span class="hljs-number">17</span>, <span class="hljs-number">44</span>, <span class="hljs-number">64</span>, <span class="hljs-number">51</span>, <span class="hljs-number">16</span>,  <span class="hljs-number">9</span>, <span class="hljs-number">89</span>],
         [<span class="hljs-number">58</span>, <span class="hljs-number">91</span>, <span class="hljs-number">33</span>, <span class="hljs-number">64</span>, <span class="hljs-number">38</span>, <span class="hljs-number">47</span>, <span class="hljs-number">19</span>],
         [<span class="hljs-number">66</span>, <span class="hljs-number">65</span>, <span class="hljs-number">48</span>, <span class="hljs-number">38</span>, <span class="hljs-number">19</span>, <span class="hljs-number">84</span>, <span class="hljs-number">12</span>],
         [<span class="hljs-number">70</span>, <span class="hljs-number">33</span>, <span class="hljs-number">25</span>, <span class="hljs-number">58</span>, <span class="hljs-number">24</span>, <span class="hljs-number">61</span>, <span class="hljs-number">59</span>]]], dtype=torch.int32)
</code></pre>
<p>抽取每个班级第0个学生，第5个学生，第9个学生的全部成绩</p>
<pre class="highlight"><code class="python">torch.index_select(scores, dim = <span class="hljs-number">1</span>, index = torch.tensor([<span class="hljs-number">0</span>,<span class="hljs-number">5</span>,<span class="hljs-number">9</span>]))

&gt;&gt;&gt;Output:
tensor([[[<span class="hljs-number">55</span>, <span class="hljs-number">95</span>,  <span class="hljs-number">3</span>, <span class="hljs-number">18</span>, <span class="hljs-number">37</span>, <span class="hljs-number">30</span>, <span class="hljs-number">93</span>],
         [<span class="hljs-number">72</span>, <span class="hljs-number">70</span>, <span class="hljs-number">20</span>, <span class="hljs-number">65</span>, <span class="hljs-number">77</span>, <span class="hljs-number">43</span>, <span class="hljs-number">51</span>],
         [<span class="hljs-number">69</span>, <span class="hljs-number">20</span>, <span class="hljs-number">68</span>, <span class="hljs-number">75</span>, <span class="hljs-number">85</span>, <span class="hljs-number">68</span>,  <span class="hljs-number">0</span>]],

        [[<span class="hljs-number">17</span>, <span class="hljs-number">74</span>, <span class="hljs-number">60</span>, <span class="hljs-number">10</span>, <span class="hljs-number">21</span>, <span class="hljs-number">97</span>, <span class="hljs-number">83</span>],
         [<span class="hljs-number">39</span>, <span class="hljs-number">60</span>, <span class="hljs-number">17</span>, <span class="hljs-number">47</span>, <span class="hljs-number">85</span>, <span class="hljs-number">44</span>, <span class="hljs-number">51</span>],
         [<span class="hljs-number">39</span>, <span class="hljs-number">29</span>, <span class="hljs-number">40</span>, <span class="hljs-number">40</span>,  <span class="hljs-number">5</span>,  <span class="hljs-number">6</span>, <span class="hljs-number">42</span>]],

        [[<span class="hljs-number">50</span>, <span class="hljs-number">27</span>, <span class="hljs-number">68</span>,  <span class="hljs-number">4</span>, <span class="hljs-number">46</span>, <span class="hljs-number">93</span>, <span class="hljs-number">29</span>],
         [<span class="hljs-number">52</span>, <span class="hljs-number">82</span>, <span class="hljs-number">12</span>, <span class="hljs-number">15</span>, <span class="hljs-number">20</span>, <span class="hljs-number">84</span>, <span class="hljs-number">32</span>],
         [<span class="hljs-number">33</span>, <span class="hljs-number">42</span>, <span class="hljs-number">50</span>, <span class="hljs-number">91</span>, <span class="hljs-number">56</span>, <span class="hljs-number">94</span>, <span class="hljs-number">80</span>]],

        [[<span class="hljs-number">18</span>, <span class="hljs-number">72</span>, <span class="hljs-number">14</span>, <span class="hljs-number">28</span>, <span class="hljs-number">64</span>, <span class="hljs-number">66</span>, <span class="hljs-number">87</span>],
         [<span class="hljs-number">26</span>, <span class="hljs-number">26</span>,  <span class="hljs-number">2</span>, <span class="hljs-number">60</span>, <span class="hljs-number">21</span>,  <span class="hljs-number">5</span>, <span class="hljs-number">93</span>],
         [<span class="hljs-number">70</span>, <span class="hljs-number">33</span>, <span class="hljs-number">25</span>, <span class="hljs-number">58</span>, <span class="hljs-number">24</span>, <span class="hljs-number">61</span>, <span class="hljs-number">59</span>]]], dtype=torch.int32)
</code></pre>
<p>抽取每个班级第0个学生，第5个学生，第9个学生的第1门课程，第3门课程，第6门课程成绩</p>
<pre class="highlight"><code class="python">q = torch.index_select(
    	torch.index_select(scores,dim = <span class="hljs-number">1</span>,index = torch.tensor([<span class="hljs-number">0</span>,<span class="hljs-number">5</span>,<span class="hljs-number">9</span>])),
    	dim=<span class="hljs-number">2</span>, index = torch.tensor([<span class="hljs-number">1</span>,<span class="hljs-number">3</span>,<span class="hljs-number">6</span>])
	)
</code></pre>
<p>抽取第0个班级第0个学生的第0门课程，第2个班级的第4个学生的第1门课程，第3个班级的第9个学生第6门课程成绩。</p>
<pre class="highlight"><code class="python"><span class="hljs-comment"># take将输入看成一维数组，输出和index同形状</span>
s = torch.take(scores,torch.tensor([<span class="hljs-number">0</span>*<span class="hljs-number">10</span>*<span class="hljs-number">7</span>+<span class="hljs-number">0</span>,<span class="hljs-number">2</span>*<span class="hljs-number">10</span>*<span class="hljs-number">7</span>+<span class="hljs-number">4</span>*<span class="hljs-number">7</span>+<span class="hljs-number">1</span>,<span class="hljs-number">3</span>*<span class="hljs-number">10</span>*<span class="hljs-number">7</span>+<span class="hljs-number">9</span>*<span class="hljs-number">7</span>+<span class="hljs-number">6</span>]))
<span class="hljs-built_in">print</span>(s)

&gt;&gt;&gt;Output:
tensor([<span class="hljs-number">55</span>, <span class="hljs-number">14</span>, <span class="hljs-number">59</span>], dtype=torch.int32)
</code></pre>
<p>抽取分数大于等于80分的分数（布尔索引）</p>
<pre class="highlight"><code class="python">g = torch.masked_select(scores, scores&gt;=<span class="hljs-number">80</span>)
<span class="hljs-built_in">print</span>(g) <span class="hljs-comment"># 结果是1维张量</span>

&gt;&gt;&gt;Output:
tensor([<span class="hljs-number">95</span>, <span class="hljs-number">93</span>, <span class="hljs-number">92</span>, <span class="hljs-number">81</span>, <span class="hljs-number">81</span>, <span class="hljs-number">99</span>, <span class="hljs-number">83</span>, <span class="hljs-number">81</span>, <span class="hljs-number">98</span>, <span class="hljs-number">91</span>, <span class="hljs-number">93</span>, <span class="hljs-number">94</span>, <span class="hljs-number">94</span>, <span class="hljs-number">88</span>, <span class="hljs-number">85</span>, <span class="hljs-number">97</span>, <span class="hljs-number">83</span>, <span class="hljs-number">95</span>,
        <span class="hljs-number">84</span>, <span class="hljs-number">85</span>, <span class="hljs-number">81</span>, <span class="hljs-number">97</span>, <span class="hljs-number">81</span>, <span class="hljs-number">97</span>, <span class="hljs-number">84</span>, <span class="hljs-number">81</span>, <span class="hljs-number">93</span>, <span class="hljs-number">95</span>, <span class="hljs-number">81</span>, <span class="hljs-number">89</span>, <span class="hljs-number">85</span>, <span class="hljs-number">89</span>, <span class="hljs-number">84</span>, <span class="hljs-number">82</span>, <span class="hljs-number">84</span>, <span class="hljs-number">92</span>,
        <span class="hljs-number">90</span>, <span class="hljs-number">99</span>, <span class="hljs-number">91</span>, <span class="hljs-number">94</span>, <span class="hljs-number">80</span>, <span class="hljs-number">87</span>, <span class="hljs-number">86</span>, <span class="hljs-number">91</span>, <span class="hljs-number">88</span>, <span class="hljs-number">91</span>, <span class="hljs-number">93</span>, <span class="hljs-number">89</span>, <span class="hljs-number">91</span>, <span class="hljs-number">84</span>],
       dtype=torch.int32)
</code></pre>
<p>以上这些方法仅能提取张量的部分元素值，但不能更改张量的部分元素值得到新的张量。<br>
如果要通过修改张量的部分元素值得到新的张量，可以使用<code>torch.where</code>，<code>torch.index_fill</code>和 <code>torch.masked_fill</code>。<br>
<code>torch.where</code>可以理解为if的张量版本。<br>
<code>torch.index_fill</code>的选取元素逻辑和<code>torch.index_select</code>相同。<br>
<code>torch.masked_fill</code>的选取元素逻辑和<code>torch.masked_select</code>相同。</p>
<pre class="highlight"><code class="python"><span class="hljs-comment">#如果分数大于60分，赋值成1，否则赋值成0</span>
ifpass = torch.where(scores&gt;<span class="hljs-number">60</span>,torch.tensor(<span class="hljs-number">1</span>),torch.tensor(<span class="hljs-number">0</span>))

<span class="hljs-comment"># 将每个班级第0个学生，第5个学生，第9个学生的全部成绩赋值成满分</span>
full = torch.index_fill(scores,dim = <span class="hljs-number">1</span>,index = torch.tensor([<span class="hljs-number">0</span>,<span class="hljs-number">5</span>,<span class="hljs-number">9</span>]),value = <span class="hljs-number">100</span>)

<span class="hljs-comment">#将分数小于60分的分数赋值成60分</span>
b = torch.masked_fill(scores,scores&lt;<span class="hljs-number">60</span>,<span class="hljs-number">60</span>)
</code></pre>
<h4 id="维度变换"><a class="markdownIt-Anchor" href="#维度变换"></a> 维度变换</h4>
<p>维度变换相关函数主要有<code>torch.reshape</code>(或者调用张量的<code>view()</code>方法), <code>torch.squeeze()</code>, <code>torch.unsqueeze()</code>, <code>torch.transpose()</code>。</p>
<ul>
<li>torch.reshape 可以改变张量的形状。</li>
<li>torch.squeeze 可以减少维度。</li>
<li>torch.unsqueeze 可以增加维度。</li>
<li>torch.transpose 可以交换维度。</li>
</ul>
<p>�张量的view方法有时候会调用失败，可以使用reshape方法。</p>
<pre class="highlight"><code class="python">minval,maxval = <span class="hljs-number">0</span>,<span class="hljs-number">255</span>
a = (minval + (maxval-minval)*torch.rand([<span class="hljs-number">1</span>,<span class="hljs-number">3</span>,<span class="hljs-number">3</span>,<span class="hljs-number">2</span>])).<span class="hljs-built_in">int</span>()
<span class="hljs-built_in">print</span>(a.shape)
<span class="hljs-built_in">print</span>(a)

<span class="hljs-comment"># 改成（3,6）形状的张量</span>
b = a.view([<span class="hljs-number">3</span>,<span class="hljs-number">6</span>]) <span class="hljs-comment"># torch.reshape(a,[3,6])</span>
<span class="hljs-built_in">print</span>(b.shape)
<span class="hljs-built_in">print</span>(b)

<span class="hljs-comment"># 改回成 [1,3,3,2] 形状的张量</span>
c = torch.reshape(b,[<span class="hljs-number">1</span>,<span class="hljs-number">3</span>,<span class="hljs-number">3</span>,<span class="hljs-number">2</span>]) <span class="hljs-comment"># b.view([1,3,3,2]) </span>

&gt;&gt;&gt;Output:
torch.Size([<span class="hljs-number">1</span>, <span class="hljs-number">3</span>, <span class="hljs-number">3</span>, <span class="hljs-number">2</span>])
tensor([[[[<span class="hljs-number">126</span>, <span class="hljs-number">195</span>],
          [ <span class="hljs-number">22</span>,  <span class="hljs-number">33</span>],
          [ <span class="hljs-number">78</span>, <span class="hljs-number">161</span>]],

         [[<span class="hljs-number">124</span>, <span class="hljs-number">228</span>],
          [<span class="hljs-number">116</span>, <span class="hljs-number">161</span>],
          [ <span class="hljs-number">88</span>, <span class="hljs-number">102</span>]],

         [[  <span class="hljs-number">5</span>,  <span class="hljs-number">43</span>],
          [ <span class="hljs-number">74</span>, <span class="hljs-number">132</span>],
          [<span class="hljs-number">177</span>, <span class="hljs-number">204</span>]]]], dtype=torch.int32)

torch.Size([<span class="hljs-number">3</span>, <span class="hljs-number">6</span>])
tensor([[<span class="hljs-number">126</span>, <span class="hljs-number">195</span>,  <span class="hljs-number">22</span>,  <span class="hljs-number">33</span>,  <span class="hljs-number">78</span>, <span class="hljs-number">161</span>],
        [<span class="hljs-number">124</span>, <span class="hljs-number">228</span>, <span class="hljs-number">116</span>, <span class="hljs-number">161</span>,  <span class="hljs-number">88</span>, <span class="hljs-number">102</span>],
        [  <span class="hljs-number">5</span>,  <span class="hljs-number">43</span>,  <span class="hljs-number">74</span>, <span class="hljs-number">132</span>, <span class="hljs-number">177</span>, <span class="hljs-number">204</span>]], dtype=torch.int32)
</code></pre>
<p>如果张量在某个维度上的维度是1，利用<code>torch.squeeze()</code>可以消除这个维度。<code>torch.unsqueeze()</code>的作用和<code>torch.squeeze()</code>的作用相反。</p>
<pre class="highlight"><code class="python">a = torch.tensor([[<span class="hljs-number">1.0</span>, <span class="hljs-number">2.0</span>]])
s = torch.squeeze(a)
<span class="hljs-built_in">print</span>(a)
<span class="hljs-built_in">print</span>(a.shape)
<span class="hljs-built_in">print</span>(s)
<span class="hljs-built_in">print</span>(s.shape)

&gt;&gt;&gt;Output:
tensor([[<span class="hljs-number">1.</span>, <span class="hljs-number">2.</span>]])
torch.Size([<span class="hljs-number">1</span>, <span class="hljs-number">2</span>])
tensor([<span class="hljs-number">1.</span>, <span class="hljs-number">2.</span>])
torch.Size([<span class="hljs-number">2</span>])
</code></pre>
<p>在张量的第0维插入一个维度</p>
<pre class="highlight"><code class="python">d = torch.unsqueeze(s,axis=<span class="hljs-number">0</span>)  

<span class="hljs-built_in">print</span>(d)
<span class="hljs-built_in">print</span>(d.shape)

&gt;&gt;&gt;Output:
tensor([[<span class="hljs-number">1.</span>, <span class="hljs-number">2.</span>]])
torch.Size([<span class="hljs-number">1</span>, <span class="hljs-number">2</span>])
</code></pre>
<p><code>torch.transpose()</code>可以交换张量的维度，<code>torch.transpose()</code>常用于图片存储格式的变换上。<br>
如果是二维的矩阵，通常会调用矩阵的转置方法 <code>matrix.t()</code>，等价于 <code>torch.transpose(matrix, 0, 1)</code>。</p>
<pre class="highlight"><code class="python">minval=<span class="hljs-number">0</span>
maxval=<span class="hljs-number">255</span>
<span class="hljs-comment"># Batch,Height,Width,Channel</span>
data = torch.floor(minval + (maxval-minval)*torch.rand([<span class="hljs-number">100</span>,<span class="hljs-number">256</span>,<span class="hljs-number">256</span>,<span class="hljs-number">4</span>])).<span class="hljs-built_in">int</span>()
<span class="hljs-built_in">print</span>(data.shape)

<span class="hljs-comment"># 转换成 Pytorch默认的图片格式 Batch,Channel,Height,Width </span>
<span class="hljs-comment"># 需要交换两次</span>
data_t = torch.transpose(torch.transpose(data,<span class="hljs-number">1</span>,<span class="hljs-number">2</span>),<span class="hljs-number">1</span>,<span class="hljs-number">3</span>)
<span class="hljs-built_in">print</span>(data_t.shape)

matrix = torch.tensor([[<span class="hljs-number">1</span>,<span class="hljs-number">2</span>,<span class="hljs-number">3</span>],[<span class="hljs-number">4</span>,<span class="hljs-number">5</span>,<span class="hljs-number">6</span>]])
<span class="hljs-built_in">print</span>(matrix)
<span class="hljs-built_in">print</span>(matrix.t()) <span class="hljs-comment"># 等价于torch.transpose(matrix,0,1)</span>

&gt;&gt;&gt;Output:
torch.Size([<span class="hljs-number">100</span>, <span class="hljs-number">256</span>, <span class="hljs-number">256</span>, <span class="hljs-number">4</span>])
torch.Size([<span class="hljs-number">100</span>, <span class="hljs-number">4</span>, <span class="hljs-number">256</span>, <span class="hljs-number">256</span>])
tensor([[<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>],
        [<span class="hljs-number">4</span>, <span class="hljs-number">5</span>, <span class="hljs-number">6</span>]])
tensor([[<span class="hljs-number">1</span>, <span class="hljs-number">4</span>],
        [<span class="hljs-number">2</span>, <span class="hljs-number">5</span>],
        [<span class="hljs-number">3</span>, <span class="hljs-number">6</span>]])
</code></pre>
<h4 id="合并分割"><a class="markdownIt-Anchor" href="#合并分割"></a> 合并分割</h4>
<p>可以用<code>torch.cat()</code>方法和<code>torch.stack()</code>方法将多个张量合并，可以用<code>torch.split()</code>方法把一个张量分割成多个张量。<br>
<code>torch.cat()</code>和<code>torch.stack()</code>有略微的区别：</p>
<ul>
<li>torch.cat是连接，不会增加维度</li>
<li>torch.stack是堆叠，会增加维度。</li>
</ul>
<p><strong>torch中dim和axis参数名可以混用�</strong>。</p>
<pre class="highlight"><code class="python">a = torch.tensor([[<span class="hljs-number">1.0</span>,<span class="hljs-number">2.0</span>],[<span class="hljs-number">3.0</span>,<span class="hljs-number">4.0</span>]])
b = torch.tensor([[<span class="hljs-number">5.0</span>,<span class="hljs-number">6.0</span>],[<span class="hljs-number">7.0</span>,<span class="hljs-number">8.0</span>]])
c = torch.tensor([[<span class="hljs-number">9.0</span>,<span class="hljs-number">10.0</span>],[<span class="hljs-number">11.0</span>,<span class="hljs-number">12.0</span>]])

abc_cat = torch.cat([a,b,c], dim = <span class="hljs-number">0</span>)
<span class="hljs-built_in">print</span>(abc_cat.shape)
<span class="hljs-built_in">print</span>(abc_cat)

<span class="hljs-comment"># torch中dim和axis参数名可以混用</span>
abc_stack = torch.stack([a,b,c], axis = <span class="hljs-number">0</span>) 
<span class="hljs-built_in">print</span>(abc_stack.shape)
<span class="hljs-built_in">print</span>(abc_stack)

&gt;&gt;&gt;Output:
torch.Size([<span class="hljs-number">6</span>, <span class="hljs-number">2</span>])
tensor([[ <span class="hljs-number">1.</span>,  <span class="hljs-number">2.</span>],
        [ <span class="hljs-number">3.</span>,  <span class="hljs-number">4.</span>],
        [ <span class="hljs-number">5.</span>,  <span class="hljs-number">6.</span>],
        [ <span class="hljs-number">7.</span>,  <span class="hljs-number">8.</span>],
        [ <span class="hljs-number">9.</span>, <span class="hljs-number">10.</span>],
        [<span class="hljs-number">11.</span>, <span class="hljs-number">12.</span>]])
torch.Size([<span class="hljs-number">3</span>, <span class="hljs-number">2</span>, <span class="hljs-number">2</span>])
tensor([[[ <span class="hljs-number">1.</span>,  <span class="hljs-number">2.</span>],
         [ <span class="hljs-number">3.</span>,  <span class="hljs-number">4.</span>]],

        [[ <span class="hljs-number">5.</span>,  <span class="hljs-number">6.</span>],
         [ <span class="hljs-number">7.</span>,  <span class="hljs-number">8.</span>]],

        [[ <span class="hljs-number">9.</span>, <span class="hljs-number">10.</span>],
         [<span class="hljs-number">11.</span>, <span class="hljs-number">12.</span>]]])
</code></pre>
<pre class="highlight"><code class="python">cat = torch.cat([a,b,c],axis = <span class="hljs-number">1</span>)
stack = torch.stack([a,b,c],axis = <span class="hljs-number">1</span>)

&gt;&gt;&gt;Output:
tensor([[ <span class="hljs-number">1.</span>,  <span class="hljs-number">2.</span>,  <span class="hljs-number">5.</span>,  <span class="hljs-number">6.</span>,  <span class="hljs-number">9.</span>, <span class="hljs-number">10.</span>],
        [ <span class="hljs-number">3.</span>,  <span class="hljs-number">4.</span>,  <span class="hljs-number">7.</span>,  <span class="hljs-number">8.</span>, <span class="hljs-number">11.</span>, <span class="hljs-number">12.</span>]])
tensor([[[ <span class="hljs-number">1.</span>,  <span class="hljs-number">2.</span>],
         [ <span class="hljs-number">5.</span>,  <span class="hljs-number">6.</span>],
         [ <span class="hljs-number">9.</span>, <span class="hljs-number">10.</span>]],

        [[ <span class="hljs-number">3.</span>,  <span class="hljs-number">4.</span>],
         [ <span class="hljs-number">7.</span>,  <span class="hljs-number">8.</span>],
         [<span class="hljs-number">11.</span>, <span class="hljs-number">12.</span>]]])
</code></pre>
<p><code>torch.split()</code>是<code>torch.cat()</code>的逆运算，可以指定分割份数平均分割，也可以通过指定每份的记录数量进行分割。�</p>
<pre class="highlight"><code class="python"><span class="hljs-built_in">print</span>(abc_cat)
a,b,c = torch.split(abc_cat,split_size_or_sections = <span class="hljs-number">2</span>,dim = <span class="hljs-number">0</span>) <span class="hljs-comment">#每份2个进行分割</span>
<span class="hljs-built_in">print</span>(a)
<span class="hljs-built_in">print</span>(b)
<span class="hljs-built_in">print</span>(c)

<span class="hljs-comment"># 每份分别为[4,1,1]</span>
p,q,r = torch.split(abc_cat,split_size_or_sections =[<span class="hljs-number">4</span>,<span class="hljs-number">1</span>,<span class="hljs-number">1</span>],dim = <span class="hljs-number">0</span>) 
<span class="hljs-built_in">print</span>(p)
<span class="hljs-built_in">print</span>(q)
<span class="hljs-built_in">print</span>(r)

&gt;&gt;&gt;Output:
tensor([[ <span class="hljs-number">1.</span>,  <span class="hljs-number">2.</span>],
        [ <span class="hljs-number">3.</span>,  <span class="hljs-number">4.</span>],
        [ <span class="hljs-number">5.</span>,  <span class="hljs-number">6.</span>],
        [ <span class="hljs-number">7.</span>,  <span class="hljs-number">8.</span>],
        [ <span class="hljs-number">9.</span>, <span class="hljs-number">10.</span>],
        [<span class="hljs-number">11.</span>, <span class="hljs-number">12.</span>]])
tensor([[<span class="hljs-number">1.</span>, <span class="hljs-number">2.</span>],
        [<span class="hljs-number">3.</span>, <span class="hljs-number">4.</span>]])
tensor([[<span class="hljs-number">5.</span>, <span class="hljs-number">6.</span>],
        [<span class="hljs-number">7.</span>, <span class="hljs-number">8.</span>]])
tensor([[ <span class="hljs-number">9.</span>, <span class="hljs-number">10.</span>],
        [<span class="hljs-number">11.</span>, <span class="hljs-number">12.</span>]])

tensor([[<span class="hljs-number">1.</span>, <span class="hljs-number">2.</span>],
        [<span class="hljs-number">3.</span>, <span class="hljs-number">4.</span>],
        [<span class="hljs-number">5.</span>, <span class="hljs-number">6.</span>],
        [<span class="hljs-number">7.</span>, <span class="hljs-number">8.</span>]])
tensor([[ <span class="hljs-number">9.</span>, <span class="hljs-number">10.</span>]])
tensor([[<span class="hljs-number">11.</span>, <span class="hljs-number">12.</span>]])
</code></pre>
<h3 id="张量的数学运算"><a class="markdownIt-Anchor" href="#张量的数学运算"></a> 张量的数学运算</h3>
<p>张量的数学运算符可以分为标量运算符、向量运算符、以及矩阵运算符。</p>
<h4 id="标量运算"><a class="markdownIt-Anchor" href="#标量运算"></a> 标量运算</h4>
<p>加减乘除乘方，以及三角函数，指数，对数等常见函数，逻辑比较运算符等都是标量运算符。<br>
<strong>标量运算符的特点是对张量实施逐元素运算</strong>。有些标量运算符对常用的数学运算符进行了重载。并且支持类似numpy的广播特性。</p>
<pre class="highlight"><code class="python">a = torch.tensor([[<span class="hljs-number">1.0</span>,<span class="hljs-number">2</span>],[-<span class="hljs-number">3</span>,<span class="hljs-number">4.0</span>]])
b = torch.tensor([[<span class="hljs-number">5.0</span>,<span class="hljs-number">6</span>],[<span class="hljs-number">7.0</span>,<span class="hljs-number">8.0</span>]])

<span class="hljs-comment"># 逐元素运算</span>
a+b  <span class="hljs-comment">#运算符重载</span>
a-b 
a*b 
a/b
a**<span class="hljs-number">2</span>
a**<span class="hljs-number">0.5</span>
a%<span class="hljs-number">3</span>
a//<span class="hljs-number">3</span>
torch.sqrt(a)

<span class="hljs-built_in">print</span>(a&gt;=<span class="hljs-number">2</span>) <span class="hljs-comment"># torch.ge(a,2)  #ge: greater_equal缩写</span>
<span class="hljs-built_in">print</span>((a&gt;=<span class="hljs-number">2</span>)&amp;(a&lt;=<span class="hljs-number">3</span>))
<span class="hljs-built_in">print</span>((a&gt;=<span class="hljs-number">2</span>)|(a&lt;=<span class="hljs-number">3</span>))
<span class="hljs-built_in">print</span>(a==<span class="hljs-number">5</span>) <span class="hljs-comment"># torch.eq(a,5)  # eq: equal缩写</span>

&gt;&gt;&gt;Output:
tensor([[<span class="hljs-literal">False</span>,  <span class="hljs-literal">True</span>],
        [<span class="hljs-literal">False</span>,  <span class="hljs-literal">True</span>]])
tensor([[<span class="hljs-literal">False</span>,  <span class="hljs-literal">True</span>],
        [<span class="hljs-literal">False</span>, <span class="hljs-literal">False</span>]])
tensor([[<span class="hljs-literal">True</span>, <span class="hljs-literal">True</span>],
        [<span class="hljs-literal">True</span>, <span class="hljs-literal">True</span>]])
tensor([[<span class="hljs-literal">False</span>, <span class="hljs-literal">False</span>],
        [<span class="hljs-literal">False</span>, <span class="hljs-literal">False</span>]])
</code></pre>
<pre class="highlight"><code class="python">a = torch.tensor([<span class="hljs-number">1.0</span>,<span class="hljs-number">8.0</span>])
b = torch.tensor([<span class="hljs-number">5.0</span>,<span class="hljs-number">6.0</span>])
c = torch.tensor([<span class="hljs-number">6.0</span>,<span class="hljs-number">7.0</span>])

d = a+b+c
<span class="hljs-built_in">print</span>(d)
<span class="hljs-built_in">print</span>(torch.<span class="hljs-built_in">max</span>(a,b))
<span class="hljs-built_in">print</span>(torch.<span class="hljs-built_in">min</span>(a,b))

&gt;&gt;&gt;Output:
tensor([<span class="hljs-number">12.</span>, <span class="hljs-number">21.</span>])
tensor([<span class="hljs-number">5.</span>, <span class="hljs-number">8.</span>])
tensor([<span class="hljs-number">1.</span>, <span class="hljs-number">6.</span>])
</code></pre>
<p><code>torch.round()</code>：保留整数部分，四舍五入。<br>
<code>torch.floor()</code>：保留整数部分，向下取整。<br>
<code>torch.ceil()</code>：保留整数部分，向上取整。<br>
<code>torch.trunc()</code>：保留整数部分，向0归整。<br>
<code>torch.fmod()</code>：作除法取余数。<br>
<code>torch.remainder()</code>：作除法取剩余的部分，结果恒正。</p>
<pre class="highlight"><code class="python">x = torch.tensor([<span class="hljs-number">2.6</span>,-<span class="hljs-number">2.7</span>])

<span class="hljs-built_in">print</span>(torch.<span class="hljs-built_in">round</span>(x)) 
<span class="hljs-built_in">print</span>(torch.floor(x)) 
<span class="hljs-built_in">print</span>(torch.ceil(x))  
<span class="hljs-built_in">print</span>(torch.trunc(x)) 
<span class="hljs-built_in">print</span>(torch.fmod(x,<span class="hljs-number">2</span>)) 
<span class="hljs-built_in">print</span>(torch.remainder(x,<span class="hljs-number">2</span>)) 

&gt;&gt;&gt;Output:
tensor([ <span class="hljs-number">3.</span>, -<span class="hljs-number">3.</span>])
tensor([ <span class="hljs-number">2.</span>, -<span class="hljs-number">3.</span>])
tensor([ <span class="hljs-number">3.</span>, -<span class="hljs-number">2.</span>])
tensor([ <span class="hljs-number">2.</span>, -<span class="hljs-number">2.</span>])
tensor([ <span class="hljs-number">0.6000</span>, -<span class="hljs-number">0.7000</span>])
tensor([<span class="hljs-number">0.6000</span>, <span class="hljs-number">1.3000</span>])
</code></pre>
<pre class="highlight"><code class="python">x = torch.tensor([<span class="hljs-number">0.9</span>,-<span class="hljs-number">0.8</span>,<span class="hljs-number">100.0</span>,-<span class="hljs-number">20.0</span>,<span class="hljs-number">0.7</span>])
y = torch.clamp(x,<span class="hljs-built_in">min</span>=-<span class="hljs-number">1</span>,<span class="hljs-built_in">max</span> = <span class="hljs-number">1</span>)
z = torch.clamp(x,<span class="hljs-built_in">max</span> = <span class="hljs-number">1</span>)
<span class="hljs-built_in">print</span>(y)
<span class="hljs-built_in">print</span>(z)

&gt;&gt;&gt;Output:
tensor([ <span class="hljs-number">0.9000</span>, -<span class="hljs-number">0.8000</span>,  <span class="hljs-number">1.0000</span>, -<span class="hljs-number">1.0000</span>,  <span class="hljs-number">0.7000</span>])
tensor([  <span class="hljs-number">0.9000</span>,  -<span class="hljs-number">0.8000</span>,   <span class="hljs-number">1.0000</span>, -<span class="hljs-number">20.0000</span>,   <span class="hljs-number">0.7000</span>])
</code></pre>
<h4 id="向量运算"><a class="markdownIt-Anchor" href="#向量运算"></a> 向量运算</h4>
<p>向量运算符只在一个特定轴上运算，将一个向量映射到一个标量或者另外一个向量。�</p>
<pre class="highlight"><code class="python">a = torch.arange(<span class="hljs-number">1</span>,<span class="hljs-number">10</span>).<span class="hljs-built_in">float</span>()
<span class="hljs-built_in">print</span>(torch.<span class="hljs-built_in">sum</span>(a))
<span class="hljs-built_in">print</span>(torch.mean(a))
<span class="hljs-built_in">print</span>(torch.<span class="hljs-built_in">max</span>(a))
<span class="hljs-built_in">print</span>(torch.<span class="hljs-built_in">min</span>(a))
<span class="hljs-built_in">print</span>(torch.prod(a)) <span class="hljs-comment">#累乘</span>
<span class="hljs-built_in">print</span>(torch.std(a))  <span class="hljs-comment">#标准差</span>
<span class="hljs-built_in">print</span>(torch.var(a))  <span class="hljs-comment">#方差</span>
<span class="hljs-built_in">print</span>(torch.median(a)) <span class="hljs-comment">#中位数</span>

&gt;&gt;&gt;Output:
tensor(<span class="hljs-number">45.</span>)
tensor(<span class="hljs-number">5.</span>)
tensor(<span class="hljs-number">9.</span>)
tensor(<span class="hljs-number">1.</span>)
tensor(<span class="hljs-number">362880.</span>)
tensor(<span class="hljs-number">2.7386</span>)
tensor(<span class="hljs-number">7.5000</span>)
tensor(<span class="hljs-number">5.</span>)
</code></pre>
<pre class="highlight"><code class="python">b = a.view(<span class="hljs-number">3</span>,<span class="hljs-number">3</span>)
<span class="hljs-built_in">print</span>(b)
<span class="hljs-built_in">print</span>(torch.<span class="hljs-built_in">max</span>(b,dim = <span class="hljs-number">0</span>))
<span class="hljs-built_in">print</span>(torch.<span class="hljs-built_in">max</span>(b,dim = <span class="hljs-number">1</span>))

&gt;&gt;&gt;Output:
tensor([[<span class="hljs-number">1.</span>, <span class="hljs-number">2.</span>, <span class="hljs-number">3.</span>],
        [<span class="hljs-number">4.</span>, <span class="hljs-number">5.</span>, <span class="hljs-number">6.</span>],
        [<span class="hljs-number">7.</span>, <span class="hljs-number">8.</span>, <span class="hljs-number">9.</span>]])
torch.return_types.<span class="hljs-built_in">max</span>(
values=tensor([<span class="hljs-number">7.</span>, <span class="hljs-number">8.</span>, <span class="hljs-number">9.</span>]),
indices=tensor([<span class="hljs-number">2</span>, <span class="hljs-number">2</span>, <span class="hljs-number">2</span>]))
torch.return_types.<span class="hljs-built_in">max</span>(
values=tensor([<span class="hljs-number">3.</span>, <span class="hljs-number">6.</span>, <span class="hljs-number">9.</span>]),
indices=tensor([<span class="hljs-number">2</span>, <span class="hljs-number">2</span>, <span class="hljs-number">2</span>]))
</code></pre>
<pre class="highlight"><code class="python">a = torch.arange(<span class="hljs-number">1</span>,<span class="hljs-number">10</span>)

<span class="hljs-built_in">print</span>(torch.cumsum(a,<span class="hljs-number">0</span>))
<span class="hljs-built_in">print</span>(torch.cumprod(a,<span class="hljs-number">0</span>))
<span class="hljs-built_in">print</span>(torch.cummax(a,<span class="hljs-number">0</span>).values)
<span class="hljs-built_in">print</span>(torch.cummax(a,<span class="hljs-number">0</span>).indices)
<span class="hljs-built_in">print</span>(torch.cummin(a,<span class="hljs-number">0</span>))

&gt;&gt;&gt;Output:
tensor([ <span class="hljs-number">1</span>,  <span class="hljs-number">3</span>,  <span class="hljs-number">6</span>, <span class="hljs-number">10</span>, <span class="hljs-number">15</span>, <span class="hljs-number">21</span>, <span class="hljs-number">28</span>, <span class="hljs-number">36</span>, <span class="hljs-number">45</span>])
tensor([     <span class="hljs-number">1</span>,      <span class="hljs-number">2</span>,      <span class="hljs-number">6</span>,     <span class="hljs-number">24</span>,    <span class="hljs-number">120</span>,    <span class="hljs-number">720</span>,   <span class="hljs-number">5040</span>,  <span class="hljs-number">40320</span>, <span class="hljs-number">362880</span>])
tensor([<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-number">4</span>, <span class="hljs-number">5</span>, <span class="hljs-number">6</span>, <span class="hljs-number">7</span>, <span class="hljs-number">8</span>, <span class="hljs-number">9</span>])
tensor([<span class="hljs-number">0</span>, <span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-number">4</span>, <span class="hljs-number">5</span>, <span class="hljs-number">6</span>, <span class="hljs-number">7</span>, <span class="hljs-number">8</span>])
torch.return_types.cummin(
values=tensor([<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>]),
indices=tensor([<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>]))
</code></pre>
<p><code>torch.sort()</code>和<code>torch.topk()</code>可以对张量排序。利用<code>torch.topk()</code>可以在Pytorch中实现KNN算法。</p>
<pre class="highlight"><code class="python">a = torch.tensor([[<span class="hljs-number">9</span>,<span class="hljs-number">7</span>,<span class="hljs-number">8</span>],[<span class="hljs-number">1</span>,<span class="hljs-number">3</span>,<span class="hljs-number">2</span>],[<span class="hljs-number">5</span>,<span class="hljs-number">6</span>,<span class="hljs-number">4</span>]]).<span class="hljs-built_in">float</span>()
<span class="hljs-built_in">print</span>(torch.topk(a,<span class="hljs-number">2</span>,dim = <span class="hljs-number">0</span>),<span class="hljs-string">&quot;\n&quot;</span>)
<span class="hljs-built_in">print</span>(torch.topk(a,<span class="hljs-number">2</span>,dim = <span class="hljs-number">1</span>),<span class="hljs-string">&quot;\n&quot;</span>)
<span class="hljs-built_in">print</span>(torch.sort(a,dim = <span class="hljs-number">1</span>),<span class="hljs-string">&quot;\n&quot;</span>)

&gt;&gt;&gt;Output:
torch.return_types.topk(
values=tensor([[<span class="hljs-number">9.</span>, <span class="hljs-number">7.</span>, <span class="hljs-number">8.</span>],
        [<span class="hljs-number">5.</span>, <span class="hljs-number">6.</span>, <span class="hljs-number">4.</span>]]),
indices=tensor([[<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>],
        [<span class="hljs-number">2</span>, <span class="hljs-number">2</span>, <span class="hljs-number">2</span>]])) 

torch.return_types.topk(
values=tensor([[<span class="hljs-number">9.</span>, <span class="hljs-number">8.</span>],
        [<span class="hljs-number">3.</span>, <span class="hljs-number">2.</span>],
        [<span class="hljs-number">6.</span>, <span class="hljs-number">5.</span>]]),
indices=tensor([[<span class="hljs-number">0</span>, <span class="hljs-number">2</span>],
        [<span class="hljs-number">1</span>, <span class="hljs-number">2</span>],
        [<span class="hljs-number">1</span>, <span class="hljs-number">0</span>]])) 

torch.return_types.sort(
values=tensor([[<span class="hljs-number">7.</span>, <span class="hljs-number">8.</span>, <span class="hljs-number">9.</span>],
        [<span class="hljs-number">1.</span>, <span class="hljs-number">2.</span>, <span class="hljs-number">3.</span>],
        [<span class="hljs-number">4.</span>, <span class="hljs-number">5.</span>, <span class="hljs-number">6.</span>]]),
indices=tensor([[<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">0</span>],
        [<span class="hljs-number">0</span>, <span class="hljs-number">2</span>, <span class="hljs-number">1</span>],
        [<span class="hljs-number">2</span>, <span class="hljs-number">0</span>, <span class="hljs-number">1</span>]])) 
</code></pre>
<h4 id="矩阵运算"><a class="markdownIt-Anchor" href="#矩阵运算"></a> 矩阵运算</h4>
<p>矩阵必须是二维的。<br>
矩阵运算包括：矩阵乘法，矩阵转置，矩阵逆，矩阵求迹，矩阵范数，矩阵行列式，矩阵求特征值，矩阵分解等运算。</p>
<pre class="highlight"><code class="python">a = torch.tensor([[<span class="hljs-number">1</span>,<span class="hljs-number">2</span>],[<span class="hljs-number">3</span>,<span class="hljs-number">4</span>]])
b = torch.tensor([[<span class="hljs-number">2</span>,<span class="hljs-number">0</span>],[<span class="hljs-number">0</span>,<span class="hljs-number">2</span>]])

<span class="hljs-comment"># 下列均等价</span>
torch.matmul(a,b)
torch.mm(a,b)
<span class="hljs-built_in">print</span>(a@b)

&gt;&gt;&gt;Output:
tensor([[<span class="hljs-number">2</span>, <span class="hljs-number">4</span>],
        [<span class="hljs-number">6</span>, <span class="hljs-number">8</span>]])
</code></pre>
<pre class="highlight"><code class="python">a = torch.tensor([[<span class="hljs-number">1.0</span>,<span class="hljs-number">2</span>],[<span class="hljs-number">3</span>,<span class="hljs-number">4</span>]])
<span class="hljs-built_in">print</span>(a.t())

&gt;&gt;&gt;Output:
tensor([[<span class="hljs-number">1.</span>, <span class="hljs-number">3.</span>],
        [<span class="hljs-number">2.</span>, <span class="hljs-number">4.</span>]])
</code></pre>
<p>矩阵求逆必须是浮点数类型。</p>
<pre class="highlight"><code class="python"><span class="hljs-comment"># 必须为浮点类型</span>
a = torch.tensor([[<span class="hljs-number">1.0</span>,<span class="hljs-number">2</span>],[<span class="hljs-number">3</span>,<span class="hljs-number">4</span>]])
<span class="hljs-built_in">print</span>(torch.inverse(a))

&gt;&gt;&gt;Output:
tensor([[-<span class="hljs-number">2.0000</span>,  <span class="hljs-number">1.0000</span>],
        [ <span class="hljs-number">1.5000</span>, -<span class="hljs-number">0.5000</span>]])
</code></pre>
<pre class="highlight"><code class="python">a = torch.tensor([[<span class="hljs-number">1.0</span>,<span class="hljs-number">2</span>],[<span class="hljs-number">3</span>,<span class="hljs-number">4</span>]])
<span class="hljs-built_in">print</span>(torch.trace(a))

&gt;&gt;&gt;Output:
tensor(<span class="hljs-number">5.</span>)
</code></pre>
<pre class="highlight"><code class="python">a = torch.tensor([[<span class="hljs-number">1.0</span>,<span class="hljs-number">2</span>],[<span class="hljs-number">3</span>,<span class="hljs-number">4</span>]])
<span class="hljs-built_in">print</span>(torch.norm(a))

&gt;&gt;&gt;Output:
tensor(<span class="hljs-number">5.4772</span>)
</code></pre>
<pre class="highlight"><code class="python">a = torch.tensor([[<span class="hljs-number">1.0</span>,<span class="hljs-number">2</span>],[<span class="hljs-number">3</span>,<span class="hljs-number">4</span>]])
<span class="hljs-built_in">print</span>(torch.det(a))

&gt;&gt;&gt;Output:
tensor(-<span class="hljs-number">2.0000</span>)
</code></pre>
<pre class="highlight"><code class="python">a = torch.tensor([[<span class="hljs-number">1.0</span>,<span class="hljs-number">2</span>],[-<span class="hljs-number">5</span>,<span class="hljs-number">4</span>]],dtype = torch.<span class="hljs-built_in">float</span>)
<span class="hljs-built_in">print</span>(torch.eig(a,eigenvectors=<span class="hljs-literal">True</span>))
<span class="hljs-comment"># 两个特征值分别是 -2.5+2.7839j, 2.5-2.7839j </span>

&gt;&gt;&gt;Output:
torch.return_types.eig(
eigenvalues=tensor([[ <span class="hljs-number">2.5000</span>,  <span class="hljs-number">2.7839</span>],
        [ <span class="hljs-number">2.5000</span>, -<span class="hljs-number">2.7839</span>]]),
eigenvectors=tensor([[ <span class="hljs-number">0.2535</span>, -<span class="hljs-number">0.4706</span>],
        [ <span class="hljs-number">0.8452</span>,  <span class="hljs-number">0.0000</span>]]))
</code></pre>
<p>矩阵QR分解，将一个方阵分解为一个正交矩阵q和上三角矩阵r。<br>
QR分解实际上是对矩阵a实施Schmidt正交化得到q。</p>
<pre class="highlight"><code class="python">a = torch.tensor([[<span class="hljs-number">1.0</span>,<span class="hljs-number">2.0</span>],[<span class="hljs-number">3.0</span>,<span class="hljs-number">4.0</span>]])
q,r = torch.qr(a)
<span class="hljs-built_in">print</span>(q,<span class="hljs-string">&quot;\n&quot;</span>)
<span class="hljs-built_in">print</span>(r,<span class="hljs-string">&quot;\n&quot;</span>)
<span class="hljs-built_in">print</span>(q@r)

&gt;&gt;&gt;Output:
tensor([[-<span class="hljs-number">0.3162</span>, -<span class="hljs-number">0.9487</span>],
        [-<span class="hljs-number">0.9487</span>,  <span class="hljs-number">0.3162</span>]]) 
tensor([[-<span class="hljs-number">3.1623</span>, -<span class="hljs-number">4.4272</span>],
        [ <span class="hljs-number">0.0000</span>, -<span class="hljs-number">0.6325</span>]]) 
tensor([[<span class="hljs-number">1.0000</span>, <span class="hljs-number">2.0000</span>],
        [<span class="hljs-number">3.0000</span>, <span class="hljs-number">4.0000</span>]])
</code></pre>
<p>矩阵svd分解，svd分解可以将任意一个矩阵分解为一个正交矩阵u，一个对角阵s和一个正交矩阵<code>v.t()</code>的乘积。svd常用于矩阵压缩和降维，利用svd分解可以在Pytorch中实现主成分分析降维。</p>
<pre class="highlight"><code class="python">a=torch.tensor([[<span class="hljs-number">1.0</span>,<span class="hljs-number">2.0</span>],[<span class="hljs-number">3.0</span>,<span class="hljs-number">4.0</span>],[<span class="hljs-number">5.0</span>,<span class="hljs-number">6.0</span>]])
u,s,v = torch.svd(a)

<span class="hljs-built_in">print</span>(u,<span class="hljs-string">&quot;\n&quot;</span>)
<span class="hljs-built_in">print</span>(s,<span class="hljs-string">&quot;\n&quot;</span>)
<span class="hljs-built_in">print</span>(v,<span class="hljs-string">&quot;\n&quot;</span>)
<span class="hljs-built_in">print</span>(u@torch.diag(s)@v.t())

&gt;&gt;&gt;Output:
tensor([[-<span class="hljs-number">0.2298</span>,  <span class="hljs-number">0.8835</span>],
        [-<span class="hljs-number">0.5247</span>,  <span class="hljs-number">0.2408</span>],
        [-<span class="hljs-number">0.8196</span>, -<span class="hljs-number">0.4019</span>]]) 

tensor([<span class="hljs-number">9.5255</span>, <span class="hljs-number">0.5143</span>]) 

tensor([[-<span class="hljs-number">0.6196</span>, -<span class="hljs-number">0.7849</span>],
        [-<span class="hljs-number">0.7849</span>,  <span class="hljs-number">0.6196</span>]]) 

tensor([[<span class="hljs-number">1.0000</span>, <span class="hljs-number">2.0000</span>],
        [<span class="hljs-number">3.0000</span>, <span class="hljs-number">4.0000</span>],
        [<span class="hljs-number">5.0000</span>, <span class="hljs-number">6.0000</span>]])
</code></pre>
<h4 id="广播机制"><a class="markdownIt-Anchor" href="#广播机制"></a> 广播机制</h4>
<p>Pytorch的广播规则和numpy是一样的:</p>
<ol>
<li>如果张量的维度不同，将维度较小的张量进行扩展，直到两个张量的维度都一样。</li>
<li>如果两个张量在某个维度上的长度是相同的，或者其中一个张量在该维度上的长度为1，那么我们就说这两个张量在该维度上是相容的。</li>
<li>如果两个张量在所有维度上都是相容的，它们就能使用广播。</li>
<li>广播之后，每个维度的长度将取两个张量在该维度长度的较大值。</li>
<li>在任何一个维度上，如果一个张量的长度为1，另一个张量长度大于1，那么在该维度上，就好像是对第一个张量进行了复制。</li>
</ol>
<p><code>torch.broadcast_tensors()</code>可以将多个张量根据广播规则转换成相同的维度。</p>
<pre class="highlight"><code class="python">a = torch.tensor([<span class="hljs-number">1</span>,<span class="hljs-number">2</span>,<span class="hljs-number">3</span>])
b = torch.tensor([[<span class="hljs-number">0</span>,<span class="hljs-number">0</span>,<span class="hljs-number">0</span>],[<span class="hljs-number">1</span>,<span class="hljs-number">1</span>,<span class="hljs-number">1</span>],[<span class="hljs-number">2</span>,<span class="hljs-number">2</span>,<span class="hljs-number">2</span>]])
<span class="hljs-built_in">print</span>(b + a) 

a_broad,b_broad = torch.broadcast_tensors(a,b)
<span class="hljs-built_in">print</span>(a_broad)
<span class="hljs-built_in">print</span>(b_broad)
<span class="hljs-built_in">print</span>(a_broad + b_broad) 

&gt;&gt;&gt;Output:
tensor([[<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>],
        [<span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-number">4</span>],
        [<span class="hljs-number">3</span>, <span class="hljs-number">4</span>, <span class="hljs-number">5</span>]])
tensor([[<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>],
        [<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>],
        [<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>]]) 
tensor([[<span class="hljs-number">0</span>, <span class="hljs-number">0</span>, <span class="hljs-number">0</span>],
        [<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>],
        [<span class="hljs-number">2</span>, <span class="hljs-number">2</span>, <span class="hljs-number">2</span>]]) 
tensor([[<span class="hljs-number">1</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>],
        [<span class="hljs-number">2</span>, <span class="hljs-number">3</span>, <span class="hljs-number">4</span>],
        [<span class="hljs-number">3</span>, <span class="hljs-number">4</span>, <span class="hljs-number">5</span>]])
</code></pre>

                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/about" rel="external nofollow noreferrer">MerryTong</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://github.com/merryTong/merryTong.github.io/2023/04/01/pytorch-ji-chu-zhi-shi/01-zhang-liang-he-zhang-liang-cao-zuo/">https://github.com/merryTong/merryTong.github.io/2023/04/01/pytorch-ji-chu-zhi-shi/01-zhang-liang-he-zhang-liang-cao-zuo/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/about" target="_blank">MerryTong</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/tags/基础知识/">
                                    <span class="chip bg-color">基础知识</span>
                                </a>
                            
                                <a href="/tags/Pytorch/">
                                    <span class="chip bg-color">Pytorch</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
                <div id="reward">
    <a href="#rewardModal" class="reward-link modal-trigger btn-floating btn-medium waves-effect waves-light red">赏</a>

    <!-- Modal Structure -->
    <div id="rewardModal" class="modal">
        <div class="modal-content">
            <a class="close modal-close"><i class="fas fa-times"></i></a>
            <h4 class="reward-title">你的赏识是我前进的动力</h4>
            <div class="reward-content">
                <div class="reward-tabs">
                    <ul class="tabs row">
                        <li class="tab col s6 alipay-tab waves-effect waves-light"><a href="#alipay">支付宝</a></li>
                        <li class="tab col s6 wechat-tab waves-effect waves-light"><a href="#wechat">微 信</a></li>
                    </ul>
                    <div id="alipay">
                        <img src="/" class="reward-img" alt="支付宝打赏二维码">
                    </div>
                    <div id="wechat">
                        <img src="/" class="reward-img" alt="微信打赏二维码">
                    </div>
                </div>
            </div>
        </div>
    </div>
</div>

<script>
    $(function () {
        $('.tabs').tabs();
    });
</script>

            
        </div>
    </div>

    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/2023/04/05/pytorch-ji-chu-zhi-shi/02-gou-jian-mo-xing-de-3-chong-fang-shi/">
                    <div class="card-image">
                        
                        
                        <img src="/medias/featureimages/animal/5.jpg" class="responsive-img" alt="Pytorch|构建模型的3种方式">
                        
                        <span class="card-title">Pytorch|构建模型的3种方式</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2023-04-05
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/categories/Pytorch/" class="post-category">
                                    Pytorch
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/tags/基础知识/">
                        <span class="chip bg-color">基础知识</span>
                    </a>
                    
                    <a href="/tags/Pytorch/">
                        <span class="chip bg-color">Pytorch</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/2023/03/23/pytorch-ji-chu-zhi-shi/shen-du-xue-xi-xun-lian-dai-ma-mo-ban/">
                    <div class="card-image">
                        
                        
                        <img src="/medias/featureimages/architect/11.jpg" class="responsive-img" alt="Pytorch训练代码模板">
                        
                        <span class="card-title">Pytorch训练代码模板</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2023-03-23
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/categories/Pytorch/" class="post-category">
                                    Pytorch
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/tags/Pytorch/">
                        <span class="chip bg-color">Pytorch</span>
                    </a>
                    
                    <a href="/tags/代码模板/">
                        <span class="chip bg-color">代码模板</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>



<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/libs/codeBlock/codeBlockFuction.js"></script>



<!-- 代码语言 -->

<script type="text/javascript" src="/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/libs/codeBlock/codeShrink.js"></script>



    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>

    

</main>


<script src="https://cdn.bootcss.com/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
<script>
    MathJax.Hub.Config({
        tex2jax: {inlineMath: [['$', '$'], ['\\(', '\\)']]}
    });
</script>





    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="503838841"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/libs/aplayer/APlayer.min.js"></script>
<script src="/libs/aplayer/Meting.min.js"></script>

    

    <div class="container row center-align"
         style="margin-bottom: 15px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2023-2024</span>
            
            <a href="/about" target="_blank">MerryTong</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            <br>
            
            
            
                
            
            
                <span id="busuanzi_container_site_pv">
                &nbsp;|&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;
                    <span id="busuanzi_value_site_pv" class="white-color"></span>
            </span>
            
            
                <span id="busuanzi_container_site_uv">
                &nbsp;|&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;
                    <span id="busuanzi_value_site_uv" class="white-color"></span>
            </span>
            
            <br>

            <!-- 运行天数提醒. -->
            
                <span id="sitetime"> Loading ...</span>
                <script>
                    var calcSiteTime = function () {
                        var seconds = 1000;
                        var minutes = seconds * 60;
                        var hours = minutes * 60;
                        var days = hours * 24;
                        var years = days * 365;
                        var today = new Date();
                        var startYear = "2023";
                        var startMonth = "5";
                        var startDate = "3";
                        var startHour = "0";
                        var startMinute = "0";
                        var startSecond = "0";
                        var todayYear = today.getFullYear();
                        var todayMonth = today.getMonth() + 1;
                        var todayDate = today.getDate();
                        var todayHour = today.getHours();
                        var todayMinute = today.getMinutes();
                        var todaySecond = today.getSeconds();
                        var t1 = Date.UTC(startYear, startMonth, startDate, startHour, startMinute, startSecond);
                        var t2 = Date.UTC(todayYear, todayMonth, todayDate, todayHour, todayMinute, todaySecond);
                        var diff = t2 - t1;
                        var diffYears = Math.floor(diff / years);
                        var diffDays = Math.floor((diff / days) - diffYears * 365);

                        // 区分是否有年份.
                        var language = 'zh-CN';
                        if (startYear === String(todayYear)) {
                            document.getElementById("year").innerHTML = todayYear;
                            var daysTip = 'This site has been running for ' + diffDays + ' days';
                            if (language === 'zh-CN') {
                                daysTip = '本站已运行 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                daysTip = '本站已運行 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = daysTip;
                        } else {
                            document.getElementById("year").innerHTML = startYear + " - " + todayYear;
                            var yearsAndDaysTip = 'This site has been running for ' + diffYears + ' years and '
                                + diffDays + ' days';
                            if (language === 'zh-CN') {
                                yearsAndDaysTip = '本站已运行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            } else if (language === 'zh-HK') {
                                yearsAndDaysTip = '本站已運行 ' + diffYears + ' 年 ' + diffDays + ' 天';
                            }
                            document.getElementById("sitetime").innerHTML = yearsAndDaysTip;
                        }
                    }

                    calcSiteTime();
                </script>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/merryTong" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:1779306478@qq.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>







    <a href="tencent://AddContact/?fromId=50&fromSubId=1&subcmd=all&uin=1779306478" class="tooltipped" target="_blank" data-tooltip="QQ联系我: 1779306478" data-position="top" data-delay="50">
        <i class="fab fa-qq"></i>
    </a>







    <a href="/atom.xml" class="tooltipped" target="_blank" data-tooltip="RSS 订阅" data-position="top" data-delay="50">
        <i class="fas fa-rss"></i>
    </a>

</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 白天和黑夜主题 -->
<div class="stars-con">
    <div id="stars"></div>
    <div id="stars2"></div>
    <div id="stars3"></div>  
</div>

<script>
    function switchNightMode() {
        $('<div class="Cuteen_DarkSky"><div class="Cuteen_DarkPlanet"></div></div>').appendTo($('body')),
        setTimeout(function () {
            $('body').hasClass('DarkMode') 
            ? ($('body').removeClass('DarkMode'), localStorage.setItem('isDark', '0'), $('#sum-moon-icon').removeClass("fa-sun").addClass('fa-moon')) 
            : ($('body').addClass('DarkMode'), localStorage.setItem('isDark', '1'), $('#sum-moon-icon').addClass("fa-sun").removeClass('fa-moon')),
            
            setTimeout(function () {
            $('.Cuteen_DarkSky').fadeOut(1e3, function () {
                $(this).remove()
            })
            }, 2e3)
        })
    }
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/libs/materialize/materialize.min.js"></script>
    <script src="/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/libs/aos/aos.js"></script>
    <script src="/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/js/matery.js"></script>

    

    
        
        <script type="text/javascript">
            // 只在桌面版网页启用特效
            var windowWidth = $(window).width();
            if (windowWidth > 10) {
                document.write('<script type="text/javascript" src="/libs/others/sakura.js"><\/script>');
            }
        </script>
    

    <!-- 雪花特效 -->
    
    <!-- 雪花特效 -->
    

    <!-- 鼠标星星特效 -->
    

     
        <script src="https://ssl.captcha.qq.com/TCaptcha.js"></script>
        <script src="/libs/others/TencentCaptcha.js"></script>
        <button id="TencentCaptcha" data-appid="xxxxxxxxxx" data-cbfn="callback" type="button" hidden></button>
    

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

    <!--腾讯兔小巢-->
    
    

    
    
    <script type="text/javascript" size="150" alpha='0.6'
        zIndex="-1" src="/libs/background/ribbon-refresh.min.js" async="async"></script>
    

    

    
    <script src="/libs/instantpage/instantpage.js" type="module"></script>
    

<script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"pluginRootPath":"live2dw/","pluginJsPath":"lib/","pluginModelPath":"assets/","tagMode":false,"log":false,"model":{"jsonPath":"/live2dw/assets/shizuku.model.json"},"display":{"position":"left","width":200,"height":400},"mobile":{"show":false},"react":{"opacity":0.7}});</script></body>

</html>
