<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title></title>
      <link href="/2023/05/28/pytorch-duo-qia-xun-lian/"/>
      <url>/2023/05/28/pytorch-duo-qia-xun-lian/</url>
      
        <content type="html"><![CDATA[<h1 id="Pytorch多卡训练"><a href="#Pytorch多卡训练" class="headerlink" title="Pytorch多卡训练"></a>Pytorch多卡训练</h1><p>多GPU的使用方法：</p><ul><li>模型并行（model parallel）：模型特别大，GPU显存限制无法将模型整个放进显卡</li><li>数据并行（data parallel）：模型可以放入GPU中，加大batchsize提升训练速度</li></ul><p>并行存在的问题：</p><ol><li><p>数据集如何在不同设备之间分配</p></li><li><p>误差梯度如何在不同设备之间通信</p></li><li><p>BN层如何在不同设备之间同步</p><p>同步会降低并行速度</p></li></ol><h2 id="DataParallel"><a href="#DataParallel" class="headerlink" title="DataParallel"></a>DataParallel</h2><p>单进程多线程，只适合单机</p><p>单机的情况下慢于DistributionDataParallel</p><h2 id="DistributedDataParallel"><a href="#DistributedDataParallel" class="headerlink" title="DistributedDataParallel"></a>DistributedDataParallel</h2><p>多进程，单机或多机</p><p>常用的多GPU训练启动方式：</p><ol><li><p><code>torch.distributed.launch</code>：代码量少，启动速度快</p><pre class="line-numbers language-python"><code class="language-python">python <span class="token operator">-</span>m torch<span class="token punctuation">.</span>distributed<span class="token punctuation">.</span>launch <span class="token operator">-</span><span class="token operator">-</span>XXX XXXX<span class="token punctuation">.</span>py <span class="token operator">-</span><span class="token operator">-</span>XX xxx<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre></li></ol><ol start="2"><li><code>torch.multiprocessing</code>：拥有更好的控制和灵活性</li></ol>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>Python的魔法方法</title>
      <link href="/2023/05/15/python-mo-fang-fang-fa/"/>
      <url>/2023/05/15/python-mo-fang-fang-fa/</url>
      
        <content type="html"><![CDATA[<h1 id="Python的魔法方法"><a href="#Python的魔法方法" class="headerlink" title="Python的魔法方法"></a>Python的魔法方法</h1><p>魔法方法：Python提供的让用户客制化一个类的方式，定义在类内的一些特殊方法，这些方法的名称前后会有两个下划线。</p><h2 id="基础方法"><a href="#基础方法" class="headerlink" title="基础方法"></a>基础方法</h2><h3 id="init-和-new"><a href="#init-和-new" class="headerlink" title="__ init__ 和__ new__"></a>__ init__ 和__ new__</h3><p>这两个魔法方法能够<strong>改变一个类实例化对象时的行为</strong>。</p><p><code>__new__</code>：是描述从一个class建立一个object的过程</p><p><code>__init__</code>：是描述当存在一个object后，对其进行初始化的过程</p><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">A</span><span class="token punctuation">:</span>    <span class="token keyword">def</span> <span class="token function">__new__</span><span class="token punctuation">(</span>cls<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"__new__"</span><span class="token punctuation">)</span>        <span class="token keyword">return</span> super<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>__new__<span class="token punctuation">(</span>cls<span class="token punctuation">)</span>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>        self<span class="token punctuation">.</span>x <span class="token operator">=</span> x        <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"__init__"</span><span class="token punctuation">)</span>o <span class="token operator">=</span> A<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true"># 等价于</span><span class="token comment" spellcheck="true"># 1. obj = __new__(A)</span><span class="token comment" spellcheck="true"># 2. __init__(obj)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>如果在建立object的过程中传入了若干参数，则这些参数既会被传入到<code>__new__</code>中，也会被传入到<code>__ini__</code>中。<code>__new__</code>因为是建立object，所以是有返回值的。</p><h3 id="del"><a href="#del" class="headerlink" title="__ del__"></a>__ del__</h3><p><code>__del__</code>可以粗略的认为是一个析构函数，是描述一个object被释放的过程。但是由于Python的对象释放过程较为复杂，因此该方法不可控。</p><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">A</span><span class="token punctuation">:</span>    <span class="token keyword">def</span> <span class="token function">__del__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"__del__"</span><span class="token punctuation">)</span>o <span class="token operator">=</span> A<span class="token punctuation">(</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><p><strong>注意</strong>：这里的<code>__del__</code>和关键字<code>del</code>没有关系，<code>del o</code>并不一定会触发<code>__del__</code>方法，只是让对象的引用-1。</p><h3 id="repr-str-和-format"><a href="#repr-str-和-format" class="headerlink" title="__ repr__ , __ str__ 和__ format__"></a>__ repr__ , __ str__ 和__ format__</h3><p><code>__repr__</code>和<code>__str__</code>这两个方法的功能是相似的，都是返回一个object的字符串表示，这两个方法主要是语义的不同。</p><p><code>__repr</code>：representation，一般要有更详细的信息，可以通过<code>repr()</code>内置方法调用该方法。</p><p><code>__str__</code>：注重可读性，可以通过<code>str()</code>内置方法调用该方法，<code>print</code>内部就是使用了<code>str()</code>方法。</p><p><code>__format__</code>：使用某种格式打印object时，会调用该方法。</p><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"{}"</span><span class="token punctuation">.</span>format<span class="token punctuation">(</span><span class="token number">15</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token comment" spellcheck="true"># 10进制</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"{:b}"</span><span class="token punctuation">.</span>format<span class="token punctuation">(</span><span class="token number">15</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token comment" spellcheck="true"># 2进制</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"{:x}"</span><span class="token punctuation">.</span>format<span class="token punctuation">(</span><span class="token number">15</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token comment" spellcheck="true"># 16进制</span><span class="token keyword">print</span><span class="token punctuation">(</span>f<span class="token string">"{15}"</span><span class="token punctuation">)</span> <span class="token comment" spellcheck="true"># 10进制</span><span class="token keyword">print</span><span class="token punctuation">(</span>f<span class="token string">"{15:b}"</span><span class="token punctuation">)</span> <span class="token comment" spellcheck="true"># 2进制</span><span class="token keyword">print</span><span class="token punctuation">(</span>f<span class="token string">"{15:x}"</span><span class="token punctuation">)</span> <span class="token comment" spellcheck="true"># 16进制</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">A</span><span class="token punctuation">:</span>    <span class="token keyword">def</span> <span class="token function">__format__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> spec<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token keyword">if</span> spec <span class="token operator">==</span> <span class="token string">"x"</span><span class="token punctuation">:</span>            <span class="token keyword">return</span> <span class="token string">"0xA"</span>        <span class="token keyword">return</span> <span class="token string">"&lt;A>"</span><span class="token keyword">print</span><span class="token punctuation">(</span>f<span class="token string">"{A()}"</span><span class="token punctuation">)</span> <span class="token comment" spellcheck="true"># &lt;A></span><span class="token keyword">print</span><span class="token punctuation">(</span>f<span class="token string">"{A():x}"</span><span class="token punctuation">)</span> <span class="token comment" spellcheck="true"># 0xA</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="bytes"><a href="#bytes" class="headerlink" title="__ bytes__"></a>__ bytes__</h3><p><code>__bytes__</code>：通过class去建立bytes时的行为，客制化object的bytes表示。通过内置方法<code>bytes()</code>调用该魔法方法。</p><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">A</span><span class="token punctuation">:</span>    <span class="token keyword">def</span> <span class="token function">__bytes__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"__bytes__"</span><span class="token punctuation">)</span>        <span class="token keyword">return</span> bytes<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>bytes<span class="token punctuation">(</span>A<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token comment" spellcheck="true"># b'\x00\x01'</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="比较方法"><a href="#比较方法" class="headerlink" title="比较方法"></a>比较方法</h2><h3 id="等于-eq-和不等于-ne"><a href="#等于-eq-和不等于-ne" class="headerlink" title="等于 __ eq__ 和不等于__ ne__"></a>等于 __ eq__ 和不等于__ ne__</h3><p><code>__eq__</code>：描述两个object进行比较时的过程，判断两个object是否相等。</p><p><code>__ne__</code>：描述两个object进行比较时的过程，判断两个object是否不相等。</p><p>上述两个方法，等于和不等于，是有默认实现的，如果没有自定义的等于和不等于，会使用<code>is</code>进行比较。</p><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">Date</span><span class="token punctuation">:</span>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> year<span class="token punctuation">,</span> month<span class="token punctuation">,</span> day<span class="token punctuation">)</span><span class="token punctuation">:</span>        self<span class="token punctuation">.</span>year <span class="token operator">=</span> year        self<span class="token punctuation">.</span>month <span class="token operator">=</span> month        self<span class="token punctuation">.</span>day <span class="token operator">=</span> day    <span class="token keyword">def</span> <span class="token function">__eq__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> other<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token keyword">return</span> <span class="token punctuation">(</span>self<span class="token punctuation">.</span>year <span class="token operator">==</span> other<span class="token punctuation">.</span>year <span class="token operator">and</span> self<span class="token punctuation">.</span>month <span class="token operator">==</span> other<span class="token punctuation">.</span>month                 <span class="token operator">and</span> self<span class="token punctuation">.</span>day <span class="token operator">=</span> other<span class="token punctuation">.</span>day<span class="token punctuation">)</span>x <span class="token operator">=</span> Date<span class="token punctuation">(</span><span class="token number">2023</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">24</span><span class="token punctuation">)</span>y <span class="token operator">=</span> Date<span class="token punctuation">(</span><span class="token number">2023</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">24</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>x <span class="token operator">==</span> y<span class="token punctuation">)</span> <span class="token comment" spellcheck="true"># True</span>z <span class="token operator">=</span> Date<span class="token punctuation">(</span><span class="token number">2023</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">,</span> <span class="token number">14</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>x <span class="token operator">!=</span> z<span class="token punctuation">)</span> <span class="token comment" spellcheck="true"># True</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p><strong>注意</strong>：当没有定义不等于的方法时，使用不等于运算符Python会对<code>__eq__</code>等于方法进行取反，所以通常情况下定义<code>__eq__</code>方法足够处理等于和不等于的运算了。当然也可以定义不等于的魔法方法<code>__ne__</code>。</p><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">Date</span><span class="token punctuation">:</span>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> year<span class="token punctuation">,</span> month<span class="token punctuation">,</span> day<span class="token punctuation">)</span><span class="token punctuation">:</span>        self<span class="token punctuation">.</span>year <span class="token operator">=</span> year        self<span class="token punctuation">.</span>month <span class="token operator">=</span> month        self<span class="token punctuation">.</span>day <span class="token operator">=</span> day    <span class="token keyword">def</span> <span class="token function">__eq__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> other<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"__eq__"</span><span class="token punctuation">)</span>        <span class="token keyword">return</span> <span class="token punctuation">(</span>self<span class="token punctuation">.</span>year <span class="token operator">==</span> other<span class="token punctuation">.</span>year <span class="token operator">and</span> self<span class="token punctuation">.</span>month <span class="token operator">==</span> other<span class="token punctuation">.</span>month                 <span class="token operator">and</span> self<span class="token punctuation">.</span>day <span class="token operator">==</span> other<span class="token punctuation">.</span>day<span class="token punctuation">)</span>    <span class="token keyword">def</span> <span class="token function">__ne__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> other<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"__ne__"</span><span class="token punctuation">)</span>        <span class="token keyword">return</span> <span class="token punctuation">(</span>self<span class="token punctuation">.</span>year <span class="token operator">!=</span> other<span class="token punctuation">.</span>year <span class="token operator">and</span> self<span class="token punctuation">.</span>month <span class="token operator">!=</span> other<span class="token punctuation">.</span>month                 <span class="token operator">and</span> self<span class="token punctuation">.</span>day <span class="token operator">!=</span> other<span class="token punctuation">.</span>day<span class="token punctuation">)</span>x <span class="token operator">=</span> Date<span class="token punctuation">(</span><span class="token number">2023</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">24</span><span class="token punctuation">)</span>y <span class="token operator">=</span> Date<span class="token punctuation">(</span><span class="token number">2023</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">24</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>x <span class="token operator">==</span> y<span class="token punctuation">)</span> <span class="token comment" spellcheck="true"># __eq__</span><span class="token comment" spellcheck="true"># True</span>z <span class="token operator">=</span> Date<span class="token punctuation">(</span><span class="token number">2023</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">,</span> <span class="token number">14</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>x <span class="token operator">!=</span> z<span class="token punctuation">)</span> <span class="token comment" spellcheck="true"># __ne__</span><span class="token comment" spellcheck="true"># True</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="大于-gt-和小于-lt"><a href="#大于-gt-和小于-lt" class="headerlink" title="大于 __ gt__ 和小于__ lt__"></a>大于 __ gt__ 和小于__ lt__</h3><p><code>__gt__</code>：描述两个object进行比较时的过程，判断一个object是否大于另一个object。</p><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">Date</span><span class="token punctuation">:</span>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> year<span class="token punctuation">,</span> month<span class="token punctuation">,</span> day<span class="token punctuation">)</span><span class="token punctuation">:</span>        self<span class="token punctuation">.</span>year <span class="token operator">=</span> year        self<span class="token punctuation">.</span>month <span class="token operator">=</span> month        self<span class="token punctuation">.</span>day <span class="token operator">=</span> day    <span class="token keyword">def</span> <span class="token function">__gt__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> other<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token keyword">if</span> self<span class="token punctuation">.</span>year <span class="token operator">></span> other<span class="token punctuation">.</span>year<span class="token punctuation">:</span>            <span class="token keyword">return</span> <span class="token boolean">True</span>           <span class="token keyword">if</span> self<span class="token punctuation">.</span>year <span class="token operator">==</span> other<span class="token punctuation">.</span>year<span class="token punctuation">:</span>            <span class="token keyword">if</span> self<span class="token punctuation">.</span>month <span class="token operator">></span> other<span class="token punctuation">.</span>month<span class="token punctuation">:</span>                <span class="token keyword">return</span> <span class="token boolean">True</span>            <span class="token keyword">if</span> self<span class="token punctuation">.</span>month <span class="token operator">==</span> other<span class="token punctuation">.</span>month<span class="token punctuation">:</span>                <span class="token keyword">return</span> self<span class="token punctuation">.</span>day <span class="token operator">></span> other<span class="token punctuation">.</span>dayx <span class="token operator">=</span> Date<span class="token punctuation">(</span><span class="token number">2023</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">22</span><span class="token punctuation">)</span>y <span class="token operator">=</span> Date<span class="token punctuation">(</span><span class="token number">2023</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">24</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>x <span class="token operator">></span> y<span class="token punctuation">)</span> <span class="token comment" spellcheck="true"># False</span><span class="token keyword">print</span><span class="token punctuation">(</span>x <span class="token operator">&lt;</span> y<span class="token punctuation">)</span> <span class="token comment" spellcheck="true"># True</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>当调用<code>&gt;</code>大于运算符时，即 <code>x &gt; y</code> 可以看作是<code>x.__gt__(y)</code>的运算。<br><strong>注意</strong>：当没有实现小于运算却调用<code>&lt;</code>小于运算符时，也可以正常返回结果，是因为<code>x &lt; y</code>就对应<code>y &gt; x</code>，所以可以认为进行了<code>y.__gt__(x)</code>的运算。</p><p><code>__lt__</code>：描述两个object进行比较时的过程，判断一个object是否小于另一个object。当一个类里同时实现了<code>__gt__</code>和<code>__lt__</code>时，使用<code>&lt;</code>运算符会优先调用<code>__lt__</code>。</p><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">Date</span><span class="token punctuation">:</span>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> year<span class="token punctuation">,</span> month<span class="token punctuation">,</span> day<span class="token punctuation">)</span><span class="token punctuation">:</span>        self<span class="token punctuation">.</span>year <span class="token operator">=</span> year        self<span class="token punctuation">.</span>month <span class="token operator">=</span> month        self<span class="token punctuation">.</span>day <span class="token operator">=</span> day    <span class="token keyword">def</span> <span class="token function">__gt__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> other<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"__gt__"</span><span class="token punctuation">)</span>        <span class="token keyword">if</span> self<span class="token punctuation">.</span>year <span class="token operator">></span> other<span class="token punctuation">.</span>year<span class="token punctuation">:</span>            <span class="token keyword">return</span> <span class="token boolean">True</span>           <span class="token keyword">if</span> self<span class="token punctuation">.</span>year <span class="token operator">==</span> other<span class="token punctuation">.</span>year<span class="token punctuation">:</span>            <span class="token keyword">if</span> self<span class="token punctuation">.</span>month <span class="token operator">></span> other<span class="token punctuation">.</span>month<span class="token punctuation">:</span>                <span class="token keyword">return</span> <span class="token boolean">True</span>            <span class="token keyword">if</span> self<span class="token punctuation">.</span>month <span class="token operator">==</span> other<span class="token punctuation">.</span>month<span class="token punctuation">:</span>                <span class="token keyword">return</span> self<span class="token punctuation">.</span>day <span class="token operator">></span> other<span class="token punctuation">.</span>day    <span class="token keyword">def</span> <span class="token function">__lt__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> other<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"__lt__"</span><span class="token punctuation">)</span>        <span class="token keyword">if</span> self<span class="token punctuation">.</span>year <span class="token operator">&lt;</span> other<span class="token punctuation">.</span>year<span class="token punctuation">:</span>            <span class="token keyword">return</span> <span class="token boolean">True</span>           <span class="token keyword">if</span> self<span class="token punctuation">.</span>year <span class="token operator">==</span> other<span class="token punctuation">.</span>year<span class="token punctuation">:</span>            <span class="token keyword">if</span> self<span class="token punctuation">.</span>month <span class="token operator">&lt;</span> other<span class="token punctuation">.</span>month<span class="token punctuation">:</span>                <span class="token keyword">return</span> <span class="token boolean">True</span>            <span class="token keyword">if</span> self<span class="token punctuation">.</span>month <span class="token operator">==</span> other<span class="token punctuation">.</span>month<span class="token punctuation">:</span>                <span class="token keyword">return</span> self<span class="token punctuation">.</span>day <span class="token operator">&lt;</span> other<span class="token punctuation">.</span>day  x <span class="token operator">=</span> Date<span class="token punctuation">(</span><span class="token number">2023</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">22</span><span class="token punctuation">)</span>y <span class="token operator">=</span> Date<span class="token punctuation">(</span><span class="token number">2023</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">24</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>x <span class="token operator">&lt;</span> y<span class="token punctuation">)</span> <span class="token comment" spellcheck="true"># __lt__</span><span class="token comment" spellcheck="true"># True</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>因为<code>x &lt; y</code>就对应<code>y &gt; x</code>，<code>x &lt; y</code>既可以通过<code>x.__lt__(y)</code>，也可以通过<code>y.__gt__(x)</code>，那么使用<code>&gt;</code>或<code>&lt;</code>到底调用哪个魔法方法呢？</p><p>调用原则：</p><ul><li>当x和y是同一个类的不同object，则优先使用运算符左侧的比较方法，当左侧object没有定义时，才会去使用右侧object的比较方法（不限于大于小于，还有等于不等于）。</li><li>如果y是x的类的子类object，则优先使用y的<strong>比较方法</strong>（不限于大于小于，还有等于不等于）。</li></ul><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">NewDate</span><span class="token punctuation">(</span>Date<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">pass</span>x <span class="token operator">=</span> Date<span class="token punctuation">(</span><span class="token number">2023</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">22</span><span class="token punctuation">)</span>y <span class="token operator">=</span> NewDate<span class="token punctuation">(</span><span class="token number">2023</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">24</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>x <span class="token operator">&lt;</span> y<span class="token punctuation">)</span><span class="token comment" spellcheck="true"># __gt__</span><span class="token comment" spellcheck="true"># True</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="大于等于-ge-和小于等于-le"><a href="#大于等于-ge-和小于等于-le" class="headerlink" title="大于等于 __ ge__ 和小于等于__ le__"></a>大于等于 __ ge__ 和小于等于__ le__</h3><p><code>__ge__</code>：与大于运算<code>__gt__</code>方法类似，greater than  or equal to</p><p><code>__le__</code>：与小于运算<code>__lt__</code>方法类似，less than  or equal to</p><h3 id="hash"><a href="#hash" class="headerlink" title="__ hash__"></a>__ hash__</h3><p><code>__hash__</code>：求某个数据结构（自定义）的哈希值，一个自定义的数据结构，是有其默认的hash算法的，使用<code>hash()</code>可以直接拿到其哈希值。</p><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">Date</span><span class="token punctuation">:</span>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> year<span class="token punctuation">,</span> month<span class="token punctuation">,</span> day<span class="token punctuation">)</span><span class="token punctuation">:</span>        self<span class="token punctuation">.</span>year <span class="token operator">=</span> year        self<span class="token punctuation">.</span>month <span class="token operator">=</span> month        self<span class="token punctuation">.</span>day <span class="token operator">=</span> dayx <span class="token operator">=</span> Date<span class="token punctuation">(</span><span class="token number">2023</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span><span class="token number">22</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>hash<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token comment" spellcheck="true"># 8771107107837</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>如果要自定义hash算法，则一般也要定义<code>__eq__</code>方法，定义<code>__hash__</code>的要求如下：</p><ol><li>返回值必须是整数</li><li>对于两个相等的object，其哈希值必须相同</li></ol><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">Date</span><span class="token punctuation">:</span>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> year<span class="token punctuation">,</span> month<span class="token punctuation">,</span> day<span class="token punctuation">)</span><span class="token punctuation">:</span>        self<span class="token punctuation">.</span>year <span class="token operator">=</span> year        self<span class="token punctuation">.</span>month <span class="token operator">=</span> month        self<span class="token punctuation">.</span>day <span class="token operator">=</span> day    <span class="token keyword">def</span> <span class="token function">__eq__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> other<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token keyword">return</span> <span class="token punctuation">(</span>self<span class="token punctuation">.</span>year <span class="token operator">==</span> other<span class="token punctuation">.</span>year <span class="token operator">and</span> self<span class="token punctuation">.</span>month <span class="token operator">==</span> other<span class="token punctuation">.</span>month         <span class="token operator">and</span> self<span class="token punctuation">.</span>day <span class="token operator">==</span> other<span class="token punctuation">.</span>day<span class="token punctuation">)</span>    <span class="token keyword">def</span> <span class="token function">__hash</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token keyword">return</span> <span class="token number">2</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="bool"><a href="#bool" class="headerlink" title="__ bool___"></a>__ bool___</h3><p><code>__bool__</code>：描述object在条件判断语句中的行为。所有的object出现在条件判断语句中时，都会被判定为True，如果想要改变条件判断的结果，可以使用该方法。</p><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">Date</span><span class="token punctuation">:</span>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> year<span class="token punctuation">,</span> month<span class="token punctuation">,</span> day<span class="token punctuation">)</span><span class="token punctuation">:</span>        self<span class="token punctuation">.</span>year <span class="token operator">=</span> year        self<span class="token punctuation">.</span>month <span class="token operator">=</span> month        self<span class="token punctuation">.</span>day <span class="token operator">=</span> daya <span class="token operator">=</span> Date<span class="token punctuation">(</span><span class="token number">2023</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">23</span><span class="token punctuation">)</span><span class="token keyword">if</span> a<span class="token punctuation">:</span>    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"hello"</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true"># hello</span><span class="token keyword">class</span> <span class="token class-name">Date</span><span class="token punctuation">:</span>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> year<span class="token punctuation">,</span> month<span class="token punctuation">,</span> day<span class="token punctuation">)</span><span class="token punctuation">:</span>        self<span class="token punctuation">.</span>year <span class="token operator">=</span> year        self<span class="token punctuation">.</span>month <span class="token operator">=</span> month        self<span class="token punctuation">.</span>day <span class="token operator">=</span> day    <span class="token keyword">def</span> <span class="token function">__bool__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"__bool__"</span><span class="token punctuation">)</span>        <span class="token keyword">return</span> <span class="token boolean">False</span>a <span class="token operator">=</span> Date<span class="token punctuation">(</span><span class="token number">2023</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">23</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>bool<span class="token punctuation">(</span>a<span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token comment" spellcheck="true"># 当把object转变为bool时也会调用该方法</span><span class="token keyword">if</span> a<span class="token punctuation">:</span>    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"hello"</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true"># __bool__</span><span class="token comment" spellcheck="true"># False</span><span class="token comment" spellcheck="true"># __bool__</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="对象属性相关的方法"><a href="#对象属性相关的方法" class="headerlink" title="对象属性相关的方法"></a>对象属性相关的方法</h2><h3 id="getattr-和-getattribute"><a href="#getattr-和-getattribute" class="headerlink" title="__ getattr__ 和 __ getattribute__"></a>__ getattr__ 和 __ getattribute__</h3><p><code>__getattr__</code>：尝试访问object的属性时，描述该属性不存在时object的行为。默认情况下如果访问一个object里不存在的属性时，会抛出<code>AttributeError</code>。</p><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">A</span><span class="token punctuation">:</span>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>        self<span class="token punctuation">.</span>exist <span class="token operator">=</span> <span class="token string">"abc"</span>    <span class="token keyword">def</span> <span class="token function">__getattr__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> name<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token keyword">print</span><span class="token punctuation">(</span>f<span class="token string">"getting {name}"</span><span class="token punctuation">)</span>o <span class="token operator">=</span> A<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>o<span class="token punctuation">.</span>exist<span class="token punctuation">)</span> <span class="token comment" spellcheck="true"># abc</span><span class="token keyword">print</span><span class="token punctuation">(</span>o<span class="token punctuation">.</span>test<span class="token punctuation">)</span> <span class="token comment" spellcheck="true"># getting test</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p><strong>注意</strong>：访问的该属性，会以字符串的形式传入到<code>__getattr__</code>里。只有在读取一个不存在的属性时才会调用该魔法方法。</p><p><code>__getattribute__</code>：尝试访问object的属性时，不论该属性是否存在，都会调用该方法。</p><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">A</span><span class="token punctuation">:</span>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>        self<span class="token punctuation">.</span>exist <span class="token operator">=</span> <span class="token string">"abc"</span>    <span class="token keyword">def</span> <span class="token function">__getattribute__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> name<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token keyword">print</span><span class="token punctuation">(</span>f<span class="token string">"getting {name}"</span><span class="token punctuation">)</span>        <span class="token keyword">return</span> Noneo <span class="token operator">=</span> A<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>o<span class="token punctuation">.</span>exist<span class="token punctuation">)</span> <span class="token comment" spellcheck="true"># getting exist</span><span class="token comment" spellcheck="true"># None</span><span class="token keyword">print</span><span class="token punctuation">(</span>o<span class="token punctuation">.</span>test<span class="token punctuation">)</span> <span class="token comment" spellcheck="true"># getting test</span><span class="token comment" spellcheck="true"># None</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="setattr-和-delattr"><a href="#setattr-和-delattr" class="headerlink" title="__ setattr__ 和 __ delattr__"></a>__ setattr__ 和 __ delattr__</h3><p><code>__setattr__</code>：尝试写入object的属性时，描述该过程中object的行为。该方法有两个参数：一是属性名，二是属性值。如果该object属性不存在，则会调用<code>__setattr__()</code>对该属性进行赋值。</p><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">A</span><span class="token punctuation">:</span>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>        self<span class="token punctuation">.</span>data <span class="token operator">=</span> <span class="token string">"abc"</span>        self<span class="token punctuation">.</span>counter <span class="token operator">=</span> <span class="token number">0</span>    <span class="token keyword">def</span> <span class="token function">__setatter__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> name<span class="token punctuation">,</span> val<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token keyword">print</span><span class="token punctuation">(</span>f<span class="token string">"set{name}"</span><span class="token punctuation">)</span>        super<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>__setattr<span class="token punctuation">(</span>name<span class="token punctuation">,</span> val<span class="token punctuation">)</span>o <span class="token operator">=</span> A<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>o<span class="token punctuation">.</span>data<span class="token punctuation">)</span><span class="token comment" spellcheck="true"># set data</span><span class="token comment" spellcheck="true"># set counter</span><span class="token comment" spellcheck="true"># abc</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p><code>__delattr__</code>：与<code>__del__</code>不同，在一个object正常产生和消亡的过程中，不会调用<code>__delattr__</code>方法，而在尝试删除一个object的属性时才会调用。</p><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">A</span><span class="token punctuation">:</span>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>        self<span class="token punctuation">.</span>data <span class="token operator">=</span> <span class="token string">"abc"</span>    <span class="token keyword">def</span> <span class="token function">__delatter__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> name<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token keyword">print</span><span class="token punctuation">(</span>f<span class="token string">"del{name}"</span><span class="token punctuation">)</span>        super<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>__delattr<span class="token punctuation">(</span>name<span class="token punctuation">)</span>o <span class="token operator">=</span> A<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token keyword">del</span> o<span class="token punctuation">.</span>data <span class="token comment" spellcheck="true"># del data</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="dir"><a href="#dir" class="headerlink" title="__ dir__"></a>__ dir__</h3><p><code>__dir__</code>：列出object的所有属性名和方法名，Python规定该方法必须返回一个sequence（如列表list）。通过内置函数<code>dir()</code>调用该魔法方法。</p><pre class="line-numbers language-python"><code class="language-python"><span class="token operator">>></span><span class="token operator">></span>dir<span class="token punctuation">(</span><span class="token punctuation">)</span>   <span class="token comment" spellcheck="true">#  获得当前模块的属性列表</span><span class="token punctuation">[</span><span class="token string">'__builtins__'</span><span class="token punctuation">,</span> <span class="token string">'__doc__'</span><span class="token punctuation">,</span> <span class="token string">'__name__'</span><span class="token punctuation">,</span> <span class="token string">'__package__'</span><span class="token punctuation">,</span> <span class="token string">'arr'</span><span class="token punctuation">,</span> <span class="token string">'myslice'</span><span class="token punctuation">]</span>a <span class="token operator">=</span> list<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>dir<span class="token punctuation">(</span>a<span class="token punctuation">)</span><span class="token punctuation">)</span>    <span class="token comment" spellcheck="true"># 查看列表的方法</span><span class="token punctuation">[</span><span class="token string">'__add__'</span><span class="token punctuation">,</span> <span class="token string">'__class__'</span><span class="token punctuation">,</span> <span class="token string">'__contains__'</span><span class="token punctuation">,</span> <span class="token string">'__delattr__'</span><span class="token punctuation">,</span> <span class="token string">'__delitem__'</span><span class="token punctuation">,</span> <span class="token string">'__delslice__'</span><span class="token punctuation">,</span> <span class="token string">'__doc__'</span><span class="token punctuation">,</span> <span class="token string">'__eq__'</span><span class="token punctuation">,</span> <span class="token string">'__format__'</span><span class="token punctuation">,</span> <span class="token string">'__ge__'</span><span class="token punctuation">,</span> <span class="token string">'__getattribute__'</span><span class="token punctuation">,</span> <span class="token string">'__getitem__'</span><span class="token punctuation">,</span> <span class="token string">'__getslice__'</span><span class="token punctuation">,</span> <span class="token string">'__gt__'</span><span class="token punctuation">,</span> <span class="token string">'__hash__'</span><span class="token punctuation">,</span> <span class="token string">'__iadd__'</span><span class="token punctuation">,</span> <span class="token string">'__imul__'</span><span class="token punctuation">,</span> <span class="token string">'__init__'</span><span class="token punctuation">,</span> <span class="token string">'__iter__'</span><span class="token punctuation">,</span> <span class="token string">'__le__'</span><span class="token punctuation">,</span> <span class="token string">'__len__'</span><span class="token punctuation">,</span> <span class="token string">'__lt__'</span><span class="token punctuation">,</span> <span class="token string">'__mul__'</span><span class="token punctuation">,</span> <span class="token string">'__ne__'</span><span class="token punctuation">,</span> <span class="token string">'__new__'</span><span class="token punctuation">,</span> <span class="token string">'__reduce__'</span><span class="token punctuation">,</span> <span class="token string">'__reduce_ex__'</span><span class="token punctuation">,</span> <span class="token string">'__repr__'</span><span class="token punctuation">,</span> <span class="token string">'__reversed__'</span><span class="token punctuation">,</span> <span class="token string">'__rmul__'</span><span class="token punctuation">,</span> <span class="token string">'__setattr__'</span><span class="token punctuation">,</span> <span class="token string">'__setitem__'</span><span class="token punctuation">,</span> <span class="token string">'__setslice__'</span><span class="token punctuation">,</span> <span class="token string">'__sizeof__'</span><span class="token punctuation">,</span> <span class="token string">'__str__'</span><span class="token punctuation">,</span> <span class="token string">'__subclasshook__'</span><span class="token punctuation">,</span> <span class="token string">'append'</span><span class="token punctuation">,</span> <span class="token string">'count'</span><span class="token punctuation">,</span> <span class="token string">'extend'</span><span class="token punctuation">,</span> <span class="token string">'index'</span><span class="token punctuation">,</span> <span class="token string">'insert'</span><span class="token punctuation">,</span> <span class="token string">'pop'</span><span class="token punctuation">,</span> <span class="token string">'remove'</span><span class="token punctuation">,</span> <span class="token string">'reverse'</span><span class="token punctuation">,</span> <span class="token string">'sort'</span><span class="token punctuation">]</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">A</span><span class="token punctuation">:</span>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>        self<span class="token punctuation">.</span>data <span class="token operator">=</span> <span class="token string">"abc"</span>    <span class="token keyword">def</span> <span class="token function">__dir__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> name<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token keyword">return</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>o <span class="token operator">=</span> A<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>dir<span class="token punctuation">(</span>o<span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token comment" spellcheck="true"># []</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="迭代器：-iter-和-next"><a href="#迭代器：-iter-和-next" class="headerlink" title="迭代器：__ iter__ 和__ next__"></a>迭代器：__ iter__ 和__ next__</h2><p>迭代器就是重复地做一些事情，可以简单的理解为循环。</p><ul><li>迭代是访问集合元素的一种方式。</li><li>迭代器是一个可以<strong>记住遍历的位置</strong>的对象。</li><li>迭代器对象从集合的第一个元素开始访问，直到所有的元素被访问完结束。<strong>迭代器只能往前不会后退</strong>。</li><li>迭代器有两个基本的方法：iter() 和 next()。</li></ul><p>Python中<strong>实现了<strong>iter</strong>方法的对象是可迭代的，实现了<strong>next</strong>方法的对象是迭代器</strong>。实际上要想让一个迭代器工作，至少要实现<strong>iter</strong>方法和<strong>next</strong>方法。<br>常见的就是我们在使用<code>for</code>语句的时候，python内部其实在<code>for</code>后面的对象上使用了内建函数<code>iter()</code>：</p><pre class="line-numbers language-python"><code class="language-python">a <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">]</span><span class="token keyword">for</span> i <span class="token keyword">in</span> a<span class="token punctuation">:</span>    do_something<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true"># 其实内部做了如下转换</span><span class="token keyword">for</span> i <span class="token keyword">in</span> iter<span class="token punctuation">(</span>a<span class="token punctuation">)</span><span class="token punctuation">:</span>    do_something<span class="token punctuation">(</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>下面引入两个概念：</p><ol><li>Iterable: 有迭代能力的对象。一个类实现了<strong>iter</strong>，那么就认为它有迭代能力，通常此函数<strong>必须</strong>返回一个实现了<strong>next</strong>方法的对象，<strong>如果自己实现了，则可以返回</strong><code>self</code>，当然这个返回值不是必须的；</li><li>Iterator: 迭代器(当然也是Iterable)。同时实现了<strong>iter</strong>和<strong>next</strong>的对象，缺少任何一个都不算是Iterator，比如下面例子中，A()可以是一个Iterable，但是A()和B()都不能算是和Iterator，因为A只实现了<strong>iter</strong>，而B只实现了<strong>next</strong>()。<pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">B</span><span class="token punctuation">(</span>object<span class="token punctuation">)</span><span class="token punctuation">:</span> <span class="token keyword">def</span> <span class="token function">__next__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>     <span class="token keyword">raise</span> StopIteration<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre></li></ol><p>class A(object):<br>    def <strong>iter</strong>(self):<br>        return B()</p><p>from collections.abc import *</p><p>a = A()<br>b = B()<br>print(isinstance(a, Iterable)) # True<br>print(isinstance(a, Iterator)) # False<br>print(isinstance(b, Iterable)) # False<br>print(isinstance(b, Iterator)) # False</p><pre><code>## 创建一个迭代器把一个类作为一个迭代器使用需要在类中实现两个方法**__iter__**与**__next__**。__iter__方法**返回一个迭代对象**， 这个迭代对象**实现了__next__方法**并**通过StopIteration异常**来标识迭代的完成。__next__方法会返回下一个迭代器对象。就可以通过`next()`函数访问这个对象的下一个元素了，并且在你不想继续有迭代的情况下抛出一个StopIteration的异常（for语句会捕获这个异常，并且自动结束for）```pythonclass MyRange(object):    def __init__(self, end):        self.start = 0        self.end = end    def __iter__(self):        return self    def __next__(self):        if self.start &lt; self.end:            ret = self.start            self.start += 1            return ret        else:            raise StopIterationa = MyRange(5)for i in a:    print(i) # 0 1 2 3 4</code></pre><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><ul><li>可以使用<code>collection.abs</code>里面的<code>Iterator</code>和<code>Iterable</code>配合<code>isinstance</code>函数来判断一个对象是否是可迭代的，是否是迭代器对象</li><li>iter()实际是映射到了<strong>iter</strong>函数</li><li>只要实现了<strong>iter</strong>的对象就是可迭代对象(Iterable)，正常情况下应该返回一个实现了<strong>next</strong>的对象（<strong>强制要求</strong>，否则报错），如果自己实现了<strong>next</strong>，也可以返回自己。</li><li>同时实现了<strong>iter</strong>和<strong>next</strong>的是迭代器(Iterator)，当然也是一个可迭代对象了，其中<strong>next</strong>应该在迭代完成后，抛出一个StopIteration异常</li><li>for语句会自动处理这个StopIteration异常以便结束for循环<h1 id="生成器-yield表达式"><a href="#生成器-yield表达式" class="headerlink" title="生成器(yield表达式)"></a>生成器(yield表达式)</h1><code>str</code>，<code>list</code>，<code>tuple</code>，<code>dict</code>，<code>set</code>这些都是可迭代的，就是可用for来访问里面的每一个元素。但他们并不是迭代器。</li></ul><p><strong>生成器是一个可以快速创建迭代器的工具</strong>。生成器是可迭代的，更准确的说法是它就是个迭代器。<br>我们可以用列表生成式来初始化一个列表：</p><pre class="line-numbers language-python"><code class="language-python">list1 <span class="token operator">=</span> <span class="token punctuation">[</span>x <span class="token keyword">for</span> x <span class="token keyword">in</span> range<span class="token punctuation">(</span><span class="token number">5</span><span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token keyword">print</span><span class="token punctuation">(</span>list1<span class="token punctuation">)</span> <span class="token comment" spellcheck="true"># [0,1,2,3,4]</span>gen <span class="token operator">=</span> <span class="token punctuation">(</span>x <span class="token keyword">for</span> x <span class="token keyword">in</span> range<span class="token punctuation">(</span><span class="token number">5</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token comment" spellcheck="true"># 生成器表达式</span><span class="token keyword">print</span><span class="token punctuation">(</span>gen<span class="token punctuation">)</span> <span class="token comment" spellcheck="true"># generator object &lt;genexpr> at 0x000000ADEF</span><span class="token comment" spellcheck="true"># 调用gen的方法和迭代器一模一样</span><span class="token comment" spellcheck="true"># 方法一</span><span class="token keyword">for</span> x <span class="token keyword">in</span> gen<span class="token punctuation">:</span>    <span class="token keyword">print</span><span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token comment" spellcheck="true"># 方法二</span><span class="token keyword">print</span><span class="token punctuation">(</span>next<span class="token punctuation">(</span>gen<span class="token punctuation">)</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>在 Python 中，使用了<code>yield</code>的函数被称为生成器（generator）。生成器是一个返回迭代器的函数，只能用于迭代操作，更简单点理解生成器就是一个迭代器。</p><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">MyRange</span><span class="token punctuation">(</span>end<span class="token punctuation">)</span><span class="token punctuation">:</span>    start <span class="token operator">=</span> <span class="token number">0</span>    <span class="token keyword">while</span> start <span class="token operator">&lt;</span> end<span class="token punctuation">:</span>        <span class="token keyword">yield</span> start        start <span class="token operator">+=</span> <span class="token number">1</span><span class="token keyword">from</span> collections<span class="token punctuation">.</span>abc <span class="token keyword">import</span> <span class="token operator">*</span>a <span class="token operator">=</span> MyRange<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>isinstance<span class="token punctuation">(</span>a<span class="token punctuation">,</span> Iterator<span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token comment" spellcheck="true"># True</span><span class="token keyword">print</span><span class="token punctuation">(</span>isinstance<span class="token punctuation">(</span>a<span class="token punctuation">,</span> Iterable<span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token comment" spellcheck="true"># True</span><span class="token keyword">for</span> i <span class="token keyword">in</span> a<span class="token punctuation">:</span>    <span class="token keyword">print</span><span class="token punctuation">(</span>i<span class="token punctuation">)</span> <span class="token comment" spellcheck="true"># 0 1</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>可以看见，函数MyRange里面使用了yield语句，结果变成了generator，也是Iterator，我们在使用for语句的时候，执行步骤如下：</p><ol><li>start = 0， while 0 &lt; 2，遇到了yield语句，返回start的值0，然后print这值；</li><li>第二次for的时候，执行yield后面的语句，start = 1，while 1&lt;2，遇到了yield语句，返回start的值1，然后print；</li><li>第三次for的时候，执行yield后面的语句，start = 2，while 2&lt;2不成立，此时函数结束运行，会抛出StopIterator的异常(这个是自动抛出的，不需要显示的调用raise语句，可以使用下面的next方法来查看这个异常)。</li></ol><p><strong>在调用生成器运行的过程中，每次遇到 yield 时函数会暂停并保存当前所有的运行信息，返回 yield 的值, 并在下一次执行 next() 方法时从当前位置继续运行</strong>。</p>]]></content>
      
      
      <categories>
          
          <category> Python </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 基础知识 </tag>
            
            <tag> Python </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>C++结构体</title>
      <link href="/2023/05/03/c-jie-gou-ti/"/>
      <url>/2023/05/03/c-jie-gou-ti/</url>
      
        <content type="html"><![CDATA[<h1 id="结构体"><a href="#结构体" class="headerlink" title="结构体"></a>结构体</h1><h2 id="1-结构体的定义和使用"><a href="#1-结构体的定义和使用" class="headerlink" title="1. 结构体的定义和使用"></a>1. 结构体的定义和使用</h2><p>语法：<code>struct 结构体名 { 结构体成员列表 };</code></p><p>创建结构体变量时，可以省略<code>struct</code>关键字</p><p>三种创建结构体变量的方式：</p><pre class="line-numbers language-c++"><code class="language-c++">struct Student{    string name;    int age;    float score;}s3; // s3在定义结构体时创建结构体变量// struct可省略struct Student s1; s1.name = "张三";s1.age = 20;s1.score = 89.5;struct Student s2 = {"李四", 19, 80.5};<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="2-结构体数组"><a href="#2-结构体数组" class="headerlink" title="2. 结构体数组"></a>2. 结构体数组</h2><p>作用：将自定义结构体存入数组中</p><h2 id="3-结构体指针"><a href="#3-结构体指针" class="headerlink" title="3. 结构体指针"></a>3. 结构体指针</h2><p>作用：通过指针访问结构体中的成员</p><p>利用<code>-&gt;</code>可以通过结构体指针访问结构体属性</p><pre class="line-numbers language-c++"><code class="language-c++">struct Student{    string name;    int age;    float score;};Student s = {"李四", 19, 80.5};Student* p = &s;cout << p->name << p->age << p->score << endl;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="4-嵌套结构体"><a href="#4-嵌套结构体" class="headerlink" title="4. 嵌套结构体"></a>4. 嵌套结构体</h2><pre class="line-numbers language-c++"><code class="language-c++">struct Student{    string name;    int age;    float score;};struct Teacher{    int id;    string name;    int age;    Student stu;};<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">import</span> numpy<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><h2 id="5-结构体做函数参数"><a href="#5-结构体做函数参数" class="headerlink" title="5. 结构体做函数参数"></a>5. 结构体做函数参数</h2><p>函数参数有两种形式：1. 值传递。2. 地址传递</p><pre><code>struct Student{    string name;    int age;    float score;};// 值传递void myprint1(Student s){    cout &lt;&lt; s.name &lt;&lt; s.age &lt;&lt; s.score &lt;&lt; endl; }// 地指传递void myprint2(Student* s){    cout &lt;&lt; s-&gt;name &lt;&lt; s-&gt;age &lt;&lt; s-&gt;score &lt;&lt; endl;}struct Student s1; s1.name = &quot;张三&quot;;s1.age = 20;s1.score = 89.5;myprint1(s);</code></pre><ul><li><code>const</code>在结构体中使用，用<code>const</code>防止误操作</li></ul><pre><code>struct Student{    string name;    int age;    float score;};void myprint2(const Student* s){    // s-&gt;age = 150; 报错    cout &lt;&lt; s-&gt;name &lt;&lt; s-&gt;age &lt;&lt; s-&gt;score &lt;&lt; endl;}struct Student s1 = {&quot;李四&quot;, 19, 80.5}; myprint2(s);</code></pre>]]></content>
      
      
      <categories>
          
          <category> 学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> C++ </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Python打包分发工具：setuptools</title>
      <link href="/2023/04/14/python-da-bao-fen-fa-gong-ju-setuptools/"/>
      <url>/2023/04/14/python-da-bao-fen-fa-gong-ju-setuptools/</url>
      
        <content type="html"><![CDATA[<p>setuptools库的前身是distutils（一个python标准库），<strong>setuptools本身不是标准库，所以需要自行安装</strong>。setuptools提供的主要的功能有：</p><ul><li><strong>python库的打包分发</strong></li><li><strong>依赖包安装与版本管理</strong></li><li><strong>python环境限制</strong></li><li><strong>生成脚本</strong></li><li><strong>c/c++ 拓展</strong><h1 id="库的打包分发"><a href="#库的打包分发" class="headerlink" title="库的打包分发"></a>库的打包分发</h1>python库的打包分发方式有两种：<strong>源码包source dist</strong>（简称sdist）、<strong>二进制包binary dist</strong>（简称bdist）。<br>源码包sdist就是我们熟悉的 .zip 、.tar.gz 等后缀文件。就是一个压缩包，里面包含了所需要的的所有源码文件以及一些静态文件（txt文本、css、图片等）。常见的格式有：</li></ul><table><thead><tr><th>格式</th><th>后缀</th></tr></thead><tbody><tr><td>zip</td><td>.zip</td></tr><tr><td>gztar</td><td>.tar.gz</td></tr><tr><td>bztar</td><td>.tar.bz2</td></tr><tr><td>ztar</td><td>.tar.Z</td></tr><tr><td>tar</td><td>.tar</td></tr></tbody></table><p>二进制包格式是wheel（.whl后缀），它的前身是egg。wheel本质也还是一个压缩包，可以像像zip一样解压缩。<strong>与源码包相比，二进制包的特点是不用再编译，也就是安装更快！</strong></p><table><thead><tr><th>格式</th><th>后缀</th></tr></thead><tbody><tr><td>egg</td><td>.egg</td></tr><tr><td>wheel</td><td>.whl</td></tr></tbody></table><h2 id="源码包sdist"><a href="#源码包sdist" class="headerlink" title="源码包sdist"></a>源码包sdist</h2><p>打包分发源码包命令</p><pre class="line-numbers language-shell"><code class="language-shell">$ python setup.py sdist --formats=gztar<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p><code>setup.py</code>指定了打包分发的配置信息。<code>--formats</code> 参数用来指定压缩格式，若不指定format格式，那么 sdist 将根据当前平台创建默认格式：在类 Unix 平台上，将创建后缀名为.tar.gz分发包，而在Windows上为 .zip 文件。<br>安装源码包命令。安装源码包有两种方法，先解压缩源码包，或者直接安装源码包。</p><ol><li><p>先解压缩源码包，再执行setup.py。</p><pre class="line-numbers language-shell"><code class="language-shell">$ python setup.py install等价于$ python setup.py build$ python setup.py install<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre></li><li><p>直接pip安装源码包</p><pre class="line-numbers language-shell"><code class="language-shell">$ pip install  xxx.tar.gz<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><h2 id="二进制包bdist"><a href="#二进制包bdist" class="headerlink" title="二进制包bdist"></a>二进制包bdist</h2><p>打包分发二进制包命令。</p></li></ol><table><thead><tr><th>命令</th><th>format参数</th><th>note</th></tr></thead><tbody><tr><td>bdist_dumb</td><td>tar,gztar,zip……</td><td>windows默认zip，Unix默认gztar</td></tr><tr><td>bdist_rpm</td><td>rpm,srpm</td><td></td></tr><tr><td>bdist_wininst</td><td>wininst</td><td></td></tr><tr><td>bdist_wheel</td><td>wheel</td><td>目前主流的二进制包，需要先安装wheel</td></tr><tr><td>bdist_egg</td><td>egg</td><td></td></tr></tbody></table><pre class="line-numbers language-shell"><code class="language-shell">$ python setup.py bdist_wheel<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>安装二进制包命令</p><pre class="line-numbers language-shell"><code class="language-shell">$ pip install xxx.whl<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><h2 id="eggs-与-wheels-有什么区别？"><a href="#eggs-与-wheels-有什么区别？" class="headerlink" title="eggs 与 wheels 有什么区别？"></a>eggs 与 wheels 有什么区别？</h2><p>Egg 格式是由 setuptools 在 2004 年引入，而 Wheel 格式是由 PEP427 在 2012 年定义。Wheel 的出现是为了替代 Egg，它的本质是一个zip包，其现在被认为是 Python 的二进制包的标准格式。<br>以下是 Wheel 和 Egg 的主要区别：</p><ul><li>Wheel 有一个官方的 PEP427 来定义，而 Egg 没有 PEP 定义</li><li>Wheel 是一种分发格式，即打包格式。而 Egg 既是一种分发格式，也是一种运行时安装的格式，并且是可以被直接 import</li><li>Wheel 文件不会包含 .pyc 文件</li><li>Wheel 使用和 PEP376 兼容的 .dist-info 目录，而 Egg 使用 .egg-info 目录</li><li>Wheel 有着更丰富的命名规则。</li><li>Wheel 是有版本的。每个 Wheel 文件都包含 wheel 规范的版本和打包的实现</li><li>Wheel 在内部被 sysconfig path type 管理，因此转向其他格式也更容易</li></ul><p>上面我们讲述了python打包分发的两种方法，很容易发现整个打包过程最重要的就是setup.py，它指定了重要的配置信息。包含以下信息：</p><ul><li><strong>python库的基本信息（作者、联系方式、当前库的版本等）</strong></li><li><strong>需要打包的文件</strong></li><li><strong>依赖包安装与版本管理</strong></li><li><strong>python环境限制</strong></li><li><strong>生成脚本</strong></li><li><strong>c/c++ 拓展</strong></li><li><strong>cmdclass自定义命令行为</strong><h1 id="setup函数的配置信息"><a href="#setup函数的配置信息" class="headerlink" title="setup函数的配置信息"></a>setup函数的配置信息</h1>setup.py 的参数非常多，能够不借助文档写好一个setup.py好像没那么简单。<br><img src="./imgs/setuptools.png" alt><h2 id="python库的基本信息"><a href="#python库的基本信息" class="headerlink" title="python库的基本信息"></a><strong>python库的基本信息</strong></h2><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">from</span> setuptools <span class="token keyword">import</span> setup<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre></li></ul><p>def readme():<br>    with open(‘README.md’, encoding=’utf-8’) as f:<br>        content = f.read()<br>    return content</p><p>setup(<br>    name = ‘myapp’, # 包名称<br>    version = ‘1.0’, # 版本<br>    author = ‘lihua’, # 作者<br>    author_email = ‘lihua@163.com’, # 作者邮箱<br>    description=’a example for pack python’, # 描述<br>    long_description=readme(), # 长文描述<br>    long_description_content_type=’text/markdown’, # 长文描述的文本格式<br>    keywords=’pack’, # 关键词<br>    url=’<a href="https://github.com/lihua/myapp&#39;">https://github.com/lihua/myapp&#39;</a>, # 项目主页<br>    classifiers=[ # 包的分类信息，见<a href="https://pypi.org/pypi?%3Aaction=list_classifiers" target="_blank" rel="noopener">https://pypi.org/pypi?%3Aaction=list_classifiers</a><br>            ‘Development Status :: 5 - Production/Stable’,<br>            ‘License :: OSI Approved :: Apache Software License’,<br>            ‘Operating System :: OS Independent’,<br>            ‘Programming Language :: Python :: 3’,<br>            ‘Programming Language :: Python :: 3.6’,<br>            ‘Programming Language :: Python :: 3.7’,<br>            ‘Programming Language :: Python :: 3.8’,<br>            ‘Programming Language :: Python :: 3.9’,<br>        ],<br>    license=’Apache License 2.0’, # 许可证<br>)</p><pre><code>## 需要打包的文件通过setup函数的这些参数`packages`、`include_package_data`（其实就是MANIFEST.in文件）、`exclude_package_data`、`package_data`、`data_files`来指定需要打包的文件。包含的文件如下：- `py_modules`和`packages`参数中所有 Python 源文件- `ext_modules`或是`libraries` 参数中提到的所有 C 源文件- `scripts` 参数指定的脚本- `package_data`和`data_files` 参数指定的所有文件- `setup.cfg`和`setup.py`- 类似于readme的文件（如README、README.txt、 README.rst、README.md）- `MANIFEST.in` 中指定的所有文件### packages参数packages参数就是用来指示打包分发时需要包含的package，类型**为list[str]**。**但是它不会递归的打包子package！只打包当前package！**所以setuptools提供了两个函数`find_namespace_packages()`，`find_packages`来快速找到所有的package。**python中的packages有两种**，一种是包含__init__.py的文件夹（姑且叫做**普通package**），一种是不含__init__.py的文件夹（这是python3引入的**Namespace Packages**命名空间包）。顾名思义，`find_packages`只会打包内含__init__.py的package。可以给`find_namespace_packages`传递参数以指定在哪个文件夹下进行搜索，比如`setup(packages=find_namespace_packages(&#39;src&#39;))`。### 打包非源码文件上面这些例子中都没有包含非源码文件（如.dat和.txt文件），需要通过别的参数`include_package_data`（其实就是`MANIFEST.in`文件）、`exclude_package_data`、`package_data`来打包**非源码文件。**#### include_package_data参数include_package_data是bool类型，默认值为True。**当为True时，将根据**`MANIFEST.in`**文件来打包分发库**。`MANIFEST.in`文件指定了一些语法规则，主要是用来打包非源码文件的，语法规则如下：| 命令 | 描述 || --- | --- || include pat1 pat2 ... | 添加与任何列出的模式匹配的所有文件（文件必须作为相对于项目根目录的路径给出） || exclude pat1 pat2 ... | 删除与任何列出的模式匹配的所有文件（文件必须作为相对于项目根目录的路径给出） || recursive-include dir-pattern pat1 pat2 ... | 递归dir-pattern及其子文件夹，添加与任何列出的模式匹配的目录下的所有文件 || recursive-exclude dir-pattern pat1 pat2 ... | 递归dir-pattern及其子文件夹，删除与任何列出的模式匹配的目录下的所有文件 || global-include pat1 pat2 ... | 在源树中的任何位置添加与任何列出的模式匹配的所有文件 || global-exclude pat1 pat2 ... | 删除源树中与任何列出的模式匹配的所有文件 || graft dir-pattern | 添加匹配目录下的所有文件 dir-pattern || prune dir-pattern | 删除匹配目录下的所有文件 dir-pattern |</code></pre><p>└── D:\workplace\python\pack_test<br>    ├──setup.py<br>    ├──MANIFEST.in<br>    ├──debug<br>    │   ├──debug.py<br>    ├──src<br>    │   ├──<strong>init</strong>.py<br>    │   ├──pack1<br>        │    ├──<strong>init</strong>.py<br>        │    ├──main.py<br>        │    ├──config.txt<br>             ├──data<br>             │   ├──main.py<br>             │   ├──a.dat</p><pre><code>则`MANIFEST.in`文件的内容如下：</code></pre><h1 id="递归遍历当前文件夹，找到符合-dat、-txt的文件"><a href="#递归遍历当前文件夹，找到符合-dat、-txt的文件" class="headerlink" title="递归遍历当前文件夹，找到符合.dat、.txt的文件"></a>递归遍历当前文件夹，找到符合<em>.dat、</em>.txt的文件</h1><p>recursive-include . *.txt *.dat </p><h1 id="或者"><a href="#或者" class="headerlink" title="或者"></a>或者</h1><p>include src/pack1/<em>.txt<br>include src/pack1/data/</em>.dat</p><pre><code>#### package_data参数还可以通过package_data参数来指定，这边建议还是统一用`MANIFEST.in`文件的方式，免得造成不一致性。```pythonfrom setuptools import setupsetup(package_data={&#39;&#39;:[&#39;*.txt&#39;],&#39;src.pk1&#39;:[&#39;*.dat&#39;]}) # 其中&#39;&#39;表示所有文件夹下</code></pre><h4 id="exclude-package-data参数"><a href="#exclude-package-data参数" class="headerlink" title="exclude_package_data参数"></a>exclude_package_data参数</h4><p>顾名思义就是去除文件</p><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">from</span> setuptools <span class="token keyword">import</span> setupsetup<span class="token punctuation">(</span>exclude_package_data<span class="token operator">=</span><span class="token punctuation">{</span><span class="token string">'src.pk1'</span><span class="token punctuation">:</span><span class="token punctuation">[</span><span class="token string">'*.txt'</span><span class="token punctuation">]</span><span class="token punctuation">}</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><h2 id="依赖包安装与版本管理"><a href="#依赖包安装与版本管理" class="headerlink" title="依赖包安装与版本管理"></a><strong>依赖包安装与版本管理</strong></h2><p>一个项目库可能会依赖于很多其他库，比如安装pandas，该库依赖于numpy。当用pip或conda这些命令安装时，从来不用操心哪些依赖包需要安装，它们的版本限制是怎么样的，而这些信息是setuptools打包分发库时就确定的。所以当setuptools打包分发库时，<strong>要指定依赖包有哪些？它们又有什么限制？</strong><br>针对依赖包安装与版本管理这项功能，setup函数提供了一些参数<code>install_requires</code>、<code>setup_requires</code>、<code>tests_require</code> 、<code>extras_require</code>。</p><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">from</span> setuptools <span class="token keyword">import</span> setup<span class="token punctuation">,</span> find_packagessetup<span class="token punctuation">(</span>    <span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span>    <span class="token comment" spellcheck="true"># 表明当前模块依赖哪些包，若环境中没有，则会从pypi中自动下载安装！！！</span>    install_requires<span class="token operator">=</span><span class="token punctuation">[</span><span class="token string">'docutils>=0.3'</span><span class="token punctuation">]</span><span class="token punctuation">,</span>    <span class="token comment" spellcheck="true"># setup.py 本身要依赖的包，这通常是为一些setuptools的插件准备的配置，这里列出的包，不会自动安装。</span>    setup_requires<span class="token operator">=</span><span class="token punctuation">[</span><span class="token string">'pbr'</span><span class="token punctuation">]</span><span class="token punctuation">,</span>    <span class="token comment" spellcheck="true"># 仅在测试时需要使用的依赖，在正常发布的代码中是没有用的。</span>    <span class="token comment" spellcheck="true"># 在执行python setup.py test时，可以自动安装这三个库，确保测试的正常运行。</span>    tests_require<span class="token operator">=</span><span class="token punctuation">[</span>        <span class="token string">'pytest>=3.3.1'</span><span class="token punctuation">,</span>        <span class="token string">'pytest-cov>=2.5.1'</span><span class="token punctuation">,</span>    <span class="token punctuation">]</span><span class="token punctuation">,</span>    <span class="token comment" spellcheck="true"># install_requires 在安装模块时会自动安装依赖包</span>    <span class="token comment" spellcheck="true"># 而 extras_require 不会，这里仅表示该模块会依赖这些包</span>    <span class="token comment" spellcheck="true"># 但是这些包通常不会使用到，只有当你深度使用模块时，才会用到，这里需要你手动安装</span>    extras_require<span class="token operator">=</span><span class="token punctuation">{</span>        <span class="token string">'PDF'</span><span class="token punctuation">:</span>  <span class="token punctuation">[</span><span class="token string">"ReportLab>=1.2"</span><span class="token punctuation">,</span> <span class="token string">"RXP"</span><span class="token punctuation">]</span><span class="token punctuation">,</span>        <span class="token string">'reST'</span><span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token string">"docutils>=0.3"</span><span class="token punctuation">]</span><span class="token punctuation">,</span>    <span class="token punctuation">}</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>关于 install_requires， 有以下四种常用的表示方法：</p><ol><li><code>argparse</code>：只包含包名。 这种形式只检查包的存在性，不检查版本。 方便，但不利于控制风险。</li><li><code>setuptools==38.2.4</code>，指定版本。 这种形式把风险降到了最低，确保了开发、测试与部署的版本一致，不会出现意外。 缺点是不利于更新，每次更新都需要改动代码。</li><li><code>docutils &gt;= 0.3</code>，这是比较常用的形式。 当对某个库比较信任时，这种形式可以自动保持版本为最新。</li><li><code>Django &gt;= 1.11, != 1.11.1, &lt;= 2</code>，这是比较复杂的形式。 如这个例子，保证了Django的大版本在1.11和2之间，也即1.11.x；并且，排除了已知有问题的版本1.11.1（仅举例）。 对于一些大型、复杂的库，这种形式是最合适的。<h2 id="python环境限制"><a href="#python环境限制" class="headerlink" title="python环境限制"></a>python环境限制</h2><pre class="line-numbers language-python"><code class="language-python">setup<span class="token punctuation">(</span>python_requires<span class="token operator">=</span><span class="token string">'>=2.7, &lt;=3'</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><h2 id="生成脚本"><a href="#生成脚本" class="headerlink" title="生成脚本"></a>生成脚本</h2>有时候我们的库包含了一些非常重要的功能，每次都提供python XXX.py来运行不太方便，最好是把脚本放入系统环境path，以命令行的形式来执行。比如tensorRT就提供了trtexec命令。<br>那么setup函数提供了entry_points和scripts这两个参数。它们的区别在于：</li></ol><ul><li>entry_points是把<strong>python文件中的函数</strong>自动生成为可执行脚本</li><li>scripts是把<strong>.sh、.py等可执行脚本</strong>生成到系统path中<pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">from</span> setuptools <span class="token keyword">import</span> setup<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre></li></ul><p>setup(<br>    …………<br>    # 把python中的函数自动生成为一个可执行的脚本<br>    # 如下：把fool.main文件中的main函数自动生成为一个可执行脚本，可以通过命令foo执行该脚本<br>    entry_points={<br>        ‘console_scripts’: [ # key值为console_scripts<br>            ‘foo = foo.main:main’ # 格式为’命令名 = 模块名:函数名’<br>        ]<br>    },<br>    # 将 bin/foo.sh 和 bar.py 脚本，生成到系统 PATH中<br>    # 执行 python setup.py install 后<br>    # 会生成 如 /usr/bin/foo.sh 和 如 /usr/bin/bar.py<br>    scripts=[‘bin/foo.sh’, ‘bar.py’]</p><p>)</p><pre><code>## C/C++扩展编译c/c++拓展源码的命令为：`python setup.py build_ext --inplace`。或者直接`python setup.py build`该命令包括了build_ext步骤。那么我们该如何指导编译器编译c/c++源码呢。本质上setuptools是根据setup.py配置来**指导生成gcc命令行**，当然你也可以粗暴地直接用gcc命令行来编译c/c++拓展源码，但工程量太大，setuptools支持很多混合编程技术cython、SWIG等等。所以甭管你采用什么混合编程技术，绕不开setuptools。setuptools编译c/c++拓展源码的过程主要是把**源代码**编译成**动态连接库**（linux下是**.so**，windows下是**.pyd**）。这样就可以在.py中愉快import并使用拓展模块了。主要看setup函数的**ext_modules**参数，该参数type为list[setuptools.Extension]。所以编译核心就在于这个**setuptools.Extension**类，该类只支持**c/c++**拓展，要实现cuda拓展需要自定义Extension类，如pytorch的CUDAExtension。setuptools.Extension类有几个重要的构造参数。- name：在python中import该拓展的名称- sources：源代码文件名- language：默认&#39;c&#39;，如果要用C++，改成&#39;c++&#39;- include_dirs：其实就是传递给 gcc 的 -I(大写i)指定include的头文件目录- library_dirs：其实就是传递给 gcc 的 -L 指定连接文件的目录- libraries：其实就是传给 gcc 的 -l(小写的L)指定连接文件，在L指定的位置找- extra_compile_args：其实传给 gcc 的额外的编译参数，比方&#39;-std=c++11&#39;- extra_link_args：其实传给 gcc 的额外的链接参数（生成动态链接库）- define_macros：定义宏- undef_macros：取消定义宏```pythonfrom setuptools import setup,Extensionsetup( ext_modules=[    Extension(        name=&#39;foo&#39;,  # type=str。并且还支持层级命名，如myapp.foo        # type=list[str]。源代码的文件名，可以用glob.glob查找所有.c文件        sources=[&#39;foo/csrc/foo1.c&#39;,&#39;foo/csrc/foo2.c&#39;],          include_dirs=[&#39;foo&#39;], # type=list[str]。拓展include头文件，相当于传递给gcc -I         )]    )</code></pre><p>setuptools.Extension用define_macros 和 undef_macros构造参数来定义或取消定义宏。define_macros的type为list[tuple( name:str , value:str|None )] 。值为 None 的宏 FOO 等价于#define FOO ，否则等价于# define FOO value值 。undef_macros 同理，等价于#undef FOO 。</p><pre class="line-numbers language-python"><code class="language-python">Extension<span class="token punctuation">(</span>define_macros<span class="token operator">=</span><span class="token punctuation">[</span><span class="token punctuation">(</span><span class="token string">'NDEBUG'</span><span class="token punctuation">,</span> <span class="token string">'1'</span><span class="token punctuation">)</span><span class="token punctuation">,</span><span class="token punctuation">(</span><span class="token string">'HAVE_STRFTIME'</span><span class="token punctuation">,</span> None<span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">,</span>          undef_macros<span class="token operator">=</span><span class="token punctuation">[</span><span class="token string">'HAVE_FOO'</span><span class="token punctuation">,</span> <span class="token string">'HAVE_BAR'</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p>上面的代码相当于在每个C文件前加上了：</p><pre class="line-numbers language-c"><code class="language-c"><span class="token macro property">#<span class="token directive keyword">define</span> NDEBUG 1</span><span class="token macro property">#<span class="token directive keyword">define</span> HAVE_STRFTIME</span><span class="token macro property">#<span class="token directive keyword">undef</span> HAVE_FOO</span><span class="token macro property">#<span class="token directive keyword">undef</span> HAVE_BAR</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre>]]></content>
      
      
      <categories>
          
          <category> Python </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Python </tag>
            
            <tag> 工具 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Pytorch|动态计算图</title>
      <link href="/2023/04/10/05-dong-tai-ji-suan-tu/"/>
      <url>/2023/04/10/05-dong-tai-ji-suan-tu/</url>
      
        <content type="html"><![CDATA[<p>本节我们将介绍 Pytorch的动态计算图。<br>包括： </p><ul><li>动态计算图简介</li><li>计算图中的Function</li><li>计算图和反向传播</li><li>叶子节点和非叶子节点<h1 id="动态图简介"><a href="#动态图简介" class="headerlink" title="动态图简介"></a>动态图简介</h1>Pytorch的计算图由节点和边组成，节点表示张量或者Function，边表示张量和Function之间的依赖关系。<br>Pytorch中的计算图是动态图。这里的动态主要有两重含义。</li></ul><ol><li>计算图的正向传播是立即执行的。无需等待完整的计算图创建完毕，每条语句都会在计算图中动态添加节点和边，并立即执行正向传播得到计算结果。</li><li>计算图在反向传播后立即销毁，下次调用需要重新构建计算图。如果在程序中使用了backward方法执行了反向传播，或者利用torch.autograd.grad方法计算了梯度，那么创建的计算图会被立即销毁，释放存储空间，下次调用需要重新创建。</li></ol><p><strong>计算图的正向传播是立即执行的</strong>。</p><pre class="line-numbers language-python"><code class="language-python">w <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">3.0</span><span class="token punctuation">,</span><span class="token number">1.0</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">,</span>requires_grad<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>b <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">3.0</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">,</span>requires_grad<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>X <span class="token operator">=</span> torch<span class="token punctuation">.</span>randn<span class="token punctuation">(</span><span class="token number">10</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">)</span>Y <span class="token operator">=</span> torch<span class="token punctuation">.</span>randn<span class="token punctuation">(</span><span class="token number">10</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">)</span>Y_hat <span class="token operator">=</span> X@w<span class="token punctuation">.</span>t<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">+</span> b  <span class="token comment" spellcheck="true"># Y_hat定义后其正向传播被立即执行，与其后面的loss创建语句无关</span>loss <span class="token operator">=</span> torch<span class="token punctuation">.</span>mean<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>pow<span class="token punctuation">(</span>Y_hat<span class="token operator">-</span>Y<span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>loss<span class="token punctuation">.</span>data<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>Y_hat<span class="token punctuation">.</span>data<span class="token punctuation">)</span><span class="token operator">>></span><span class="token operator">></span>Output<span class="token punctuation">:</span>tensor<span class="token punctuation">(</span><span class="token number">36.4831</span><span class="token punctuation">)</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">0.0435</span><span class="token punctuation">]</span><span class="token punctuation">,</span>        <span class="token punctuation">[</span> <span class="token number">3.1951</span><span class="token punctuation">]</span><span class="token punctuation">,</span>        <span class="token punctuation">[</span> <span class="token number">5.3738</span><span class="token punctuation">]</span><span class="token punctuation">,</span>        <span class="token punctuation">[</span> <span class="token number">4.2230</span><span class="token punctuation">]</span><span class="token punctuation">,</span>        <span class="token punctuation">[</span><span class="token number">14.0631</span><span class="token punctuation">]</span><span class="token punctuation">,</span>        <span class="token punctuation">[</span> <span class="token number">1.2098</span><span class="token punctuation">]</span><span class="token punctuation">,</span>        <span class="token punctuation">[</span> <span class="token number">4.8157</span><span class="token punctuation">]</span><span class="token punctuation">,</span>        <span class="token punctuation">[</span> <span class="token number">2.8586</span><span class="token punctuation">]</span><span class="token punctuation">,</span>        <span class="token punctuation">[</span> <span class="token number">3.0681</span><span class="token punctuation">]</span><span class="token punctuation">,</span>        <span class="token punctuation">[</span> <span class="token number">8.1599</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p><strong>计算图在反向传播后立即销毁。</strong><br>计算图在反向传播后立即销毁，如果需要保留计算图, 需要设置<code>retain_graph = True</code>。</p><pre class="line-numbers language-python"><code class="language-python">w <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">3.0</span><span class="token punctuation">,</span><span class="token number">1.0</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">,</span>requires_grad<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>b <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">3.0</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">,</span>requires_grad<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>X <span class="token operator">=</span> torch<span class="token punctuation">.</span>randn<span class="token punctuation">(</span><span class="token number">10</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">)</span>Y <span class="token operator">=</span> torch<span class="token punctuation">.</span>randn<span class="token punctuation">(</span><span class="token number">10</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">)</span>Y_hat <span class="token operator">=</span> X@w<span class="token punctuation">.</span>t<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">+</span> b  <span class="token comment" spellcheck="true"># Y_hat定义后其正向传播被立即执行，与其后面的loss创建语句无关</span>loss <span class="token operator">=</span> torch<span class="token punctuation">.</span>mean<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>pow<span class="token punctuation">(</span>Y_hat<span class="token operator">-</span>Y<span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">)</span>loss<span class="token punctuation">.</span>backward<span class="token punctuation">(</span><span class="token punctuation">)</span>  <span class="token comment" spellcheck="true">#loss.backward(retain_graph = True) </span><span class="token comment" spellcheck="true">#loss.backward() #如果再次执行反向传播将报错</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h1 id="计算图中的Function"><a href="#计算图中的Function" class="headerlink" title="计算图中的Function"></a>计算图中的Function</h1><p>计算图中的张量我们已经比较熟悉了，计算图中的另外一种节点是Function，实际上就是 Pytorch中各种对张量操作的函数。<br>这些Function和我们Python中的函数有一个较大的区别，那就是它同时包括正向计算逻辑和反向传播的逻辑。我们可以通过继承<code>torch.autograd.Function</code>来创建这种支持反向传播的Function。</p><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">MyReLU</span><span class="token punctuation">(</span>torch<span class="token punctuation">.</span>autograd<span class="token punctuation">.</span>Function<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token comment" spellcheck="true">#正向传播逻辑，可以用ctx存储一些值，供反向传播使用。</span>    @staticmethod    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>ctx<span class="token punctuation">,</span> input<span class="token punctuation">)</span><span class="token punctuation">:</span>        ctx<span class="token punctuation">.</span>save_for_backward<span class="token punctuation">(</span>input<span class="token punctuation">)</span>        <span class="token keyword">return</span> input<span class="token punctuation">.</span>clamp<span class="token punctuation">(</span>min<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span>    <span class="token comment" spellcheck="true">#反向传播逻辑</span>    @staticmethod    <span class="token keyword">def</span> <span class="token function">backward</span><span class="token punctuation">(</span>ctx<span class="token punctuation">,</span> grad_output<span class="token punctuation">)</span><span class="token punctuation">:</span>        input<span class="token punctuation">,</span> <span class="token operator">=</span> ctx<span class="token punctuation">.</span>saved_tensors        grad_input <span class="token operator">=</span> grad_output<span class="token punctuation">.</span>clone<span class="token punctuation">(</span><span class="token punctuation">)</span>        grad_input<span class="token punctuation">[</span>input <span class="token operator">&lt;</span> <span class="token number">0</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token number">0</span>        <span class="token keyword">return</span> grad_input<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><pre class="line-numbers language-python"><code class="language-python">w <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">3.0</span><span class="token punctuation">,</span><span class="token number">1.0</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">,</span>requires_grad<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>b <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">3.0</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">,</span>requires_grad<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>X <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">1.0</span><span class="token punctuation">,</span><span class="token operator">-</span><span class="token number">1.0</span><span class="token punctuation">]</span><span class="token punctuation">,</span><span class="token punctuation">[</span><span class="token number">1.0</span><span class="token punctuation">,</span><span class="token number">1.0</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span>Y <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">2.0</span><span class="token punctuation">,</span><span class="token number">3.0</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span>relu <span class="token operator">=</span> MyReLU<span class="token punctuation">.</span>apply <span class="token comment" spellcheck="true"># relu现在也可以具有正向传播和反向传播功能</span>Y_hat <span class="token operator">=</span> relu<span class="token punctuation">(</span>X@w<span class="token punctuation">.</span>t<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">+</span> b<span class="token punctuation">)</span>loss <span class="token operator">=</span> torch<span class="token punctuation">.</span>mean<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>pow<span class="token punctuation">(</span>Y_hat<span class="token operator">-</span>Y<span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">)</span>loss<span class="token punctuation">.</span>backward<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>w<span class="token punctuation">.</span>grad<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>b<span class="token punctuation">.</span>grad<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>Y_hat<span class="token punctuation">.</span>grad_fn<span class="token punctuation">)</span><span class="token operator">>></span><span class="token operator">></span>Output<span class="token punctuation">:</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">4.5000</span><span class="token punctuation">,</span> <span class="token number">4.5000</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">4.5000</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token operator">&lt;</span>torch<span class="token punctuation">.</span>autograd<span class="token punctuation">.</span>function<span class="token punctuation">.</span>MyReLUBackward object at <span class="token number">0x7f6979f47200</span><span class="token operator">></span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h1 id="计算图与反向传播"><a href="#计算图与反向传播" class="headerlink" title="计算图与反向传播"></a>计算图与反向传播</h1><p>了解了Function的功能，我们可以简单地理解一下反向传播的原理和过程。理解该部分原理需要一些高等数学中求导链式法则的基础知识。</p><pre class="line-numbers language-python"><code class="language-python">x <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token number">3.0</span><span class="token punctuation">,</span>requires_grad<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>y1 <span class="token operator">=</span> x <span class="token operator">+</span> <span class="token number">1</span>y2 <span class="token operator">=</span> <span class="token number">2</span><span class="token operator">*</span>xloss <span class="token operator">=</span> <span class="token punctuation">(</span>y1<span class="token operator">-</span>y2<span class="token punctuation">)</span><span class="token operator">**</span><span class="token number">2</span>loss<span class="token punctuation">.</span>backward<span class="token punctuation">(</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p><code>loss.backward()</code>语句调用后，依次发生以下计算过程。</p><ol><li>loss自己的grad梯度赋值为1，即对自身的梯度为1。</li><li>loss根据其自身梯度以及关联的backward方法，计算出其对应的自变量即y1和y2的梯度，将该值赋值到y1.grad和y2.grad。</li><li>y2和y1根据其自身梯度以及关联的backward方法, 分别计算出其对应的自变量x的梯度，x.grad将其收到的多个梯度值累加。</li></ol><p>注意，1,2,3步骤的求梯度顺序和对多个梯度值的累加规则恰好是求导链式法则的程序表述。<br>正因为求导链式法则衍生的梯度累加规则，张量的grad梯度不会自动清零，在需要的时候需要手动置零。</p><h1 id="叶子节点和非叶子节点"><a href="#叶子节点和非叶子节点" class="headerlink" title="叶子节点和非叶子节点"></a>叶子节点和非叶子节点</h1><p>执行下面代码，我们会发现 loss.grad并不是我们期望的1，而是 None。类似地 y1.grad 以及 y2.grad也是 None。<br>这是为什么呢？这是由于它们不是叶子节点张量。<br>在反向传播过程中，只有 is_leaf=True 的叶子节点，需要求导的张量的导数结果才会被最后保留下来。那么什么是叶子节点张量呢？叶子节点张量需要满足两个条件。</p><ol><li>叶子节点张量是由用户直接创建的张量，而非由某个Function通过计算得到的张量。</li><li>叶子节点张量的 requires_grad属性必须为True.</li></ol><p>Pytorch设计这样的规则主要是为了节约内存或者显存空间，因为几乎所有的时候，用户只会关心他自己直接创建的张量的梯度。<br>所有依赖于叶子节点张量的张量, 其requires_grad 属性必定是True的，但其梯度值只在计算过程中被用到，不会最终存储到grad属性中。<br>如果需要保留中间计算结果的梯度到grad属性中，可以使用 retain_grad方法。如果仅仅是为了调试代码查看梯度值，可以利用register_hook打印日志。</p><pre class="line-numbers language-python"><code class="language-python">x <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token number">3.0</span><span class="token punctuation">,</span>requires_grad<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>y1 <span class="token operator">=</span> x <span class="token operator">+</span> <span class="token number">1</span>y2 <span class="token operator">=</span> <span class="token number">2</span><span class="token operator">*</span>xloss <span class="token operator">=</span> <span class="token punctuation">(</span>y1<span class="token operator">-</span>y2<span class="token punctuation">)</span><span class="token operator">**</span><span class="token number">2</span>loss<span class="token punctuation">.</span>backward<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"loss.grad:"</span><span class="token punctuation">,</span> loss<span class="token punctuation">.</span>grad<span class="token punctuation">)</span> <span class="token comment" spellcheck="true"># non-leaf</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"y1.grad:"</span><span class="token punctuation">,</span> y1<span class="token punctuation">.</span>grad<span class="token punctuation">)</span> <span class="token comment" spellcheck="true"># non-leaf</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"y2.grad:"</span><span class="token punctuation">,</span> y2<span class="token punctuation">.</span>grad<span class="token punctuation">)</span> <span class="token comment" spellcheck="true"># non-leaf</span><span class="token keyword">print</span><span class="token punctuation">(</span>x<span class="token punctuation">.</span>grad<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>x<span class="token punctuation">.</span>is_leaf<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>y1<span class="token punctuation">.</span>is_leaf<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>y2<span class="token punctuation">.</span>is_leaf<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>loss<span class="token punctuation">.</span>is_leaf<span class="token punctuation">)</span><span class="token operator">>></span><span class="token operator">></span>Output<span class="token punctuation">:</span>loss<span class="token punctuation">.</span>grad<span class="token punctuation">:</span> Noney1<span class="token punctuation">.</span>grad<span class="token punctuation">:</span> Noney2<span class="token punctuation">.</span>grad<span class="token punctuation">:</span> Nonetensor<span class="token punctuation">(</span><span class="token number">4</span><span class="token punctuation">.</span><span class="token punctuation">)</span><span class="token boolean">True</span><span class="token boolean">False</span><span class="token boolean">False</span><span class="token boolean">False</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>利用<code>retain_grad</code>可以保留非叶子节点的梯度值。利用<code>register_hook</code>可以查看非叶子节点的梯度值。</p><pre class="line-numbers language-python"><code class="language-python">x <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token number">3.0</span><span class="token punctuation">,</span>requires_grad<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>y1 <span class="token operator">=</span> x <span class="token operator">+</span> <span class="token number">1</span>y2 <span class="token operator">=</span> <span class="token number">2</span><span class="token operator">*</span>xloss <span class="token operator">=</span> <span class="token punctuation">(</span>y1<span class="token operator">-</span>y2<span class="token punctuation">)</span><span class="token operator">**</span><span class="token number">2</span><span class="token comment" spellcheck="true"># 非叶子节点梯度显示控制</span>y1<span class="token punctuation">.</span>register_hook<span class="token punctuation">(</span><span class="token keyword">lambda</span> grad<span class="token punctuation">:</span> <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'y1 grad: '</span><span class="token punctuation">,</span> grad<span class="token punctuation">)</span><span class="token punctuation">)</span>y2<span class="token punctuation">.</span>register_hook<span class="token punctuation">(</span><span class="token keyword">lambda</span> grad<span class="token punctuation">:</span> <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'y2 grad: '</span><span class="token punctuation">,</span> grad<span class="token punctuation">)</span><span class="token punctuation">)</span>loss<span class="token punctuation">.</span>retain_grad<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true"># 反向传播</span>loss<span class="token punctuation">.</span>backward<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"loss.grad:"</span><span class="token punctuation">,</span> loss<span class="token punctuation">.</span>grad<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"x.grad:"</span><span class="token punctuation">,</span> x<span class="token punctuation">.</span>grad<span class="token punctuation">)</span><span class="token operator">>></span><span class="token operator">></span>Output<span class="token punctuation">:</span>y2 grad<span class="token punctuation">:</span>  tensor<span class="token punctuation">(</span><span class="token number">4</span><span class="token punctuation">.</span><span class="token punctuation">)</span>y1 grad<span class="token punctuation">:</span>  tensor<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">4</span><span class="token punctuation">.</span><span class="token punctuation">)</span>loss<span class="token punctuation">.</span>grad<span class="token punctuation">:</span> tensor<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">.</span><span class="token punctuation">)</span>x<span class="token punctuation">.</span>grad<span class="token punctuation">:</span> tensor<span class="token punctuation">(</span><span class="token number">4</span><span class="token punctuation">.</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>]]></content>
      
      
      <categories>
          
          <category> Pytorch </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 基础知识 </tag>
            
            <tag> pytorch </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Pytorch|自动微分和反向求导</title>
      <link href="/2023/04/08/04-zi-dong-wei-fen-yu-backward/"/>
      <url>/2023/04/08/04-zi-dong-wei-fen-yu-backward/</url>
      
        <content type="html"><![CDATA[<p>神经网络通常依赖反向传播求梯度来更新网络参数，求梯度过程通常是一件非常复杂而容易出错的事情。而深度学习框架可以帮助我们自动地完成这种求梯度运算。<br>Pytorch一般通过反向传播<code>backward()</code>方法实现这种求梯度计算。该方法求得的梯度将存在对应自变量张量的<code>grad</code>属性下。除此之外，也能够调用<code>torch.autograd.grad()</code>函数来实现求梯度计算。这就是Pytorch的自动微分机制。</p><h1 id="求导数"><a href="#求导数" class="headerlink" title="求导数"></a>求导数</h1><h2 id="利用backward方法求导数"><a href="#利用backward方法求导数" class="headerlink" title="利用backward方法求导数"></a>利用backward方法求导数</h2><p><code>backward()</code>方法通常在一个标量张量上调用，该方法求得的梯度将存在对应自变量张量的<code>grad</code>属性下。<br>如果调用的张量非标量，则要传入一个和它同形状的<code>gradient</code>参数张量。相当于用该<code>gradient</code>参数张量与调用张量<strong>作向量点乘再求和</strong>，得到的标量结果再反向传播。</p><h3 id="标量的反向传播"><a href="#标量的反向传播" class="headerlink" title="标量的反向传播"></a><strong>标量的反向传播</strong></h3><p>求$f(x) = a\times x^2 + b \times x + c$的导数。<br>$f(x)$的导数是$f’(x) = 2ax+b$。<br>当$x=0$时导数为-2。<br>当$x=2$时导数为2。<br>当$x=-2$时导数为-6。</p><pre class="line-numbers language-python"><code class="language-python">x <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token number">0.0</span><span class="token punctuation">,</span> requires_grad <span class="token operator">=</span> <span class="token boolean">True</span><span class="token punctuation">)</span> <span class="token comment" spellcheck="true"># x需要被求导</span>a <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token number">1.0</span><span class="token punctuation">)</span>b <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">2.0</span><span class="token punctuation">)</span>c <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token number">1.0</span><span class="token punctuation">)</span>y <span class="token operator">=</span> a<span class="token operator">*</span>torch<span class="token punctuation">.</span>pow<span class="token punctuation">(</span>x<span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">)</span> <span class="token operator">+</span> b<span class="token operator">*</span>x <span class="token operator">+</span> c <span class="token keyword">print</span><span class="token punctuation">(</span>y<span class="token punctuation">)</span>y<span class="token punctuation">.</span>backward<span class="token punctuation">(</span><span class="token punctuation">)</span>dy_dx <span class="token operator">=</span> x<span class="token punctuation">.</span>grad<span class="token keyword">print</span><span class="token punctuation">(</span>dy_dx<span class="token punctuation">)</span><span class="token operator">>></span><span class="token operator">></span>Output<span class="token punctuation">:</span>tensor<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">.</span><span class="token punctuation">,</span> grad_fn<span class="token operator">=</span><span class="token operator">&lt;</span>AddBackward0<span class="token operator">></span><span class="token punctuation">)</span>tensor<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">2</span><span class="token punctuation">.</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="非标量的反向传播"><a href="#非标量的反向传播" class="headerlink" title="非标量的反向传播"></a><strong>非标量的反向传播</strong></h3><p>求$f(x) = a\times x^2 + b \times x + c$的导数。</p><pre class="line-numbers language-python"><code class="language-python">x <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">0.0</span><span class="token punctuation">,</span><span class="token number">0.0</span><span class="token punctuation">]</span><span class="token punctuation">,</span><span class="token punctuation">[</span><span class="token number">1.0</span><span class="token punctuation">,</span><span class="token number">2.0</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">,</span>requires_grad <span class="token operator">=</span> <span class="token boolean">True</span><span class="token punctuation">)</span> <span class="token comment" spellcheck="true"># x需要被求导</span>a <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token number">1.0</span><span class="token punctuation">)</span>b <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">2.0</span><span class="token punctuation">)</span>c <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token number">1.0</span><span class="token punctuation">)</span>y <span class="token operator">=</span> a<span class="token operator">*</span>torch<span class="token punctuation">.</span>pow<span class="token punctuation">(</span>x<span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">)</span> <span class="token operator">+</span> b<span class="token operator">*</span>x <span class="token operator">+</span> c gradient <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">1.0</span><span class="token punctuation">,</span><span class="token number">1.0</span><span class="token punctuation">]</span><span class="token punctuation">,</span><span class="token punctuation">[</span><span class="token number">1.0</span><span class="token punctuation">,</span><span class="token number">1.0</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"x:\n"</span><span class="token punctuation">,</span>x<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"y:\n"</span><span class="token punctuation">,</span>y<span class="token punctuation">)</span>y<span class="token punctuation">.</span>backward<span class="token punctuation">(</span>gradient <span class="token operator">=</span> gradient<span class="token punctuation">)</span>x_grad <span class="token operator">=</span> x<span class="token punctuation">.</span>grad<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"x_grad:\n"</span><span class="token punctuation">,</span>x_grad<span class="token punctuation">)</span><span class="token operator">>></span><span class="token operator">></span>Output<span class="token punctuation">:</span>x<span class="token punctuation">:</span> tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">]</span><span class="token punctuation">,</span>        <span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">.</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">,</span> requires_grad<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>y<span class="token punctuation">:</span> tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">.</span><span class="token punctuation">]</span><span class="token punctuation">,</span>        <span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">.</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">,</span> grad_fn<span class="token operator">=</span><span class="token operator">&lt;</span>AddBackward0<span class="token operator">></span><span class="token punctuation">)</span>x_grad<span class="token punctuation">:</span> tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">2</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">2</span><span class="token punctuation">.</span><span class="token punctuation">]</span><span class="token punctuation">,</span>        <span class="token punctuation">[</span> <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span>  <span class="token number">2</span><span class="token punctuation">.</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="用标量的反向传播实现非标量的反向传播"><a href="#用标量的反向传播实现非标量的反向传播" class="headerlink" title="用标量的反向传播实现非标量的反向传播"></a><strong>用标量的反向传播实现</strong><strong>非标量的反向传播</strong></h3><p>求$f(x) = a\times x^2 + b \times x + c$的导数。</p><pre class="line-numbers language-python"><code class="language-python">x <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">0.0</span><span class="token punctuation">,</span><span class="token number">0.0</span><span class="token punctuation">]</span><span class="token punctuation">,</span><span class="token punctuation">[</span><span class="token number">1.0</span><span class="token punctuation">,</span><span class="token number">2.0</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">,</span>requires_grad <span class="token operator">=</span> <span class="token boolean">True</span><span class="token punctuation">)</span> <span class="token comment" spellcheck="true"># x需要被求导</span>a <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token number">1.0</span><span class="token punctuation">)</span>b <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">2.0</span><span class="token punctuation">)</span>c <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token number">1.0</span><span class="token punctuation">)</span>y <span class="token operator">=</span> a<span class="token operator">*</span>torch<span class="token punctuation">.</span>pow<span class="token punctuation">(</span>x<span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">)</span> <span class="token operator">+</span> b<span class="token operator">*</span>x <span class="token operator">+</span> c gradient <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">1.0</span><span class="token punctuation">,</span><span class="token number">1.0</span><span class="token punctuation">]</span><span class="token punctuation">,</span><span class="token punctuation">[</span><span class="token number">1.0</span><span class="token punctuation">,</span><span class="token number">1.0</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span>z <span class="token operator">=</span> torch<span class="token punctuation">.</span>sum<span class="token punctuation">(</span>y<span class="token operator">*</span>gradient<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"x:"</span><span class="token punctuation">,</span>x<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"y:"</span><span class="token punctuation">,</span>y<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>z<span class="token punctuation">)</span>z<span class="token punctuation">.</span>backward<span class="token punctuation">(</span><span class="token punctuation">)</span>x_grad <span class="token operator">=</span> x<span class="token punctuation">.</span>grad<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"x_grad:\n"</span><span class="token punctuation">,</span>x_grad<span class="token punctuation">)</span><span class="token operator">>></span><span class="token operator">></span>Output<span class="token punctuation">:</span>x<span class="token punctuation">:</span> tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">]</span><span class="token punctuation">,</span>        <span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">.</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">,</span> requires_grad<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>y<span class="token punctuation">:</span> tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">.</span><span class="token punctuation">]</span><span class="token punctuation">,</span>        <span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">.</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">,</span> grad_fn<span class="token operator">=</span><span class="token operator">&lt;</span>AddBackward0<span class="token operator">></span><span class="token punctuation">)</span>tensor<span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">.</span><span class="token punctuation">,</span> grad_fn<span class="token operator">=</span><span class="token operator">&lt;</span>SumBackward0<span class="token operator">></span><span class="token punctuation">)</span>x_grad<span class="token punctuation">:</span> tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">2</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">2</span><span class="token punctuation">.</span><span class="token punctuation">]</span><span class="token punctuation">,</span>        <span class="token punctuation">[</span> <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span>  <span class="token number">2</span><span class="token punctuation">.</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="利用autograd-grad方法求导数"><a href="#利用autograd-grad方法求导数" class="headerlink" title="利用autograd.grad方法求导数"></a>利用autograd.grad方法求导数</h2><p>求$f(x) = a\times x^2 + b \times x + c$的导数。<br>$f(x)$的一阶导数是$f’(x) = 2ax+b$。<br>$f(x)$的二阶导数是$f’’(x)=2a$。<br><code>torch.autograd.grad</code>方法中<code>create_graph</code>参数设置为 True 将允许创建更高阶的导数 。</p><pre class="line-numbers language-python"><code class="language-python">x <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token number">0.0</span><span class="token punctuation">,</span>requires_grad <span class="token operator">=</span> <span class="token boolean">True</span><span class="token punctuation">)</span> <span class="token comment" spellcheck="true"># x需要被求导</span>a <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token number">1.0</span><span class="token punctuation">)</span>b <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">2.0</span><span class="token punctuation">)</span>c <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token number">1.0</span><span class="token punctuation">)</span>y <span class="token operator">=</span> a<span class="token operator">*</span>torch<span class="token punctuation">.</span>pow<span class="token punctuation">(</span>x<span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">)</span> <span class="token operator">+</span> b<span class="token operator">*</span>x <span class="token operator">+</span> cgradient <span class="token operator">=</span> torch<span class="token punctuation">.</span>autograd<span class="token punctuation">.</span>grad<span class="token punctuation">(</span>y<span class="token punctuation">,</span>x<span class="token punctuation">,</span>create_graph<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>gradient<span class="token punctuation">)</span>dy_dx <span class="token operator">=</span> gradient<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token keyword">print</span><span class="token punctuation">(</span>dy_dx<span class="token punctuation">.</span>data<span class="token punctuation">)</span><span class="token comment" spellcheck="true"># 求二阶导数</span>dy2_dx2 <span class="token operator">=</span> torch<span class="token punctuation">.</span>autograd<span class="token punctuation">.</span>grad<span class="token punctuation">(</span>dy_dx<span class="token punctuation">,</span>x<span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span> <span class="token keyword">print</span><span class="token punctuation">(</span>dy2_dx2<span class="token punctuation">.</span>data<span class="token punctuation">)</span><span class="token operator">>></span><span class="token operator">></span>Output<span class="token punctuation">:</span><span class="token punctuation">(</span>tensor<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">2</span><span class="token punctuation">.</span><span class="token punctuation">,</span> grad_fn<span class="token operator">=</span><span class="token operator">&lt;</span>AddBackward0<span class="token operator">></span><span class="token punctuation">)</span><span class="token punctuation">,</span><span class="token punctuation">)</span>tensor<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">2</span><span class="token punctuation">.</span><span class="token punctuation">)</span>tensor<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">.</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p><code>torch.autograd.grad</code>允许同时对多个自变量求导数。如果有多个因变量，相当于把多个因变量的<strong>梯度结果求和</strong>。<br>$y_1 = x_1 \times x_2$<br>$y_2=x_1+x_2$<br>$\frac{\partial y_1}{\partial X} = \left[ \frac{\partial y_1}{\partial x_1} , \frac{\partial y_1}{\partial x_2} \right]= \left[x_2, x_1 \right]$<br>$\frac{\partial Y}{\partial X} =\left[ \left[ \frac{\partial y_1}{\partial x_1} , \frac{\partial y_1}{\partial x_2},  \right] \left[ \frac{\partial y_2}{\partial x_1} , \frac{\partial y_2}{\partial x_2}, \right] \right]= \begin{bmatrix}<br>x_2 &amp; x_1\<br>1 &amp; 1<br>\end{bmatrix}$</p><pre class="line-numbers language-python"><code class="language-python">x1 <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token number">1.0</span><span class="token punctuation">,</span>requires_grad <span class="token operator">=</span> <span class="token boolean">True</span><span class="token punctuation">)</span>x2 <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token number">2.0</span><span class="token punctuation">,</span>requires_grad <span class="token operator">=</span> <span class="token boolean">True</span><span class="token punctuation">)</span>y1 <span class="token operator">=</span> x1<span class="token operator">*</span>x2y2 <span class="token operator">=</span> x1<span class="token operator">+</span>x2<span class="token punctuation">(</span>dy1_dx1<span class="token punctuation">,</span>dy1_dx2<span class="token punctuation">)</span> <span class="token operator">=</span> torch<span class="token punctuation">.</span>autograd<span class="token punctuation">.</span>grad<span class="token punctuation">(</span>outputs<span class="token operator">=</span>y1<span class="token punctuation">,</span>inputs <span class="token operator">=</span> <span class="token punctuation">[</span>x1<span class="token punctuation">,</span>x2<span class="token punctuation">]</span><span class="token punctuation">,</span>retain_graph <span class="token operator">=</span> <span class="token boolean">True</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>dy1_dx1<span class="token punctuation">,</span> dy1_dx2<span class="token punctuation">)</span><span class="token punctuation">(</span>dy12_dx1<span class="token punctuation">,</span>dy12_dx2<span class="token punctuation">)</span> <span class="token operator">=</span> torch<span class="token punctuation">.</span>autograd<span class="token punctuation">.</span>grad<span class="token punctuation">(</span>outputs<span class="token operator">=</span><span class="token punctuation">[</span>y1<span class="token punctuation">,</span>y2<span class="token punctuation">]</span><span class="token punctuation">,</span>inputs <span class="token operator">=</span> <span class="token punctuation">[</span>x1<span class="token punctuation">,</span>x2<span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>dy12_dx1<span class="token punctuation">,</span> dy12_dx2<span class="token punctuation">)</span><span class="token operator">>></span><span class="token operator">></span>Output<span class="token punctuation">:</span>tensor<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">.</span><span class="token punctuation">)</span> tensor<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">.</span><span class="token punctuation">)</span>tensor<span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">.</span><span class="token punctuation">)</span> tensor<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">.</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h1 id="利用自动微分和优化器求最小值"><a href="#利用自动微分和优化器求最小值" class="headerlink" title="利用自动微分和优化器求最小值"></a>利用自动微分和优化器求最小值</h1><p>求$f(x) = a\times x^2 + b \times x + c$的最小值。</p><pre class="line-numbers language-python"><code class="language-python">x <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token number">0.0</span><span class="token punctuation">,</span>requires_grad <span class="token operator">=</span> <span class="token boolean">True</span><span class="token punctuation">)</span> <span class="token comment" spellcheck="true"># x需要被求导</span>a <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token number">1.0</span><span class="token punctuation">)</span>b <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">2.0</span><span class="token punctuation">)</span>c <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token number">1.0</span><span class="token punctuation">)</span>optimizer <span class="token operator">=</span> torch<span class="token punctuation">.</span>optim<span class="token punctuation">.</span>SGD<span class="token punctuation">(</span>params<span class="token operator">=</span><span class="token punctuation">[</span>x<span class="token punctuation">]</span><span class="token punctuation">,</span>lr <span class="token operator">=</span> <span class="token number">0.1</span><span class="token punctuation">)</span><span class="token keyword">for</span> param <span class="token keyword">in</span> optimizer<span class="token punctuation">.</span>state_dict<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">print</span><span class="token punctuation">(</span>param<span class="token punctuation">,</span> optimizer<span class="token punctuation">.</span>state_dict<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">[</span>param<span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token keyword">def</span> <span class="token function">f</span><span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">:</span>    result <span class="token operator">=</span> a<span class="token operator">*</span>torch<span class="token punctuation">.</span>pow<span class="token punctuation">(</span>x<span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">)</span> <span class="token operator">+</span> b<span class="token operator">*</span>x <span class="token operator">+</span> c     <span class="token keyword">return</span><span class="token punctuation">(</span>result<span class="token punctuation">)</span><span class="token keyword">for</span> i <span class="token keyword">in</span> range<span class="token punctuation">(</span><span class="token number">10</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    optimizer<span class="token punctuation">.</span>zero_grad<span class="token punctuation">(</span><span class="token punctuation">)</span>    y <span class="token operator">=</span> f<span class="token punctuation">(</span>x<span class="token punctuation">)</span>    <span class="token keyword">print</span><span class="token punctuation">(</span>y<span class="token punctuation">)</span>    y<span class="token punctuation">.</span>backward<span class="token punctuation">(</span><span class="token punctuation">)</span>    optimizer<span class="token punctuation">.</span>step<span class="token punctuation">(</span><span class="token punctuation">)</span>    <span class="token keyword">print</span><span class="token punctuation">(</span>optimizer<span class="token punctuation">.</span>param_groups<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"y="</span><span class="token punctuation">,</span>f<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">.</span>data<span class="token punctuation">,</span><span class="token string">";"</span><span class="token punctuation">,</span><span class="token string">"x="</span><span class="token punctuation">,</span>x<span class="token punctuation">.</span>data<span class="token punctuation">)</span><span class="token operator">>></span><span class="token operator">></span>Output<span class="token punctuation">:</span>state <span class="token punctuation">{</span><span class="token punctuation">}</span>param_groups <span class="token punctuation">[</span><span class="token punctuation">{</span><span class="token string">'lr'</span><span class="token punctuation">:</span> <span class="token number">0.1</span><span class="token punctuation">,</span> <span class="token string">'momentum'</span><span class="token punctuation">:</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token string">'dampening'</span><span class="token punctuation">:</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token string">'weight_decay'</span><span class="token punctuation">:</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token string">'nesterov'</span><span class="token punctuation">:</span> <span class="token boolean">False</span><span class="token punctuation">,</span> <span class="token string">'params'</span><span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">}</span><span class="token punctuation">]</span>tensor<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">.</span><span class="token punctuation">,</span> grad_fn<span class="token operator">=</span><span class="token operator">&lt;</span>AddBackward0<span class="token operator">></span><span class="token punctuation">)</span><span class="token punctuation">{</span><span class="token string">'params'</span><span class="token punctuation">:</span> <span class="token punctuation">[</span>tensor<span class="token punctuation">(</span><span class="token number">0.2000</span><span class="token punctuation">,</span> requires_grad<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token string">'lr'</span><span class="token punctuation">:</span> <span class="token number">0.1</span><span class="token punctuation">,</span> <span class="token string">'momentum'</span><span class="token punctuation">:</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token string">'dampening'</span><span class="token punctuation">:</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token string">'weight_decay'</span><span class="token punctuation">:</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token string">'nesterov'</span><span class="token punctuation">:</span> <span class="token boolean">False</span><span class="token punctuation">}</span>tensor<span class="token punctuation">(</span><span class="token number">0.6400</span><span class="token punctuation">,</span> grad_fn<span class="token operator">=</span><span class="token operator">&lt;</span>AddBackward0<span class="token operator">></span><span class="token punctuation">)</span><span class="token punctuation">{</span><span class="token string">'params'</span><span class="token punctuation">:</span> <span class="token punctuation">[</span>tensor<span class="token punctuation">(</span><span class="token number">0.3600</span><span class="token punctuation">,</span> requires_grad<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token string">'lr'</span><span class="token punctuation">:</span> <span class="token number">0.1</span><span class="token punctuation">,</span> <span class="token string">'momentum'</span><span class="token punctuation">:</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token string">'dampening'</span><span class="token punctuation">:</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token string">'weight_decay'</span><span class="token punctuation">:</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token string">'nesterov'</span><span class="token punctuation">:</span> <span class="token boolean">False</span><span class="token punctuation">}</span>tensor<span class="token punctuation">(</span><span class="token number">0.4096</span><span class="token punctuation">,</span> grad_fn<span class="token operator">=</span><span class="token operator">&lt;</span>AddBackward0<span class="token operator">></span><span class="token punctuation">)</span><span class="token punctuation">{</span><span class="token string">'params'</span><span class="token punctuation">:</span> <span class="token punctuation">[</span>tensor<span class="token punctuation">(</span><span class="token number">0.4880</span><span class="token punctuation">,</span> requires_grad<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token string">'lr'</span><span class="token punctuation">:</span> <span class="token number">0.1</span><span class="token punctuation">,</span> <span class="token string">'momentum'</span><span class="token punctuation">:</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token string">'dampening'</span><span class="token punctuation">:</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token string">'weight_decay'</span><span class="token punctuation">:</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token string">'nesterov'</span><span class="token punctuation">:</span> <span class="token boolean">False</span><span class="token punctuation">}</span>tensor<span class="token punctuation">(</span><span class="token number">0.2621</span><span class="token punctuation">,</span> grad_fn<span class="token operator">=</span><span class="token operator">&lt;</span>AddBackward0<span class="token operator">></span><span class="token punctuation">)</span><span class="token punctuation">{</span><span class="token string">'params'</span><span class="token punctuation">:</span> <span class="token punctuation">[</span>tensor<span class="token punctuation">(</span><span class="token number">0.5904</span><span class="token punctuation">,</span> requires_grad<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token string">'lr'</span><span class="token punctuation">:</span> <span class="token number">0.1</span><span class="token punctuation">,</span> <span class="token string">'momentum'</span><span class="token punctuation">:</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token string">'dampening'</span><span class="token punctuation">:</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token string">'weight_decay'</span><span class="token punctuation">:</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token string">'nesterov'</span><span class="token punctuation">:</span> <span class="token boolean">False</span><span class="token punctuation">}</span>tensor<span class="token punctuation">(</span><span class="token number">0.1678</span><span class="token punctuation">,</span> grad_fn<span class="token operator">=</span><span class="token operator">&lt;</span>AddBackward0<span class="token operator">></span><span class="token punctuation">)</span><span class="token punctuation">{</span><span class="token string">'params'</span><span class="token punctuation">:</span> <span class="token punctuation">[</span>tensor<span class="token punctuation">(</span><span class="token number">0.6723</span><span class="token punctuation">,</span> requires_grad<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token string">'lr'</span><span class="token punctuation">:</span> <span class="token number">0.1</span><span class="token punctuation">,</span> <span class="token string">'momentum'</span><span class="token punctuation">:</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token string">'dampening'</span><span class="token punctuation">:</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token string">'weight_decay'</span><span class="token punctuation">:</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token string">'nesterov'</span><span class="token punctuation">:</span> <span class="token boolean">False</span><span class="token punctuation">}</span>tensor<span class="token punctuation">(</span><span class="token number">0.1074</span><span class="token punctuation">,</span> grad_fn<span class="token operator">=</span><span class="token operator">&lt;</span>AddBackward0<span class="token operator">></span><span class="token punctuation">)</span><span class="token punctuation">{</span><span class="token string">'params'</span><span class="token punctuation">:</span> <span class="token punctuation">[</span>tensor<span class="token punctuation">(</span><span class="token number">0.7379</span><span class="token punctuation">,</span> requires_grad<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token string">'lr'</span><span class="token punctuation">:</span> <span class="token number">0.1</span><span class="token punctuation">,</span> <span class="token string">'momentum'</span><span class="token punctuation">:</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token string">'dampening'</span><span class="token punctuation">:</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token string">'weight_decay'</span><span class="token punctuation">:</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token string">'nesterov'</span><span class="token punctuation">:</span> <span class="token boolean">False</span><span class="token punctuation">}</span>tensor<span class="token punctuation">(</span><span class="token number">0.0687</span><span class="token punctuation">,</span> grad_fn<span class="token operator">=</span><span class="token operator">&lt;</span>AddBackward0<span class="token operator">></span><span class="token punctuation">)</span><span class="token punctuation">{</span><span class="token string">'params'</span><span class="token punctuation">:</span> <span class="token punctuation">[</span>tensor<span class="token punctuation">(</span><span class="token number">0.7903</span><span class="token punctuation">,</span> requires_grad<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token string">'lr'</span><span class="token punctuation">:</span> <span class="token number">0.1</span><span class="token punctuation">,</span> <span class="token string">'momentum'</span><span class="token punctuation">:</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token string">'dampening'</span><span class="token punctuation">:</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token string">'weight_decay'</span><span class="token punctuation">:</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token string">'nesterov'</span><span class="token punctuation">:</span> <span class="token boolean">False</span><span class="token punctuation">}</span>tensor<span class="token punctuation">(</span><span class="token number">0.0440</span><span class="token punctuation">,</span> grad_fn<span class="token operator">=</span><span class="token operator">&lt;</span>AddBackward0<span class="token operator">></span><span class="token punctuation">)</span><span class="token punctuation">{</span><span class="token string">'params'</span><span class="token punctuation">:</span> <span class="token punctuation">[</span>tensor<span class="token punctuation">(</span><span class="token number">0.8322</span><span class="token punctuation">,</span> requires_grad<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token string">'lr'</span><span class="token punctuation">:</span> <span class="token number">0.1</span><span class="token punctuation">,</span> <span class="token string">'momentum'</span><span class="token punctuation">:</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token string">'dampening'</span><span class="token punctuation">:</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token string">'weight_decay'</span><span class="token punctuation">:</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token string">'nesterov'</span><span class="token punctuation">:</span> <span class="token boolean">False</span><span class="token punctuation">}</span>tensor<span class="token punctuation">(</span><span class="token number">0.0281</span><span class="token punctuation">,</span> grad_fn<span class="token operator">=</span><span class="token operator">&lt;</span>AddBackward0<span class="token operator">></span><span class="token punctuation">)</span><span class="token punctuation">{</span><span class="token string">'params'</span><span class="token punctuation">:</span> <span class="token punctuation">[</span>tensor<span class="token punctuation">(</span><span class="token number">0.8658</span><span class="token punctuation">,</span> requires_grad<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token string">'lr'</span><span class="token punctuation">:</span> <span class="token number">0.1</span><span class="token punctuation">,</span> <span class="token string">'momentum'</span><span class="token punctuation">:</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token string">'dampening'</span><span class="token punctuation">:</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token string">'weight_decay'</span><span class="token punctuation">:</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token string">'nesterov'</span><span class="token punctuation">:</span> <span class="token boolean">False</span><span class="token punctuation">}</span>tensor<span class="token punctuation">(</span><span class="token number">0.0180</span><span class="token punctuation">,</span> grad_fn<span class="token operator">=</span><span class="token operator">&lt;</span>AddBackward0<span class="token operator">></span><span class="token punctuation">)</span><span class="token punctuation">{</span><span class="token string">'params'</span><span class="token punctuation">:</span> <span class="token punctuation">[</span>tensor<span class="token punctuation">(</span><span class="token number">0.8926</span><span class="token punctuation">,</span> requires_grad<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token string">'lr'</span><span class="token punctuation">:</span> <span class="token number">0.1</span><span class="token punctuation">,</span> <span class="token string">'momentum'</span><span class="token punctuation">:</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token string">'dampening'</span><span class="token punctuation">:</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token string">'weight_decay'</span><span class="token punctuation">:</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token string">'nesterov'</span><span class="token punctuation">:</span> <span class="token boolean">False</span><span class="token punctuation">}</span>y<span class="token operator">=</span> tensor<span class="token punctuation">(</span><span class="token number">0.0115</span><span class="token punctuation">)</span> <span class="token punctuation">;</span> x<span class="token operator">=</span> tensor<span class="token punctuation">(</span><span class="token number">0.8926</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h1 id="反向传播的梯度累加"><a href="#反向传播的梯度累加" class="headerlink" title="反向传播的梯度累加"></a>反向传播的梯度累加</h1><p>以下面的计算过程为例，进行反向传播计算。<br>$x_2 = x_1<em>w_1$<br>$y = x_2</em>w_2$<br>$L = Y-y$</p><pre class="line-numbers language-python"><code class="language-python">x1 <span class="token operator">=</span> torch<span class="token punctuation">.</span>from_numpy<span class="token punctuation">(</span><span class="token number">2</span><span class="token operator">*</span>np<span class="token punctuation">.</span>ones<span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">,</span> dtype<span class="token operator">=</span>np<span class="token punctuation">.</span>float32<span class="token punctuation">)</span><span class="token punctuation">)</span>x1<span class="token punctuation">.</span>requires_grad_<span class="token punctuation">(</span><span class="token boolean">True</span><span class="token punctuation">)</span>w1 <span class="token operator">=</span> torch<span class="token punctuation">.</span>from_numpy<span class="token punctuation">(</span><span class="token number">5</span><span class="token operator">*</span>np<span class="token punctuation">.</span>ones<span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">,</span> dtype<span class="token operator">=</span>np<span class="token punctuation">.</span>float32<span class="token punctuation">)</span><span class="token punctuation">)</span>w1<span class="token punctuation">.</span>requires_grad_<span class="token punctuation">(</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"x1 = "</span><span class="token punctuation">,</span> x1<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"w1 = "</span><span class="token punctuation">,</span> w1<span class="token punctuation">)</span><span class="token operator">>></span><span class="token operator">></span>Output<span class="token punctuation">:</span>x1 <span class="token operator">=</span>  tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">.</span><span class="token punctuation">]</span><span class="token punctuation">,</span>        <span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">.</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">,</span> requires_grad<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>w1 <span class="token operator">=</span>  tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">5</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">.</span><span class="token punctuation">]</span><span class="token punctuation">,</span>        <span class="token punctuation">[</span><span class="token number">5</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">.</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">,</span> requires_grad<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><pre class="line-numbers language-python"><code class="language-python">x2 <span class="token operator">=</span> x1 <span class="token operator">*</span> w1w2 <span class="token operator">=</span> torch<span class="token punctuation">.</span>from_numpy<span class="token punctuation">(</span><span class="token number">6</span><span class="token operator">*</span>np<span class="token punctuation">.</span>ones<span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">,</span> dtype<span class="token operator">=</span>np<span class="token punctuation">.</span>float32<span class="token punctuation">)</span><span class="token punctuation">)</span>w2<span class="token punctuation">.</span>requires_grad_<span class="token punctuation">(</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"x2 = "</span><span class="token punctuation">,</span> x2<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"w2 = "</span><span class="token punctuation">,</span> w2<span class="token punctuation">)</span><span class="token operator">>></span><span class="token operator">></span>Output<span class="token punctuation">:</span>x2 <span class="token operator">=</span>  tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">10</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">10</span><span class="token punctuation">.</span><span class="token punctuation">]</span><span class="token punctuation">,</span>        <span class="token punctuation">[</span><span class="token number">10</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">10</span><span class="token punctuation">.</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">,</span> grad_fn<span class="token operator">=</span><span class="token operator">&lt;</span>MulBackward0<span class="token operator">></span><span class="token punctuation">)</span>w2 <span class="token operator">=</span>  tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">6</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">6</span><span class="token punctuation">.</span><span class="token punctuation">]</span><span class="token punctuation">,</span>        <span class="token punctuation">[</span><span class="token number">6</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">6</span><span class="token punctuation">.</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">,</span> requires_grad<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><pre class="line-numbers language-python"><code class="language-python">y <span class="token operator">=</span> x2 <span class="token operator">*</span> w2Y <span class="token operator">=</span> torch<span class="token punctuation">.</span>from_numpy<span class="token punctuation">(</span><span class="token number">10</span><span class="token operator">*</span>np<span class="token punctuation">.</span>ones<span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">,</span> dtype<span class="token operator">=</span>np<span class="token punctuation">.</span>float32<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"y = "</span><span class="token punctuation">,</span> y<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"Y = "</span><span class="token punctuation">,</span> Y<span class="token punctuation">)</span>L <span class="token operator">=</span> Y<span class="token operator">-</span>y<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"Loss = "</span><span class="token punctuation">,</span> L<span class="token punctuation">)</span><span class="token operator">>></span><span class="token operator">></span>Output<span class="token punctuation">:</span>y <span class="token operator">=</span>  tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">60</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">60</span><span class="token punctuation">.</span><span class="token punctuation">]</span><span class="token punctuation">,</span>        <span class="token punctuation">[</span><span class="token number">60</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">60</span><span class="token punctuation">.</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">,</span> grad_fn<span class="token operator">=</span><span class="token operator">&lt;</span>MulBackward0<span class="token operator">></span><span class="token punctuation">)</span>Y <span class="token operator">=</span>  tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">10</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">10</span><span class="token punctuation">.</span><span class="token punctuation">]</span><span class="token punctuation">,</span>        <span class="token punctuation">[</span><span class="token number">10</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">10</span><span class="token punctuation">.</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span>Loss <span class="token operator">=</span>  tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">50</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">50</span><span class="token punctuation">.</span><span class="token punctuation">]</span><span class="token punctuation">,</span>        <span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">50</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">50</span><span class="token punctuation">.</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">,</span> grad_fn<span class="token operator">=</span><span class="token operator">&lt;</span>SubBackward0<span class="token operator">></span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>$L = Y-y=Y-x_1<em>w_1</em>w2$<br>$\frac{\partial L}{\partial x_1} = \frac{\partial L}{\partial y} * \frac{\partial y}{\partial x_1}=-w_1<em>w_2$<br>$\frac{\partial L}{\partial w_1} = \frac{\partial L}{\partial y} * \frac{\partial y}{\partial w_1}=-x_1</em>w_2$<br>$\frac{\partial L}{\partial w_2} = \frac{\partial L}{\partial y} * \frac{\partial y}{\partial w_2}=-x_1*w_1$</p><pre class="line-numbers language-python"><code class="language-python">grad <span class="token operator">=</span> torch<span class="token punctuation">.</span>ones<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">,</span>dtype<span class="token operator">=</span>torch<span class="token punctuation">.</span>float<span class="token punctuation">)</span>L<span class="token punctuation">.</span>backward<span class="token punctuation">(</span>grad<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"x1.grad = "</span><span class="token punctuation">,</span> x1<span class="token punctuation">.</span>grad<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"w1.grad = "</span><span class="token punctuation">,</span> w1<span class="token punctuation">.</span>grad<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"w2.grad = "</span><span class="token punctuation">,</span> w2<span class="token punctuation">.</span>grad<span class="token punctuation">)</span><span class="token operator">>></span><span class="token operator">></span>Output<span class="token punctuation">:</span>x1<span class="token punctuation">.</span>grad <span class="token operator">=</span>  tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">30</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">30</span><span class="token punctuation">.</span><span class="token punctuation">]</span><span class="token punctuation">,</span>        <span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">30</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">30</span><span class="token punctuation">.</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span>w1<span class="token punctuation">.</span>grad <span class="token operator">=</span>  tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">12</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">12</span><span class="token punctuation">.</span><span class="token punctuation">]</span><span class="token punctuation">,</span>        <span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">12</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">12</span><span class="token punctuation">.</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span>w2<span class="token punctuation">.</span>grad <span class="token operator">=</span>  tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">10</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">10</span><span class="token punctuation">.</span><span class="token punctuation">]</span><span class="token punctuation">,</span>        <span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">10</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">10</span><span class="token punctuation">.</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>张量的梯度是累加的，如果不对梯度清零，梯度会一直累加下去。<br>此时进行新的运算，查看$x_1$的梯度。<br>$L_2 = x_1<em>x_1$<br>$\frac{\partial L_2}{\partial x_1} = 2</em>x_1$<br>经过第一轮运算后$x_1$的梯度是$\begin{bmatrix} -30 &amp; -30 \ -30 &amp; -30 \end{bmatrix}$，新的运算后计算得到$x_1$的梯度是$\begin{bmatrix} 4 &amp; 4 \ 4 &amp; 4 \end{bmatrix}$，则累加后的$x_1$的最终梯度是$\begin{bmatrix} -26 &amp; -26 \ -26 &amp; -26 \end{bmatrix}$。</p><pre class="line-numbers language-python"><code class="language-python">L2 <span class="token operator">=</span> x1<span class="token operator">*</span>x1L2<span class="token punctuation">.</span>backward<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>ones<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">,</span> dtype<span class="token operator">=</span>torch<span class="token punctuation">.</span>float<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"x1.grad = "</span><span class="token punctuation">,</span> x1<span class="token punctuation">.</span>grad<span class="token punctuation">)</span><span class="token operator">>></span><span class="token operator">></span>Output<span class="token punctuation">:</span>x1<span class="token punctuation">.</span>grad <span class="token operator">=</span>  tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">26</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">26</span><span class="token punctuation">.</span><span class="token punctuation">]</span><span class="token punctuation">,</span>        <span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">26</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">26</span><span class="token punctuation">.</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>]]></content>
      
      
      <categories>
          
          <category> Pytorch </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 基础知识 </tag>
            
            <tag> pytorch </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Pytorch|模型和参数</title>
      <link href="/2023/04/06/03function-he-module/"/>
      <url>/2023/04/06/03function-he-module/</url>
      
        <content type="html"><![CDATA[<p>这里的function和Module是指<code>nn.functional</code>和<code>nn.Module</code>。<br>Pytorch和神经网络相关的功能组件大多都封装在<code>torch.nn</code>模块下。这些功能组件的绝大部分既有<strong>函数形式</strong>实现，也有<strong>类形式</strong>实现。<br>其中<code>nn.functional</code>(一般引入后改名为F)有各种功能组件的<strong>函数实现</strong>。为了便于对参数进行管理，一般通过继承<code>nn.Module</code>转换成为<strong>类的实现形式</strong>，并直接封装在 nn 模块下。例如：</p><table><thead><tr><th></th><th><code>nn.functional</code></th><th><code>nn.Module</code></th></tr></thead><tbody><tr><td>激活函数</td><td>F.relu</td><td>nn.ReLU</td></tr><tr><td></td><td>F.sigmoid</td><td>nn.Sigmoid</td></tr><tr><td>模型层</td><td>F.linear</td><td>nn.Linear</td></tr><tr><td></td><td>F.conv2d</td><td>nn.Conv2d</td></tr><tr><td>损失函数</td><td>F.binary_cross_entropy</td><td>nn.BCELoss</td></tr><tr><td></td><td>F.mse_loss</td><td>nn.MSELoss</td></tr><tr><td></td><td>F.cross_entropy</td><td>nn.CrossEntropyLoss</td></tr></tbody></table><p>实际上<code>nn.Module</code>除了可以管理其引用的各种参数，还可以管理其引用的子模块，功能十分强大。</p><h1 id="使用nn-Module来管理参数"><a href="#使用nn-Module来管理参数" class="headerlink" title="使用nn.Module来管理参数"></a>使用nn.Module来管理参数</h1><p>在Pytorch中，模型的参数是需要被优化器训练的，因此，通常要设置其参数为<code>requires_grad = True</code>的张量。同时，在一个模型中，往往有许多的参数，要手动管理这些参数并不是一件容易的事情。<br>Pytorch一般将参数用<code>nn.Parameter</code>来表示，并且用<code>nn.Module</code>来管理其结构下的所有参数。<br><code>nn.Parameter</code>具有<code>requires_grad = True</code>属性。</p><pre class="line-numbers language-python"><code class="language-python">w <span class="token operator">=</span> nn<span class="token punctuation">.</span>Parameter<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>randn<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>w<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>w<span class="token punctuation">.</span>requires_grad<span class="token punctuation">)</span><span class="token operator">>></span><span class="token operator">></span>Output<span class="token punctuation">:</span>Parameter containing<span class="token punctuation">:</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span> <span class="token number">0.3544</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1.1643</span><span class="token punctuation">]</span><span class="token punctuation">,</span>        <span class="token punctuation">[</span> <span class="token number">1.2302</span><span class="token punctuation">,</span>  <span class="token number">1.3952</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">,</span> requires_grad<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token boolean">True</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p><code>nn.ParameterList</code>可以将多个nn.Parameter组成一个列表。</p><pre class="line-numbers language-python"><code class="language-python">params_list <span class="token operator">=</span> nn<span class="token punctuation">.</span>ParameterList<span class="token punctuation">(</span><span class="token punctuation">[</span>    nn<span class="token punctuation">.</span>Parameter<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>rand<span class="token punctuation">(</span><span class="token number">8</span><span class="token punctuation">,</span>i<span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token keyword">for</span> i <span class="token keyword">in</span> range<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">3</span><span class="token punctuation">)</span>    <span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>params_list<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>params_list<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">.</span>requires_grad<span class="token punctuation">)</span><span class="token operator">>></span><span class="token operator">></span>Output<span class="token punctuation">:</span>ParameterList<span class="token punctuation">(</span>    <span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">:</span> Parameter containing<span class="token punctuation">:</span> <span class="token punctuation">[</span>torch<span class="token punctuation">.</span>FloatTensor of size 8x1<span class="token punctuation">]</span>    <span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">:</span> Parameter containing<span class="token punctuation">:</span> <span class="token punctuation">[</span>torch<span class="token punctuation">.</span>FloatTensor of size 8x2<span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token boolean">True</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p><code>nn.ParameterDict</code>可以将多个nn.Parameter组成一个字典。</p><pre class="line-numbers language-python"><code class="language-python">params_dict <span class="token operator">=</span> nn<span class="token punctuation">.</span>ParameterDict<span class="token punctuation">(</span><span class="token punctuation">{</span><span class="token string">"a"</span><span class="token punctuation">:</span>nn<span class="token punctuation">.</span>Parameter<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>rand<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span>                               <span class="token string">"b"</span><span class="token punctuation">:</span>nn<span class="token punctuation">.</span>Parameter<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">}</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>params_dict<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>params_dict<span class="token punctuation">[</span><span class="token string">"a"</span><span class="token punctuation">]</span><span class="token punctuation">.</span>requires_grad<span class="token punctuation">)</span><span class="token operator">>></span><span class="token operator">></span>Output<span class="token punctuation">:</span>ParameterDict<span class="token punctuation">(</span>    <span class="token punctuation">(</span>a<span class="token punctuation">)</span><span class="token punctuation">:</span> Parameter containing<span class="token punctuation">:</span> <span class="token punctuation">[</span>torch<span class="token punctuation">.</span>FloatTensor of size 2x2<span class="token punctuation">]</span>    <span class="token punctuation">(</span>b<span class="token punctuation">)</span><span class="token punctuation">:</span> Parameter containing<span class="token punctuation">:</span> <span class="token punctuation">[</span>torch<span class="token punctuation">.</span>FloatTensor of size <span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token boolean">True</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>可以用Module将它们管理起来。<br><code>module.parameters()</code>返回一个生成器，包括其结构下的所有Parameter 。</p><pre class="line-numbers language-python"><code class="language-python">module <span class="token operator">=</span> nn<span class="token punctuation">.</span>Module<span class="token punctuation">(</span><span class="token punctuation">)</span>module<span class="token punctuation">.</span>w <span class="token operator">=</span> wmodule<span class="token punctuation">.</span>params_list <span class="token operator">=</span> params_listmodule<span class="token punctuation">.</span>params_dict <span class="token operator">=</span> params_dictnum_param <span class="token operator">=</span> <span class="token number">0</span><span class="token keyword">for</span> param <span class="token keyword">in</span> module<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">print</span><span class="token punctuation">(</span>param<span class="token punctuation">,</span><span class="token string">"\n"</span><span class="token punctuation">)</span>    num_param <span class="token operator">=</span> num_param <span class="token operator">+</span> <span class="token number">1</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"number of Parameters ="</span><span class="token punctuation">,</span>num_param<span class="token punctuation">)</span><span class="token operator">>></span><span class="token operator">></span>Output<span class="token punctuation">:</span>Parameter containing<span class="token punctuation">:</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span> <span class="token number">0.3544</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1.1643</span><span class="token punctuation">]</span><span class="token punctuation">,</span>        <span class="token punctuation">[</span> <span class="token number">1.2302</span><span class="token punctuation">,</span>  <span class="token number">1.3952</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">,</span> requires_grad<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span> Parameter containing<span class="token punctuation">:</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">0.9391</span><span class="token punctuation">]</span><span class="token punctuation">,</span>        <span class="token punctuation">[</span><span class="token number">0.7590</span><span class="token punctuation">]</span><span class="token punctuation">,</span>        <span class="token punctuation">[</span><span class="token number">0.6899</span><span class="token punctuation">]</span><span class="token punctuation">,</span>        <span class="token punctuation">[</span><span class="token number">0.4786</span><span class="token punctuation">]</span><span class="token punctuation">,</span>        <span class="token punctuation">[</span><span class="token number">0.2392</span><span class="token punctuation">]</span><span class="token punctuation">,</span>        <span class="token punctuation">[</span><span class="token number">0.9645</span><span class="token punctuation">]</span><span class="token punctuation">,</span>        <span class="token punctuation">[</span><span class="token number">0.1968</span><span class="token punctuation">]</span><span class="token punctuation">,</span>        <span class="token punctuation">[</span><span class="token number">0.1353</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">,</span> requires_grad<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span> Parameter containing<span class="token punctuation">:</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">0.8012</span><span class="token punctuation">,</span> <span class="token number">0.9587</span><span class="token punctuation">]</span><span class="token punctuation">,</span>        <span class="token punctuation">[</span><span class="token number">0.0276</span><span class="token punctuation">,</span> <span class="token number">0.5995</span><span class="token punctuation">]</span><span class="token punctuation">,</span>        <span class="token punctuation">[</span><span class="token number">0.7338</span><span class="token punctuation">,</span> <span class="token number">0.5559</span><span class="token punctuation">]</span><span class="token punctuation">,</span>        <span class="token punctuation">[</span><span class="token number">0.1704</span><span class="token punctuation">,</span> <span class="token number">0.5814</span><span class="token punctuation">]</span><span class="token punctuation">,</span>        <span class="token punctuation">[</span><span class="token number">0.7626</span><span class="token punctuation">,</span> <span class="token number">0.1179</span><span class="token punctuation">]</span><span class="token punctuation">,</span>        <span class="token punctuation">[</span><span class="token number">0.4945</span><span class="token punctuation">,</span> <span class="token number">0.2408</span><span class="token punctuation">]</span><span class="token punctuation">,</span>        <span class="token punctuation">[</span><span class="token number">0.7179</span><span class="token punctuation">,</span> <span class="token number">0.0575</span><span class="token punctuation">]</span><span class="token punctuation">,</span>        <span class="token punctuation">[</span><span class="token number">0.3418</span><span class="token punctuation">,</span> <span class="token number">0.7291</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">,</span> requires_grad<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span> Parameter containing<span class="token punctuation">:</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">0.7729</span><span class="token punctuation">,</span> <span class="token number">0.2383</span><span class="token punctuation">]</span><span class="token punctuation">,</span>        <span class="token punctuation">[</span><span class="token number">0.7054</span><span class="token punctuation">,</span> <span class="token number">0.9937</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">,</span> requires_grad<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span> Parameter containing<span class="token punctuation">:</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">]</span><span class="token punctuation">,</span> requires_grad<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span> number of Parameters <span class="token operator">=</span> <span class="token number">5</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>下面以nn.Linear观察其参数。</p><pre class="line-numbers language-python"><code class="language-python">linear <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">20</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token keyword">for</span> param <span class="token keyword">in</span> linear<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">print</span><span class="token punctuation">(</span>param<span class="token punctuation">)</span>    <span class="token keyword">print</span><span class="token punctuation">(</span>param<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> param<span class="token punctuation">.</span>shape<span class="token punctuation">)</span>    <span class="token keyword">print</span><span class="token punctuation">(</span>param<span class="token punctuation">.</span>requires_grad<span class="token punctuation">)</span>    <span class="token keyword">print</span><span class="token punctuation">(</span>param<span class="token punctuation">.</span>numel<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token operator">>></span><span class="token operator">></span>Output<span class="token punctuation">:</span>Parameter containing<span class="token punctuation">:</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">0.2206</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">0.0732</span><span class="token punctuation">,</span>  <span class="token number">0.2090</span><span class="token punctuation">,</span>  <span class="token number">0.0789</span><span class="token punctuation">,</span>  <span class="token number">0.0329</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">0.1918</span><span class="token punctuation">,</span>  <span class="token number">0.1738</span><span class="token punctuation">,</span>  <span class="token number">0.1015</span><span class="token punctuation">,</span>          <span class="token number">0.1667</span><span class="token punctuation">,</span>  <span class="token number">0.0078</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">0.1058</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">0.1306</span><span class="token punctuation">,</span>  <span class="token number">0.0223</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">0.0715</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">0.0484</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">0.0651</span><span class="token punctuation">,</span>          <span class="token number">0.1581</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">0.0084</span><span class="token punctuation">,</span>  <span class="token number">0.0357</span><span class="token punctuation">,</span>  <span class="token number">0.1935</span><span class="token punctuation">]</span><span class="token punctuation">,</span>        <span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">0.0515</span><span class="token punctuation">,</span>  <span class="token number">0.2016</span><span class="token punctuation">,</span>  <span class="token number">0.2110</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">0.1053</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">0.0663</span><span class="token punctuation">,</span>  <span class="token number">0.1520</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">0.0612</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">0.1197</span><span class="token punctuation">,</span>          <span class="token number">0.0597</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">0.0840</span><span class="token punctuation">,</span>  <span class="token number">0.1601</span><span class="token punctuation">,</span>  <span class="token number">0.0842</span><span class="token punctuation">,</span>  <span class="token number">0.0731</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">0.1282</span><span class="token punctuation">,</span>  <span class="token number">0.0515</span><span class="token punctuation">,</span>  <span class="token number">0.0054</span><span class="token punctuation">,</span>          <span class="token number">0.0433</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">0.1549</span><span class="token punctuation">,</span>  <span class="token number">0.1223</span><span class="token punctuation">,</span>  <span class="token number">0.0270</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">,</span> requires_grad<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">20</span><span class="token punctuation">]</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">20</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token boolean">True</span><span class="token number">40</span>Parameter containing<span class="token punctuation">:</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">0.2008</span><span class="token punctuation">,</span> <span class="token number">0.2086</span><span class="token punctuation">]</span><span class="token punctuation">,</span> requires_grad<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token boolean">True</span><span class="token number">2</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>实践当中，一般通过继承<code>nn.Module</code>来构建模块类，并将所有含有需要学习的参数的部分放在构造函数中。<br>以下范例为Pytorch中<code>nn.Linear</code>的源码的简化版本。可以看到它将需要学习的参数放在了<code>__init__</code>构造函数中，并在forward中调用F.linear函数来实现计算逻辑。</p><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">Linear</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>    __constants__ <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token string">'in_features'</span><span class="token punctuation">,</span> <span class="token string">'out_features'</span><span class="token punctuation">]</span>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> in_features<span class="token punctuation">,</span> out_features<span class="token punctuation">,</span> bias<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token punctuation">:</span>        super<span class="token punctuation">(</span>Linear<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>in_features <span class="token operator">=</span> in_features        self<span class="token punctuation">.</span>out_features <span class="token operator">=</span> out_features        self<span class="token punctuation">.</span>weight <span class="token operator">=</span> nn<span class="token punctuation">.</span>Parameter<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">(</span>out_features<span class="token punctuation">,</span> in_features<span class="token punctuation">)</span><span class="token punctuation">)</span>        <span class="token keyword">if</span> bias<span class="token punctuation">:</span>            self<span class="token punctuation">.</span>bias <span class="token operator">=</span> nn<span class="token punctuation">.</span>Parameter<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">(</span>out_features<span class="token punctuation">)</span><span class="token punctuation">)</span>        <span class="token keyword">else</span><span class="token punctuation">:</span>            self<span class="token punctuation">.</span>register_parameter<span class="token punctuation">(</span><span class="token string">'bias'</span><span class="token punctuation">,</span> None<span class="token punctuation">)</span>    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> input<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token keyword">return</span> F<span class="token punctuation">.</span>linear<span class="token punctuation">(</span>input<span class="token punctuation">,</span> self<span class="token punctuation">.</span>weight<span class="token punctuation">,</span> self<span class="token punctuation">.</span>bias<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h1 id="使用nn-Module来管理子模块"><a href="#使用nn-Module来管理子模块" class="headerlink" title="使用nn.Module来管理子模块"></a>使用nn.Module来管理子模块</h1><p>一般情况下，我们都很少直接使用<code>nn.Parameter</code>来定义参数构建模型，而是通过拼装一些常用的模型层来构造模型。<br>这些模型层也是继承自nn.Module的对象，本身也包括参数，属于我们要定义的模块的子模块。<br>nn.Module提供了一些方法可以管理这些子模块。</p><ul><li><code>children()</code>方法：返回一个生成器，包括模块下的所有子模块。</li><li><code>named_children()</code>方法：返回一个生成器，包括模块下的所有子模块，以及它们的名字。</li><li><code>modules()</code>方法：返回一个生成器，递归地找到模块下的所有各个层级的模块，包括模块本身。</li><li><code>named_modules()</code>方法：返回一个生成器，递归地找到模块下的所有各个层级的模块以及它们的名字，包括模块本身。</li></ul><p>其中<code>chidren()</code>方法和<code>named_children()</code>方法较多使用。<code>modules()</code>方法和<code>named_modules()</code>方法较少使用，其功能可以通过多个<code>named_children()</code>的嵌套使用实现。</p><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">Net</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>        super<span class="token punctuation">(</span>Net<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>embedding <span class="token operator">=</span> nn<span class="token punctuation">.</span>Embedding<span class="token punctuation">(</span>num_embeddings <span class="token operator">=</span> <span class="token number">10000</span><span class="token punctuation">,</span>embedding_dim <span class="token operator">=</span> <span class="token number">3</span><span class="token punctuation">,</span>padding_idx <span class="token operator">=</span> <span class="token number">1</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>conv <span class="token operator">=</span> nn<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>conv<span class="token punctuation">.</span>add_module<span class="token punctuation">(</span><span class="token string">"conv_1"</span><span class="token punctuation">,</span>nn<span class="token punctuation">.</span>Conv1d<span class="token punctuation">(</span>in_channels <span class="token operator">=</span> <span class="token number">3</span><span class="token punctuation">,</span>out_channels <span class="token operator">=</span> <span class="token number">16</span><span class="token punctuation">,</span>kernel_size <span class="token operator">=</span> <span class="token number">5</span><span class="token punctuation">)</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>conv<span class="token punctuation">.</span>add_module<span class="token punctuation">(</span><span class="token string">"pool_1"</span><span class="token punctuation">,</span>nn<span class="token punctuation">.</span>MaxPool1d<span class="token punctuation">(</span>kernel_size <span class="token operator">=</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>conv<span class="token punctuation">.</span>add_module<span class="token punctuation">(</span><span class="token string">"relu_1"</span><span class="token punctuation">,</span>nn<span class="token punctuation">.</span>ReLU<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>conv<span class="token punctuation">.</span>add_module<span class="token punctuation">(</span><span class="token string">"conv_2"</span><span class="token punctuation">,</span>nn<span class="token punctuation">.</span>Conv1d<span class="token punctuation">(</span>in_channels <span class="token operator">=</span> <span class="token number">16</span><span class="token punctuation">,</span>out_channels <span class="token operator">=</span> <span class="token number">128</span><span class="token punctuation">,</span>kernel_size <span class="token operator">=</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>conv<span class="token punctuation">.</span>add_module<span class="token punctuation">(</span><span class="token string">"pool_2"</span><span class="token punctuation">,</span>nn<span class="token punctuation">.</span>MaxPool1d<span class="token punctuation">(</span>kernel_size <span class="token operator">=</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>conv<span class="token punctuation">.</span>add_module<span class="token punctuation">(</span><span class="token string">"relu_2"</span><span class="token punctuation">,</span>nn<span class="token punctuation">.</span>ReLU<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>dense <span class="token operator">=</span> nn<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>dense<span class="token punctuation">.</span>add_module<span class="token punctuation">(</span><span class="token string">"flatten"</span><span class="token punctuation">,</span>nn<span class="token punctuation">.</span>Flatten<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>dense<span class="token punctuation">.</span>add_module<span class="token punctuation">(</span><span class="token string">"linear"</span><span class="token punctuation">,</span>nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">6144</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>dense<span class="token punctuation">.</span>add_module<span class="token punctuation">(</span><span class="token string">"sigmoid"</span><span class="token punctuation">,</span>nn<span class="token punctuation">.</span>Sigmoid<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span>x<span class="token punctuation">)</span><span class="token punctuation">:</span>        x <span class="token operator">=</span> self<span class="token punctuation">.</span>embedding<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">.</span>transpose<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">)</span>        x <span class="token operator">=</span> self<span class="token punctuation">.</span>conv<span class="token punctuation">(</span>x<span class="token punctuation">)</span>        y <span class="token operator">=</span> self<span class="token punctuation">.</span>dense<span class="token punctuation">(</span>x<span class="token punctuation">)</span>        <span class="token keyword">return</span> ynet <span class="token operator">=</span> Net<span class="token punctuation">(</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><pre class="line-numbers language-python"><code class="language-python">i <span class="token operator">=</span> <span class="token number">0</span><span class="token keyword">for</span> child <span class="token keyword">in</span> net<span class="token punctuation">.</span>children<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    i<span class="token operator">+=</span><span class="token number">1</span>    <span class="token keyword">print</span><span class="token punctuation">(</span>child<span class="token punctuation">,</span><span class="token string">"\n"</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"child number"</span><span class="token punctuation">,</span>i<span class="token punctuation">)</span><span class="token operator">>></span><span class="token operator">></span>Output<span class="token punctuation">:</span>Embedding<span class="token punctuation">(</span><span class="token number">10000</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> padding_idx<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span> Sequential<span class="token punctuation">(</span>  <span class="token punctuation">(</span>conv_1<span class="token punctuation">)</span><span class="token punctuation">:</span> Conv1d<span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">16</span><span class="token punctuation">,</span> kernel_size<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">5</span><span class="token punctuation">,</span><span class="token punctuation">)</span><span class="token punctuation">,</span> stride<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token punctuation">)</span><span class="token punctuation">)</span>  <span class="token punctuation">(</span>pool_1<span class="token punctuation">)</span><span class="token punctuation">:</span> MaxPool1d<span class="token punctuation">(</span>kernel_size<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span> stride<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">,</span> dilation<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> ceil_mode<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span>  <span class="token punctuation">(</span>relu_1<span class="token punctuation">)</span><span class="token punctuation">:</span> ReLU<span class="token punctuation">(</span><span class="token punctuation">)</span>  <span class="token punctuation">(</span>conv_2<span class="token punctuation">)</span><span class="token punctuation">:</span> Conv1d<span class="token punctuation">(</span><span class="token number">16</span><span class="token punctuation">,</span> <span class="token number">128</span><span class="token punctuation">,</span> kernel_size<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token punctuation">)</span><span class="token punctuation">,</span> stride<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token punctuation">)</span><span class="token punctuation">)</span>  <span class="token punctuation">(</span>pool_2<span class="token punctuation">)</span><span class="token punctuation">:</span> MaxPool1d<span class="token punctuation">(</span>kernel_size<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span> stride<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">,</span> dilation<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> ceil_mode<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span>  <span class="token punctuation">(</span>relu_2<span class="token punctuation">)</span><span class="token punctuation">:</span> ReLU<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span> Sequential<span class="token punctuation">(</span>  <span class="token punctuation">(</span>flatten<span class="token punctuation">)</span><span class="token punctuation">:</span> Flatten<span class="token punctuation">(</span><span class="token punctuation">)</span>  <span class="token punctuation">(</span>linear<span class="token punctuation">)</span><span class="token punctuation">:</span> Linear<span class="token punctuation">(</span>in_features<span class="token operator">=</span><span class="token number">6144</span><span class="token punctuation">,</span> out_features<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> bias<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>  <span class="token punctuation">(</span>sigmoid<span class="token punctuation">)</span><span class="token punctuation">:</span> Sigmoid<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span> child number <span class="token number">3</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><pre class="line-numbers language-python"><code class="language-python">i <span class="token operator">=</span> <span class="token number">0</span><span class="token keyword">for</span> name<span class="token punctuation">,</span>child <span class="token keyword">in</span> net<span class="token punctuation">.</span>named_children<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    i<span class="token operator">+=</span><span class="token number">1</span>    <span class="token keyword">print</span><span class="token punctuation">(</span>name<span class="token punctuation">,</span><span class="token string">":"</span><span class="token punctuation">,</span>child<span class="token punctuation">,</span><span class="token string">"\n"</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"child number"</span><span class="token punctuation">,</span>i<span class="token punctuation">)</span><span class="token operator">>></span><span class="token operator">></span>Output<span class="token punctuation">:</span>embedding <span class="token punctuation">:</span> Embedding<span class="token punctuation">(</span><span class="token number">10000</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> padding_idx<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span> conv <span class="token punctuation">:</span> Sequential<span class="token punctuation">(</span>  <span class="token punctuation">(</span>conv_1<span class="token punctuation">)</span><span class="token punctuation">:</span> Conv1d<span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">16</span><span class="token punctuation">,</span> kernel_size<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">5</span><span class="token punctuation">,</span><span class="token punctuation">)</span><span class="token punctuation">,</span> stride<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token punctuation">)</span><span class="token punctuation">)</span>  <span class="token punctuation">(</span>pool_1<span class="token punctuation">)</span><span class="token punctuation">:</span> MaxPool1d<span class="token punctuation">(</span>kernel_size<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span> stride<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">,</span> dilation<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> ceil_mode<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span>  <span class="token punctuation">(</span>relu_1<span class="token punctuation">)</span><span class="token punctuation">:</span> ReLU<span class="token punctuation">(</span><span class="token punctuation">)</span>  <span class="token punctuation">(</span>conv_2<span class="token punctuation">)</span><span class="token punctuation">:</span> Conv1d<span class="token punctuation">(</span><span class="token number">16</span><span class="token punctuation">,</span> <span class="token number">128</span><span class="token punctuation">,</span> kernel_size<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token punctuation">)</span><span class="token punctuation">,</span> stride<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token punctuation">)</span><span class="token punctuation">)</span>  <span class="token punctuation">(</span>pool_2<span class="token punctuation">)</span><span class="token punctuation">:</span> MaxPool1d<span class="token punctuation">(</span>kernel_size<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span> stride<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">,</span> dilation<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> ceil_mode<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span>  <span class="token punctuation">(</span>relu_2<span class="token punctuation">)</span><span class="token punctuation">:</span> ReLU<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span> dense <span class="token punctuation">:</span> Sequential<span class="token punctuation">(</span>  <span class="token punctuation">(</span>flatten<span class="token punctuation">)</span><span class="token punctuation">:</span> Flatten<span class="token punctuation">(</span>start_dim<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> end_dim<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>  <span class="token punctuation">(</span>linear<span class="token punctuation">)</span><span class="token punctuation">:</span> Linear<span class="token punctuation">(</span>in_features<span class="token operator">=</span><span class="token number">6144</span><span class="token punctuation">,</span> out_features<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> bias<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>  <span class="token punctuation">(</span>sigmoid<span class="token punctuation">)</span><span class="token punctuation">:</span> Sigmoid<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span> child number <span class="token number">3</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><pre class="line-numbers language-python"><code class="language-python">i <span class="token operator">=</span> <span class="token number">0</span><span class="token keyword">for</span> module <span class="token keyword">in</span> net<span class="token punctuation">.</span>modules<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    i<span class="token operator">+=</span><span class="token number">1</span>    <span class="token keyword">print</span><span class="token punctuation">(</span>module<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"module number:"</span><span class="token punctuation">,</span>i<span class="token punctuation">)</span><span class="token operator">>></span><span class="token operator">></span>Output<span class="token punctuation">:</span>Net<span class="token punctuation">(</span>  <span class="token punctuation">(</span>embedding<span class="token punctuation">)</span><span class="token punctuation">:</span> Embedding<span class="token punctuation">(</span><span class="token number">10000</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> padding_idx<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>  <span class="token punctuation">(</span>conv<span class="token punctuation">)</span><span class="token punctuation">:</span> Sequential<span class="token punctuation">(</span>    <span class="token punctuation">(</span>conv_1<span class="token punctuation">)</span><span class="token punctuation">:</span> Conv1d<span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">16</span><span class="token punctuation">,</span> kernel_size<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">5</span><span class="token punctuation">,</span><span class="token punctuation">)</span><span class="token punctuation">,</span> stride<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token punctuation">)</span><span class="token punctuation">)</span>    <span class="token punctuation">(</span>pool_1<span class="token punctuation">)</span><span class="token punctuation">:</span> MaxPool1d<span class="token punctuation">(</span>kernel_size<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span> stride<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">,</span> dilation<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> ceil_mode<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span>    <span class="token punctuation">(</span>relu_1<span class="token punctuation">)</span><span class="token punctuation">:</span> ReLU<span class="token punctuation">(</span><span class="token punctuation">)</span>    <span class="token punctuation">(</span>conv_2<span class="token punctuation">)</span><span class="token punctuation">:</span> Conv1d<span class="token punctuation">(</span><span class="token number">16</span><span class="token punctuation">,</span> <span class="token number">128</span><span class="token punctuation">,</span> kernel_size<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token punctuation">)</span><span class="token punctuation">,</span> stride<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token punctuation">)</span><span class="token punctuation">)</span>    <span class="token punctuation">(</span>pool_2<span class="token punctuation">)</span><span class="token punctuation">:</span> MaxPool1d<span class="token punctuation">(</span>kernel_size<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span> stride<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">,</span> dilation<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> ceil_mode<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span>    <span class="token punctuation">(</span>relu_2<span class="token punctuation">)</span><span class="token punctuation">:</span> ReLU<span class="token punctuation">(</span><span class="token punctuation">)</span>  <span class="token punctuation">)</span>  <span class="token punctuation">(</span>dense<span class="token punctuation">)</span><span class="token punctuation">:</span> Sequential<span class="token punctuation">(</span>    <span class="token punctuation">(</span>flatten<span class="token punctuation">)</span><span class="token punctuation">:</span> Flatten<span class="token punctuation">(</span>start_dim<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> end_dim<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>    <span class="token punctuation">(</span>linear<span class="token punctuation">)</span><span class="token punctuation">:</span> Linear<span class="token punctuation">(</span>in_features<span class="token operator">=</span><span class="token number">6144</span><span class="token punctuation">,</span> out_features<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> bias<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>    <span class="token punctuation">(</span>sigmoid<span class="token punctuation">)</span><span class="token punctuation">:</span> Sigmoid<span class="token punctuation">(</span><span class="token punctuation">)</span>  <span class="token punctuation">)</span><span class="token punctuation">)</span>Embedding<span class="token punctuation">(</span><span class="token number">10000</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> padding_idx<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>Sequential<span class="token punctuation">(</span>  <span class="token punctuation">(</span>conv_1<span class="token punctuation">)</span><span class="token punctuation">:</span> Conv1d<span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">16</span><span class="token punctuation">,</span> kernel_size<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">5</span><span class="token punctuation">,</span><span class="token punctuation">)</span><span class="token punctuation">,</span> stride<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token punctuation">)</span><span class="token punctuation">)</span>  <span class="token punctuation">(</span>pool_1<span class="token punctuation">)</span><span class="token punctuation">:</span> MaxPool1d<span class="token punctuation">(</span>kernel_size<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span> stride<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">,</span> dilation<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> ceil_mode<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span>  <span class="token punctuation">(</span>relu_1<span class="token punctuation">)</span><span class="token punctuation">:</span> ReLU<span class="token punctuation">(</span><span class="token punctuation">)</span>  <span class="token punctuation">(</span>conv_2<span class="token punctuation">)</span><span class="token punctuation">:</span> Conv1d<span class="token punctuation">(</span><span class="token number">16</span><span class="token punctuation">,</span> <span class="token number">128</span><span class="token punctuation">,</span> kernel_size<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token punctuation">)</span><span class="token punctuation">,</span> stride<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token punctuation">)</span><span class="token punctuation">)</span>  <span class="token punctuation">(</span>pool_2<span class="token punctuation">)</span><span class="token punctuation">:</span> MaxPool1d<span class="token punctuation">(</span>kernel_size<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span> stride<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">,</span> dilation<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> ceil_mode<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span>  <span class="token punctuation">(</span>relu_2<span class="token punctuation">)</span><span class="token punctuation">:</span> ReLU<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>Conv1d<span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">16</span><span class="token punctuation">,</span> kernel_size<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">5</span><span class="token punctuation">,</span><span class="token punctuation">)</span><span class="token punctuation">,</span> stride<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token punctuation">)</span><span class="token punctuation">)</span>MaxPool1d<span class="token punctuation">(</span>kernel_size<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span> stride<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">,</span> dilation<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> ceil_mode<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span>ReLU<span class="token punctuation">(</span><span class="token punctuation">)</span>Conv1d<span class="token punctuation">(</span><span class="token number">16</span><span class="token punctuation">,</span> <span class="token number">128</span><span class="token punctuation">,</span> kernel_size<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token punctuation">)</span><span class="token punctuation">,</span> stride<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token punctuation">)</span><span class="token punctuation">)</span>MaxPool1d<span class="token punctuation">(</span>kernel_size<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span> stride<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">,</span> dilation<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> ceil_mode<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span>ReLU<span class="token punctuation">(</span><span class="token punctuation">)</span>Sequential<span class="token punctuation">(</span>  <span class="token punctuation">(</span>flatten<span class="token punctuation">)</span><span class="token punctuation">:</span> Flatten<span class="token punctuation">(</span>start_dim<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> end_dim<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>  <span class="token punctuation">(</span>linear<span class="token punctuation">)</span><span class="token punctuation">:</span> Linear<span class="token punctuation">(</span>in_features<span class="token operator">=</span><span class="token number">6144</span><span class="token punctuation">,</span> out_features<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> bias<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>  <span class="token punctuation">(</span>sigmoid<span class="token punctuation">)</span><span class="token punctuation">:</span> Sigmoid<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>Flatten<span class="token punctuation">(</span>start_dim<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> end_dim<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>Linear<span class="token punctuation">(</span>in_features<span class="token operator">=</span><span class="token number">6144</span><span class="token punctuation">,</span> out_features<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> bias<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>Sigmoid<span class="token punctuation">(</span><span class="token punctuation">)</span>module number<span class="token punctuation">:</span> <span class="token number">13</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>下面我们通过<code>named_children()</code>方法找到embedding层，并将其参数设置为不可训练(相当于冻结embedding层)。</p><pre class="line-numbers language-python"><code class="language-python">children_dict <span class="token operator">=</span> <span class="token punctuation">{</span>name<span class="token punctuation">:</span>module <span class="token keyword">for</span> name<span class="token punctuation">,</span>module <span class="token keyword">in</span> net<span class="token punctuation">.</span>named_children<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">}</span><span class="token keyword">print</span><span class="token punctuation">(</span>children_dict<span class="token punctuation">)</span>embedding <span class="token operator">=</span> children_dict<span class="token punctuation">[</span><span class="token string">"embedding"</span><span class="token punctuation">]</span>embedding<span class="token punctuation">.</span>requires_grad_<span class="token punctuation">(</span><span class="token boolean">False</span><span class="token punctuation">)</span> <span class="token comment" spellcheck="true">#冻结其参数</span><span class="token operator">>></span><span class="token operator">></span>Output<span class="token punctuation">:</span><span class="token punctuation">{</span><span class="token string">'embedding'</span><span class="token punctuation">:</span> Embedding<span class="token punctuation">(</span><span class="token number">10000</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> padding_idx<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span>  <span class="token string">'conv'</span><span class="token punctuation">:</span> Sequential<span class="token punctuation">(</span>  <span class="token punctuation">(</span>conv_1<span class="token punctuation">)</span><span class="token punctuation">:</span> Conv1d<span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">16</span><span class="token punctuation">,</span> kernel_size<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">5</span><span class="token punctuation">,</span><span class="token punctuation">)</span><span class="token punctuation">,</span> stride<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token punctuation">)</span><span class="token punctuation">)</span>  <span class="token punctuation">(</span>pool_1<span class="token punctuation">)</span><span class="token punctuation">:</span> MaxPool1d<span class="token punctuation">(</span>kernel_size<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span> stride<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">,</span> dilation<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> ceil_mode<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span>  <span class="token punctuation">(</span>relu_1<span class="token punctuation">)</span><span class="token punctuation">:</span> ReLU<span class="token punctuation">(</span><span class="token punctuation">)</span>  <span class="token punctuation">(</span>conv_2<span class="token punctuation">)</span><span class="token punctuation">:</span> Conv1d<span class="token punctuation">(</span><span class="token number">16</span><span class="token punctuation">,</span> <span class="token number">128</span><span class="token punctuation">,</span> kernel_size<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token punctuation">)</span><span class="token punctuation">,</span> stride<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token punctuation">)</span><span class="token punctuation">)</span>  <span class="token punctuation">(</span>pool_2<span class="token punctuation">)</span><span class="token punctuation">:</span> MaxPool1d<span class="token punctuation">(</span>kernel_size<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span> stride<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">,</span> dilation<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> ceil_mode<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span>  <span class="token punctuation">(</span>relu_2<span class="token punctuation">)</span><span class="token punctuation">:</span> ReLU<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span>   <span class="token string">'dense'</span><span class="token punctuation">:</span> Sequential<span class="token punctuation">(</span>  <span class="token punctuation">(</span>flatten<span class="token punctuation">)</span><span class="token punctuation">:</span> Flatten<span class="token punctuation">(</span><span class="token punctuation">)</span>  <span class="token punctuation">(</span>linear<span class="token punctuation">)</span><span class="token punctuation">:</span> Linear<span class="token punctuation">(</span>in_features<span class="token operator">=</span><span class="token number">6144</span><span class="token punctuation">,</span> out_features<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> bias<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>  <span class="token punctuation">(</span>sigmoid<span class="token punctuation">)</span><span class="token punctuation">:</span> Sigmoid<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">}</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">for</span> param <span class="token keyword">in</span> embedding<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">print</span><span class="token punctuation">(</span>param<span class="token punctuation">.</span>requires_grad<span class="token punctuation">)</span>    <span class="token keyword">print</span><span class="token punctuation">(</span>param<span class="token punctuation">.</span>numel<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token operator">>></span><span class="token operator">></span>Output<span class="token punctuation">:</span><span class="token boolean">False</span><span class="token number">30000</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>]]></content>
      
      
      <categories>
          
          <category> Pytorch </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 基础知识 </tag>
            
            <tag> pytorch </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Pytorch|构建数据管道</title>
      <link href="/2023/04/06/dataset-sampler-he-dataloader/"/>
      <url>/2023/04/06/dataset-sampler-he-dataloader/</url>
      
        <content type="html"><![CDATA[<h1 id="功能简介"><a href="#功能简介" class="headerlink" title="功能简介"></a>功能简介</h1><p>Pytorch通常使用Dataset和DataLoader这两个工具类来构建数据管道。<br>Dataset定义了数据集的内容，它相当于一个类似列表的数据结构，具有确定的长度，能够用索引获取数据集中的元素。<br>而DataLoader定义了按batch加载数据集的方法，它是一个实现了<code>__iter__</code>方法的可迭代对象，每次迭代输出一个batch的数据。<br>DataLoader能够控制batch的大小，batch中元素的采样方法，以及将batch结果整理成模型所需输入形式的方法，并且能够使用多进程读取数据。<br>在绝大部分情况下，用户只需实现Dataset的<code>__len__</code>方法和<code>__getitem__</code>方法，就可以轻松构建自己的数据集，并用默认数据管道进行加载。对于一些复杂的数据集，用户可能还要自己设计 DataLoader中的<code>collate_fn</code>方法以便将获取的一个批次的数据整理成模型需要的输入形式。</p><h1 id="深入理解Dataset和DataLoader原理"><a href="#深入理解Dataset和DataLoader原理" class="headerlink" title="深入理解Dataset和DataLoader原理"></a>深入理解Dataset和DataLoader原理</h1><h2 id="获取一个batch数据的步骤"><a href="#获取一个batch数据的步骤" class="headerlink" title="获取一个batch数据的步骤"></a><strong>获取一个batch数据的步骤</strong></h2><p>假定数据集的特征和标签分别表示为张量$X$和$Y$，数据集可以表示为$(X, Y)$，假定batch大小为$m$。</p><ol><li>首先我们要确定数据集的长度$n$。结果类似：$n = 1000$。</li><li>然后我们从$0$到$n-1$的范围中抽样出$m$个数(batch大小)。假定$m=4$，拿到的结果是一个列表，类似：<code>indices = [1,4,8,9]</code>。</li><li>接着我们从数据集中去取这$m$个数对应下标的元素。拿到的结果是一个元组列表，类似：<code>samples = [(X[1],Y[1]),(X[4],Y[4]),(X[8],Y[8]),(X[9],Y[9])]</code>。</li><li>最后我们将结果整理成<strong>两个张量</strong>作为输出。</li></ol><p>拿到的结果是两个张量，类似batch = (features, labels)，其中<code>features = torch.stack([X[1],X[4],X[8],X[9]])</code>；<code>labels = torch.stack([Y[1],Y[4],Y[8],Y[9]])</code>。</p><h2 id="Dataset和DataLoader的功能分工"><a href="#Dataset和DataLoader的功能分工" class="headerlink" title="Dataset和DataLoader的功能分工"></a>Dataset和DataLoader的功能分工</h2><p>第1个步骤确定数据集的长度是由 Dataset的<code>__len__</code>方法实现的。<br>第2个步骤从$0$到$n-1$的范围中抽样出$m$个数的方法是由 DataLoader的<code>sampler</code>和<code>batch_sampler</code>参数指定的。<code>sampler</code>参数<strong>指定单个元素抽样方法</strong>，一般无需用户设置，程序默认在DataLoader的参数<code>shuffle=True</code>时采用随机抽样，<code>shuffle=False</code>时采用顺序抽样。<code>batch_sampler</code>参数<strong>将多个抽样的元素整理成一个列表</strong>，一般无需用户设置，默认方法在DataLoader的参数<code>drop_last=True</code>时会丢弃数据集最后一个长度不能被batch大小整除的批次，在<code>drop_last=False</code>时保留最后一个批次。<br>第3个步骤的核心逻辑根据下标取数据集中的元素，是由 Dataset的<code>__getitem__</code>方法实现的。<br>第4个步骤的逻辑由DataLoader的参数<code>collate_fn</code>指定。一般情况下也无需用户设置。<br>Dataset和DataLoader的一般使用方式如下：</p><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">import</span> torch   <span class="token keyword">from</span> torch<span class="token punctuation">.</span>utils<span class="token punctuation">.</span>data <span class="token keyword">import</span> TensorDataset<span class="token punctuation">,</span>Dataset<span class="token punctuation">,</span>DataLoader  <span class="token keyword">from</span> torch<span class="token punctuation">.</span>utils<span class="token punctuation">.</span>data <span class="token keyword">import</span> RandomSampler<span class="token punctuation">,</span>BatchSampler   ds <span class="token operator">=</span> TensorDataset<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>randn<span class="token punctuation">(</span><span class="token number">1000</span><span class="token punctuation">,</span><span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">,</span>                     torch<span class="token punctuation">.</span>randint<span class="token punctuation">(</span>low<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">,</span>high<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span>size<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">1000</span><span class="token punctuation">,</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">.</span>float<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>  dl <span class="token operator">=</span> DataLoader<span class="token punctuation">(</span>ds<span class="token punctuation">,</span> batch_size<span class="token operator">=</span><span class="token number">4</span><span class="token punctuation">,</span> drop_last <span class="token operator">=</span> <span class="token boolean">False</span><span class="token punctuation">)</span>  features<span class="token punctuation">,</span> labels <span class="token operator">=</span> next<span class="token punctuation">(</span>iter<span class="token punctuation">(</span>dl<span class="token punctuation">)</span><span class="token punctuation">)</span>  <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"features = "</span><span class="token punctuation">,</span>features <span class="token punctuation">)</span>  <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"labels = "</span><span class="token punctuation">,</span>labels <span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>将DataLoader内部调用方式步骤拆解如下：</p><pre class="line-numbers language-python"><code class="language-python"><span class="token comment" spellcheck="true"># step1: 确定数据集长度 (Dataset的 __len__ 方法实现)  </span>ds <span class="token operator">=</span> TensorDataset<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>randn<span class="token punctuation">(</span><span class="token number">1000</span><span class="token punctuation">,</span><span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">,</span>                     torch<span class="token punctuation">.</span>randint<span class="token punctuation">(</span>low<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">,</span>high<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span>size<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">1000</span><span class="token punctuation">,</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">.</span>float<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>  <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"n = "</span><span class="token punctuation">,</span> len<span class="token punctuation">(</span>ds<span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token comment" spellcheck="true"># len(ds)等价于 ds.__len__()  </span><span class="token comment" spellcheck="true"># step2: 确定抽样indices (DataLoader中的 Sampler和BatchSampler实现)  </span>sampler <span class="token operator">=</span> RandomSampler<span class="token punctuation">(</span>data_source <span class="token operator">=</span> ds<span class="token punctuation">)</span>  batch_sampler <span class="token operator">=</span> BatchSampler<span class="token punctuation">(</span>sampler <span class="token operator">=</span> sampler<span class="token punctuation">,</span>                                batch_size <span class="token operator">=</span> <span class="token number">4</span><span class="token punctuation">,</span> drop_last <span class="token operator">=</span> <span class="token boolean">False</span><span class="token punctuation">)</span>  <span class="token keyword">for</span> idxs <span class="token keyword">in</span> batch_sampler<span class="token punctuation">:</span>      indices <span class="token operator">=</span> idxs      <span class="token keyword">break</span>   <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"indices = "</span><span class="token punctuation">,</span>indices<span class="token punctuation">)</span>  <span class="token comment" spellcheck="true"># step3: 取出一批样本batch (Dataset的 __getitem__ 方法实现)  </span>batch <span class="token operator">=</span> <span class="token punctuation">[</span>ds<span class="token punctuation">[</span>i<span class="token punctuation">]</span> <span class="token keyword">for</span> i <span class="token keyword">in</span>  indices<span class="token punctuation">]</span>  <span class="token comment" spellcheck="true">#  ds[i] 等价于 ds.__getitem__(i)  </span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"batch = "</span><span class="token punctuation">,</span> batch<span class="token punctuation">)</span>  <span class="token comment" spellcheck="true"># step4: 整理成features和labels (DataLoader 的 collate_fn 方法实现)  </span><span class="token keyword">def</span> <span class="token function">collate_fn</span><span class="token punctuation">(</span>batch<span class="token punctuation">)</span><span class="token punctuation">:</span>      features <span class="token operator">=</span> torch<span class="token punctuation">.</span>stack<span class="token punctuation">(</span><span class="token punctuation">[</span>sample<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span> <span class="token keyword">for</span> sample <span class="token keyword">in</span> batch<span class="token punctuation">]</span><span class="token punctuation">)</span>      labels <span class="token operator">=</span> torch<span class="token punctuation">.</span>stack<span class="token punctuation">(</span><span class="token punctuation">[</span>sample<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span> <span class="token keyword">for</span> sample <span class="token keyword">in</span> batch<span class="token punctuation">]</span><span class="token punctuation">)</span>      <span class="token keyword">return</span> features<span class="token punctuation">,</span>labels   features<span class="token punctuation">,</span>labels <span class="token operator">=</span> collate_fn<span class="token punctuation">(</span>batch<span class="token punctuation">)</span>  <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"features = "</span><span class="token punctuation">,</span>features<span class="token punctuation">)</span>  <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"labels = "</span><span class="token punctuation">,</span>labels<span class="token punctuation">)</span> <span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="Dataset和DataLoader的核心源码"><a href="#Dataset和DataLoader的核心源码" class="headerlink" title="Dataset和DataLoader的核心源码"></a><strong>Dataset和DataLoader的核心源码</strong></h2><p>以下是 Dataset和 DataLoader的核心源码，省略了为了提升性能而引入的诸如多进程读取数据相关的代码。</p><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">import</span> torch  <span class="token keyword">class</span> <span class="token class-name">Dataset</span><span class="token punctuation">(</span>object<span class="token punctuation">)</span><span class="token punctuation">:</span>      <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>          <span class="token keyword">pass</span>      <span class="token keyword">def</span> <span class="token function">__len__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>          <span class="token keyword">raise</span> NotImplementedError      <span class="token keyword">def</span> <span class="token function">__getitem__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span>index<span class="token punctuation">)</span><span class="token punctuation">:</span>          <span class="token keyword">raise</span> NotImplementedError  <span class="token keyword">class</span> <span class="token class-name">DataLoader</span><span class="token punctuation">(</span>object<span class="token punctuation">)</span><span class="token punctuation">:</span>      <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span>dataset<span class="token punctuation">,</span>batch_size<span class="token punctuation">,</span>collate_fn <span class="token operator">=</span> None<span class="token punctuation">,</span>shuffle <span class="token operator">=</span> <span class="token boolean">True</span><span class="token punctuation">,</span>drop_last <span class="token operator">=</span> <span class="token boolean">False</span><span class="token punctuation">)</span><span class="token punctuation">:</span>          self<span class="token punctuation">.</span>dataset <span class="token operator">=</span> dataset          self<span class="token punctuation">.</span>sampler <span class="token operator">=</span> torch<span class="token punctuation">.</span>utils<span class="token punctuation">.</span>data<span class="token punctuation">.</span>RandomSampler <span class="token keyword">if</span> shuffle <span class="token keyword">else</span> torch<span class="token punctuation">.</span>utils<span class="token punctuation">.</span>data<span class="token punctuation">.</span>SequentialSampler          self<span class="token punctuation">.</span>batch_sampler <span class="token operator">=</span> torch<span class="token punctuation">.</span>utils<span class="token punctuation">.</span>data<span class="token punctuation">.</span>BatchSampler          self<span class="token punctuation">.</span>sample_iter <span class="token operator">=</span> self<span class="token punctuation">.</span>batch_sampler<span class="token punctuation">(</span>self<span class="token punctuation">.</span>sampler<span class="token punctuation">(</span>self<span class="token punctuation">.</span>dataset<span class="token punctuation">)</span><span class="token punctuation">,</span>                          batch_size <span class="token operator">=</span> batch_size<span class="token punctuation">,</span> drop_last <span class="token operator">=</span> drop_last<span class="token punctuation">)</span>          self<span class="token punctuation">.</span>collate_fn <span class="token operator">=</span> collate_fn <span class="token keyword">if</span> collate_fn <span class="token keyword">is</span> <span class="token operator">not</span> None <span class="token keyword">else</span> torch<span class="token punctuation">.</span>utils<span class="token punctuation">.</span>data<span class="token punctuation">.</span>_utils<span class="token punctuation">.</span>collate<span class="token punctuation">.</span>default_collate      <span class="token keyword">def</span> <span class="token function">__next__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>          indices <span class="token operator">=</span> next<span class="token punctuation">(</span>iter<span class="token punctuation">(</span>self<span class="token punctuation">.</span>sample_iter<span class="token punctuation">)</span><span class="token punctuation">)</span>          batch <span class="token operator">=</span> self<span class="token punctuation">.</span>collate_fn<span class="token punctuation">(</span><span class="token punctuation">[</span>self<span class="token punctuation">.</span>dataset<span class="token punctuation">[</span>i<span class="token punctuation">]</span> <span class="token keyword">for</span> i <span class="token keyword">in</span> indices<span class="token punctuation">]</span><span class="token punctuation">)</span>          <span class="token keyword">return</span> batch      <span class="token keyword">def</span> <span class="token function">__iter__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>          <span class="token keyword">return</span> self <span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h1 id="Sampler原理"><a href="#Sampler原理" class="headerlink" title="Sampler原理"></a>Sampler原理</h1><h2 id="Sampler参数传递"><a href="#Sampler参数传递" class="headerlink" title="Sampler参数传递"></a>Sampler参数传递</h2><p>假设数据是一组图像，每一张图像对应一个index，那么如果要读取数据就只需要对应的index即可，即上面代码中的indices，而选取index的方式有多种，有按顺序的，也有乱序的，所以这个工作需要Sampler完成。<br>首先看一下DataLoader的源代码长什么样。为方便理解只选取了num_works为0的情况（num_works简单理解就是能够并行化地读取数据）。</p><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">DataLoader</span><span class="token punctuation">(</span>object<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> dataset<span class="token punctuation">,</span> batch_size<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> shuffle<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span> sampler<span class="token operator">=</span>None<span class="token punctuation">,</span>                 batch_sampler<span class="token operator">=</span>None<span class="token punctuation">,</span> num_workers<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">,</span> collate_fn<span class="token operator">=</span>default_collate<span class="token punctuation">,</span>                 pin_memory<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span> drop_last<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span> timeout<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">,</span>                 worker_init_fn<span class="token operator">=</span>None<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span>    <span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span>        <span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span>    <span class="token keyword">def</span> <span class="token function">__next__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token keyword">if</span> self<span class="token punctuation">.</span>num_workers <span class="token operator">==</span> <span class="token number">0</span><span class="token punctuation">:</span>              indices <span class="token operator">=</span> next<span class="token punctuation">(</span>self<span class="token punctuation">.</span>sample_iter<span class="token punctuation">)</span>  <span class="token comment" spellcheck="true"># Sampler</span>            batch <span class="token operator">=</span> self<span class="token punctuation">.</span>collate_fn<span class="token punctuation">(</span><span class="token punctuation">[</span>self<span class="token punctuation">.</span>dataset<span class="token punctuation">[</span>i<span class="token punctuation">]</span> <span class="token keyword">for</span> i <span class="token keyword">in</span> indices<span class="token punctuation">]</span><span class="token punctuation">)</span> <span class="token comment" spellcheck="true"># Dataset</span>            <span class="token keyword">if</span> self<span class="token punctuation">.</span>pin_memory<span class="token punctuation">:</span>                batch <span class="token operator">=</span> _utils<span class="token punctuation">.</span>pin_memory<span class="token punctuation">.</span>pin_memory_batch<span class="token punctuation">(</span>batch<span class="token punctuation">)</span>            <span class="token keyword">return</span> batch<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>可以看到初始化参数里有两种sampler：<code>sampler</code>和<code>batch_sampler</code>，都默认为None。前者的作用是生成一系列的index，而batch_sampler则是将sampler生成的indices打包分组，得到一个又一个batch的index。例如下面示例中，<code>BatchSampler</code>将<code>SequentialSampler</code>生成的index按照指定的batch size分组。<br><code>collate_fn</code>的作用就是将一个batch的数据进行整理合并的操作。默认的<code>collate_fn</code>是将img和label分别合并成imgs和labels，所以如果你的<strong>getitem</strong>方法只是返回 img、label，那么你可以使用默认的collate_fn方法，但是如果你每次读取的数据有img，box，label等等，那么你就需要自定义<code>collate_fn</code>来将对应的数据合并成一个batch数据，这样方便后续的训练步骤。</p><pre class="line-numbers language-python"><code class="language-python"><span class="token operator">>></span><span class="token operator">></span><span class="token keyword">in</span> <span class="token punctuation">:</span> list<span class="token punctuation">(</span>BatchSampler<span class="token punctuation">(</span>SequentialSampler<span class="token punctuation">(</span>range<span class="token punctuation">(</span><span class="token number">10</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span> batch_size<span class="token operator">=</span><span class="token number">3</span><span class="token punctuation">,</span> drop_last<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token operator">>></span><span class="token operator">></span>out<span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token number">6</span><span class="token punctuation">,</span> <span class="token number">7</span><span class="token punctuation">,</span> <span class="token number">8</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token number">9</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p>Pytorch中已经实现的Sampler有如下几种：</p><ul><li>SequentialSampler</li><li>RandomSampler</li><li>WeightedSampler</li><li>SubsetRandomSampler</li></ul><p>需要注意的是DataLoader的部分<strong>初始化参数之间存在互斥关系</strong>：</p><ul><li><p>如果自定义了<code>batch_sampler</code>，那么这些参数都必须使用默认值：batch_size，shuffle，sampler，drop_last.</p></li><li><p>如果自定义了<code>sampler</code>，那么<code>shuffle</code>需要设置为False</p></li><li><p>如果<code>sampler</code>和<code>batch_sampler</code>都为None，那么<code>batch_sampler</code>使用Pytorch已经实现好的BatchSampler，而sampler分两种情况：</p><ul><li><p>若<code>shuffle=True</code>，则<code>sampler=RandomSampler(dataset)</code></p></li><li><p>若<code>shuffle=False</code>，则<code>sampler=SequentialSampler(dataset)</code></p><h2 id="自定义Sampler和BatchSampler"><a href="#自定义Sampler和BatchSampler" class="headerlink" title="自定义Sampler和BatchSampler"></a>自定义Sampler和BatchSampler</h2><p>查看源代码其实可以发现，所有采样器其实都继承自同一个父类，即Sampler,其代码定义如下：</p><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">Sampler</span><span class="token punctuation">(</span>object<span class="token punctuation">)</span><span class="token punctuation">:</span><span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> data_source<span class="token punctuation">)</span><span class="token punctuation">:</span>   <span class="token keyword">pass</span><span class="token keyword">def</span> <span class="token function">__iter__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>   <span class="token keyword">raise</span> NotImplementedError<span class="token keyword">def</span> <span class="token function">__len__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>   <span class="token keyword">return</span> len<span class="token punctuation">(</span>self<span class="token punctuation">.</span>data_source<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>所以要做的就是定义好<code>__iter__(self)</code>函数，不过要注意的是该函数的返回值需要是可迭代的。例如<code>SequentialSampler</code>返回的是<code>iter(range(len(self.data_source)))</code>。<br>另外<code>BatchSampler</code>与其他Sampler的主要区别是它需要将Sampler作为参数进行打包，进而每次迭代返回以batch_size为大小的index列表。也就是说在后面的读取数据过程中使用的都是batch sampler。</p><h1 id="创建数据集的3种方式"><a href="#创建数据集的3种方式" class="headerlink" title="创建数据集的3种方式"></a>创建数据集的3种方式</h1><p>Dataset创建数据集常用的方法有：</p></li></ul></li></ul><ol><li>使用<code>torch.utils.data.TensorDataset</code>根据Tensor创建数据集(numpy的array，Pandas的DataFrame需要先转换成Tensor)。</li><li>使用<code>torchvision.datasets.ImageFolder</code>根据图片目录创建图片数据集。</li><li>继承<code>torch.utils.data.Dataset</code>创建自定义数据集。</li></ol><p>此外，还可以通过</p><ul><li><code>torch.utils.data.random_split</code>将一个数据集分割成多份，常用于分割训练集，验证集和测试集。</li><li>调用Dataset的加法运算符($+$)将多个数据集合并成一个数据集。<h2 id="根据Tensor创建数据集"><a href="#根据Tensor创建数据集" class="headerlink" title="根据Tensor创建数据集"></a><strong>根据Tensor创建数据集</strong></h2><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">import</span> numpy <span class="token keyword">as</span> np   <span class="token keyword">import</span> torch   <span class="token keyword">from</span> torch<span class="token punctuation">.</span>utils<span class="token punctuation">.</span>data <span class="token keyword">import</span> TensorDataset<span class="token punctuation">,</span>Dataset<span class="token punctuation">,</span>DataLoader<span class="token punctuation">,</span>random_split <span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre></li></ul><h1 id="根据Tensor创建数据集-1"><a href="#根据Tensor创建数据集-1" class="headerlink" title="根据Tensor创建数据集"></a>根据Tensor创建数据集</h1><p>from sklearn import datasets<br>iris = datasets.load_iris()<br>ds_iris = TensorDataset(torch.tensor(iris.data), torch.tensor(iris.target))  </p><h1 id="分割成训练集和预测集"><a href="#分割成训练集和预测集" class="headerlink" title="分割成训练集和预测集"></a>分割成训练集和预测集</h1><p>n_train = int(len(ds_iris)*0.8)<br>n_val = len(ds_iris) - n_train<br>ds_train, ds_val = random_split(ds_iris,[n_train,n_val])  </p><p>print(type(ds_iris))<br>print(type(ds_train)) </p><pre><code>```python# 使用DataLoader加载数据集  dl_train,dl_val = DataLoader(ds_train, batch_size = 8), DataLoader(ds_val, batch_size = 8)  for features,labels in dl_train:      print(features,labels)      break  # 演示加法运算符（`+`）的合并作用  ds_data = ds_train + ds_val  print(&#39;len(ds_train) = &#39;,len(ds_train))  print(&#39;len(ds_valid) = &#39;,len(ds_val))  print(&#39;len(ds_train+ds_valid) = &#39;,len(ds_data))  print(type(ds_data))  </code></pre><h2 id="根据图片目录创建图片数据集"><a href="#根据图片目录创建图片数据集" class="headerlink" title="根据图片目录创建图片数据集"></a><strong>根据图片目录创建图片数据集</strong></h2><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">import</span> numpy <span class="token keyword">as</span> np   <span class="token keyword">import</span> torch   <span class="token keyword">from</span> torch<span class="token punctuation">.</span>utils<span class="token punctuation">.</span>data <span class="token keyword">import</span> DataLoader  <span class="token keyword">from</span> torchvision <span class="token keyword">import</span> transforms<span class="token punctuation">,</span>datasets   <span class="token comment" spellcheck="true"># 定义图片增强操作  </span>transform_train <span class="token operator">=</span> transforms<span class="token punctuation">.</span>Compose<span class="token punctuation">(</span><span class="token punctuation">[</span>     transforms<span class="token punctuation">.</span>RandomHorizontalFlip<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token comment" spellcheck="true">#随机水平翻转  </span>   transforms<span class="token punctuation">.</span>RandomVerticalFlip<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token comment" spellcheck="true">#随机垂直翻转  </span>   transforms<span class="token punctuation">.</span>RandomRotation<span class="token punctuation">(</span><span class="token number">45</span><span class="token punctuation">)</span><span class="token punctuation">,</span>  <span class="token comment" spellcheck="true">#随机在45度角度内旋转  </span>   transforms<span class="token punctuation">.</span>ToTensor<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token comment" spellcheck="true">#转换成张量并进行标准化</span>  <span class="token punctuation">]</span>  <span class="token punctuation">)</span>   transform_valid <span class="token operator">=</span> transforms<span class="token punctuation">.</span>Compose<span class="token punctuation">(</span><span class="token punctuation">[</span>      transforms<span class="token punctuation">.</span>ToTensor<span class="token punctuation">(</span><span class="token punctuation">)</span>    <span class="token punctuation">]</span>  <span class="token punctuation">)</span>  <span class="token comment" spellcheck="true"># 根据图片目录创建数据集  </span><span class="token keyword">def</span> <span class="token function">transform_label</span><span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">:</span>      <span class="token keyword">return</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span>x<span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">.</span>float<span class="token punctuation">(</span><span class="token punctuation">)</span>  ds_train <span class="token operator">=</span> datasets<span class="token punctuation">.</span>ImageFolder<span class="token punctuation">(</span><span class="token string">"xx/xxx/cifar2/train/"</span><span class="token punctuation">,</span>              transform <span class="token operator">=</span> transform_train<span class="token punctuation">,</span> target_transform<span class="token operator">=</span> transform_label<span class="token punctuation">)</span>  ds_val <span class="token operator">=</span> datasets<span class="token punctuation">.</span>ImageFolder<span class="token punctuation">(</span><span class="token string">"xx/xxx/cifar2/test/"</span><span class="token punctuation">,</span>              transform <span class="token operator">=</span> transform_valid<span class="token punctuation">,</span> target_transform<span class="token operator">=</span> transform_label<span class="token punctuation">)</span>  <span class="token keyword">print</span><span class="token punctuation">(</span>ds_train<span class="token punctuation">.</span>class_to_idx<span class="token punctuation">)</span>  <span class="token comment" spellcheck="true"># 使用DataLoader加载数据集  </span>dl_train <span class="token operator">=</span> DataLoader<span class="token punctuation">(</span>ds_train<span class="token punctuation">,</span> batch_size <span class="token operator">=</span> <span class="token number">50</span><span class="token punctuation">,</span> shuffle <span class="token operator">=</span> <span class="token boolean">True</span><span class="token punctuation">)</span>  dl_val <span class="token operator">=</span> DataLoader<span class="token punctuation">(</span>ds_val<span class="token punctuation">,</span> batch_size <span class="token operator">=</span> <span class="token number">50</span><span class="token punctuation">,</span> shuffle <span class="token operator">=</span> <span class="token boolean">True</span><span class="token punctuation">)</span>  <span class="token keyword">for</span> features<span class="token punctuation">,</span>labels <span class="token keyword">in</span> dl_train<span class="token punctuation">:</span>      <span class="token keyword">print</span><span class="token punctuation">(</span>features<span class="token punctuation">.</span>shape<span class="token punctuation">)</span>      <span class="token keyword">print</span><span class="token punctuation">(</span>labels<span class="token punctuation">.</span>shape<span class="token punctuation">)</span>      <span class="token keyword">break</span>  <span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="创建自定义数据集"><a href="#创建自定义数据集" class="headerlink" title="创建自定义数据集"></a><strong>创建自定义数据集</strong></h2><p>下面通过继承<code>torch.utils.data.Dataset</code>创建自定义数据集的方式来对 cifar2 构建数据管道。</p><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">from</span> pathlib <span class="token keyword">import</span> Path   <span class="token keyword">from</span> PIL <span class="token keyword">import</span> Image   <span class="token keyword">class</span> <span class="token class-name">Cifar2Dataset</span><span class="token punctuation">(</span>Dataset<span class="token punctuation">)</span><span class="token punctuation">:</span>      <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span>imgs_dir<span class="token punctuation">,</span> img_transform<span class="token punctuation">)</span><span class="token punctuation">:</span>          self<span class="token punctuation">.</span>files <span class="token operator">=</span> list<span class="token punctuation">(</span>Path<span class="token punctuation">(</span>imgs_dir<span class="token punctuation">)</span><span class="token punctuation">.</span>rglob<span class="token punctuation">(</span><span class="token string">"*.jpg"</span><span class="token punctuation">)</span><span class="token punctuation">)</span>          self<span class="token punctuation">.</span>transform <span class="token operator">=</span> img_transform      <span class="token keyword">def</span> <span class="token function">__len__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span><span class="token punctuation">)</span><span class="token punctuation">:</span>          <span class="token keyword">return</span> len<span class="token punctuation">(</span>self<span class="token punctuation">.</span>files<span class="token punctuation">)</span>      <span class="token keyword">def</span> <span class="token function">__getitem__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span>i<span class="token punctuation">)</span><span class="token punctuation">:</span>          file_i <span class="token operator">=</span> str<span class="token punctuation">(</span>self<span class="token punctuation">.</span>files<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">)</span>          img <span class="token operator">=</span> Image<span class="token punctuation">.</span>open<span class="token punctuation">(</span>file_i<span class="token punctuation">)</span>          tensor <span class="token operator">=</span> self<span class="token punctuation">.</span>transform<span class="token punctuation">(</span>img<span class="token punctuation">)</span>          label <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">1.0</span><span class="token punctuation">]</span><span class="token punctuation">)</span> <span class="token keyword">if</span>  <span class="token string">"1_automobile"</span> <span class="token keyword">in</span> file_i <span class="token keyword">else</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">0.0</span><span class="token punctuation">]</span><span class="token punctuation">)</span>          <span class="token keyword">return</span> tensor<span class="token punctuation">,</span>label   <span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><pre class="line-numbers language-python"><code class="language-python">train_dir <span class="token operator">=</span> <span class="token string">"xx/xxx/cifar2/train/"</span>  test_dir <span class="token operator">=</span> <span class="token string">"xx/xxx/cifar2/test/"</span> <span class="token comment" spellcheck="true"># 图片增强使用上一小节定义的</span>ds_train <span class="token operator">=</span> Cifar2Dataset<span class="token punctuation">(</span>train_dir<span class="token punctuation">,</span> transform_train<span class="token punctuation">)</span>  ds_val <span class="token operator">=</span> Cifar2Dataset<span class="token punctuation">(</span>test_dir<span class="token punctuation">,</span> transform_val<span class="token punctuation">)</span>  dl_train <span class="token operator">=</span> DataLoader<span class="token punctuation">(</span>ds_train<span class="token punctuation">,</span>batch_size <span class="token operator">=</span> <span class="token number">50</span><span class="token punctuation">,</span>shuffle <span class="token operator">=</span> <span class="token boolean">True</span><span class="token punctuation">)</span>  dl_val <span class="token operator">=</span> DataLoader<span class="token punctuation">(</span>ds_val<span class="token punctuation">,</span>batch_size <span class="token operator">=</span> <span class="token number">50</span><span class="token punctuation">,</span>shuffle <span class="token operator">=</span> <span class="token boolean">True</span><span class="token punctuation">)</span>  <span class="token keyword">for</span> features<span class="token punctuation">,</span>labels <span class="token keyword">in</span> dl_train<span class="token punctuation">:</span>      <span class="token keyword">print</span><span class="token punctuation">(</span>features<span class="token punctuation">.</span>shape<span class="token punctuation">)</span>      <span class="token keyword">print</span><span class="token punctuation">(</span>labels<span class="token punctuation">.</span>shape<span class="token punctuation">)</span>      <span class="token keyword">break</span>  <span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h1 id="使用DataLoader加载数据集"><a href="#使用DataLoader加载数据集" class="headerlink" title="使用DataLoader加载数据集"></a>使用DataLoader加载数据集</h1><p>DataLoader能够控制batch的大小，batch中元素的采样方法，以及将batch结果整理成模型所需输入形式的方法，并且能够使用多进程读取数据。<br>DataLoader的函数签名如下。</p><pre class="line-numbers language-python"><code class="language-python">DataLoader<span class="token punctuation">(</span>      dataset<span class="token punctuation">,</span>      batch_size<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span>      shuffle<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span>      sampler<span class="token operator">=</span>None<span class="token punctuation">,</span>      batch_sampler<span class="token operator">=</span>None<span class="token punctuation">,</span>      num_workers<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">,</span>      collate_fn<span class="token operator">=</span>None<span class="token punctuation">,</span>      pin_memory<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span>      drop_last<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span>      timeout<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">,</span>      worker_init_fn<span class="token operator">=</span>None<span class="token punctuation">,</span>      multiprocessing_context<span class="token operator">=</span>None<span class="token punctuation">,</span>  <span class="token punctuation">)</span>  <span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><ul><li>dataset : 数据集</li><li>batch_size: 批次大小</li><li>shuffle: 是否乱序</li><li>sampler: 样本采样函数，一般无需设置。</li><li>batch_sampler: 批次采样函数，一般无需设置。</li><li>num_workers: 使用多进程读取数据，设置的进程数。</li><li>collate_fn: 整理一个批次数据的函数。</li><li>pin_memory: 是否设置为锁业内存。默认为False，锁业内存不会使用虚拟内存(硬盘)，从锁业内存拷贝到GPU上速度会更快。</li><li>drop_last: 是否丢弃最后一个样本数量不足batch_size批次数据。</li><li>timeout: 加载一个数据批次的最长等待时间，一般无需设置。</li><li>worker_init_fn: 每个worker中dataset的初始化函数，常用于IterableDataset。一般不使用。</li></ul><p>一般情况下，我们仅仅会配置<code>dataset</code>，<code>batch_size</code>，<code>shuffle</code>，<code>num_workers</code>，<code>pin_memory</code>，<code>drop_last</code>这六个参数，有时候对于一些复杂结构的数据集，还需要自定义collate_fn函数，其他参数一般使用默认值即可。<br>DataLoader除了可以加载我们前面讲的<code>torch.utils.data.Dataset</code>外，还能够加载另外一种数据集 <code>torch.utils.data.IterableDataset</code>。和Dataset数据集相当于一种列表结构不同，IterableDataset相当于一种迭代器结构。它更加复杂，一般较少使用。</p>]]></content>
      
      
      <categories>
          
          <category> Pytorch </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 基础知识 </tag>
            
            <tag> pytorch </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Pytorch|构建模型的3种方式</title>
      <link href="/2023/04/05/02-gou-jian-mo-xing-de-3-chong-fang-shi/"/>
      <url>/2023/04/05/02-gou-jian-mo-xing-de-3-chong-fang-shi/</url>
      
        <content type="html"><![CDATA[<p>可以使用以下3种方式构建模型：</p><ol><li>继承nn.Module基类构建自定义模型。</li><li>使用nn.Sequential按层顺序构建模型。</li><li>继承nn.Module基类构建模型，并辅助应用模型容器(nn.Sequential, nn.ModuleList, nn.ModuleDict)进行封装。</li></ol><p>模型的每一层最好有对应的名字，比如XXX_layer，这样可以通过<code>model.XXX_layer</code>访问到这一层，可以查看该层的具体信息，比如该层的权重<code>model.XXX_layer.weight</code>，卷积层的卷积核大小<code>model.XXX_layer.kernel_size</code>，这些信息的查看和定义层的类时传入的参数名一致。</p><h1 id="继承nn-Module类构建模型"><a href="#继承nn-Module类构建模型" class="headerlink" title="继承nn.Module类构建模型"></a>继承nn.Module类构建模型</h1><p>模型中的用到的层一般在<code>__init__()</code>函数中定义，然后在<code>forward()</code>方法中定义模型的正向传播逻辑。</p><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">Net</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>        super<span class="token punctuation">(</span>Net<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>pool1 <span class="token operator">=</span> nn<span class="token punctuation">.</span>MaxPool2d<span class="token punctuation">(</span>kernel_size <span class="token operator">=</span> <span class="token number">2</span><span class="token punctuation">,</span>stride <span class="token operator">=</span> <span class="token number">2</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>conv1 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span>in_channels<span class="token operator">=</span><span class="token number">3</span><span class="token punctuation">,</span>out_channels<span class="token operator">=</span><span class="token number">32</span><span class="token punctuation">,</span>kernel_size <span class="token operator">=</span> <span class="token number">3</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>conv2 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span>in_channels<span class="token operator">=</span><span class="token number">32</span><span class="token punctuation">,</span>out_channels<span class="token operator">=</span><span class="token number">64</span><span class="token punctuation">,</span>kernel_size <span class="token operator">=</span> <span class="token number">5</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>dropout <span class="token operator">=</span> nn<span class="token punctuation">.</span>Dropout2d<span class="token punctuation">(</span>p <span class="token operator">=</span> <span class="token number">0.1</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>adaptive_pool <span class="token operator">=</span> nn<span class="token punctuation">.</span>AdaptiveMaxPool2d<span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>flatten <span class="token operator">=</span> nn<span class="token punctuation">.</span>Flatten<span class="token punctuation">(</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>linear1 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">64</span><span class="token punctuation">,</span><span class="token number">32</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>relu <span class="token operator">=</span> nn<span class="token punctuation">.</span>ReLU<span class="token punctuation">(</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>linear2 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">32</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>sigmoid <span class="token operator">=</span> nn<span class="token punctuation">.</span>Sigmoid<span class="token punctuation">(</span><span class="token punctuation">)</span>    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span>x<span class="token punctuation">)</span><span class="token punctuation">:</span>        x <span class="token operator">=</span> self<span class="token punctuation">.</span>conv1<span class="token punctuation">(</span>x<span class="token punctuation">)</span>        x <span class="token operator">=</span> self<span class="token punctuation">.</span>pool1<span class="token punctuation">(</span>x<span class="token punctuation">)</span>        x <span class="token operator">=</span> self<span class="token punctuation">.</span>conv2<span class="token punctuation">(</span>x<span class="token punctuation">)</span>        x <span class="token operator">=</span> self<span class="token punctuation">.</span>pool1<span class="token punctuation">(</span>x<span class="token punctuation">)</span>        x <span class="token operator">=</span> self<span class="token punctuation">.</span>dropout<span class="token punctuation">(</span>x<span class="token punctuation">)</span>        x <span class="token operator">=</span> self<span class="token punctuation">.</span>adaptive_pool<span class="token punctuation">(</span>x<span class="token punctuation">)</span>        x <span class="token operator">=</span> self<span class="token punctuation">.</span>flatten<span class="token punctuation">(</span>x<span class="token punctuation">)</span>        x <span class="token operator">=</span> self<span class="token punctuation">.</span>linear1<span class="token punctuation">(</span>x<span class="token punctuation">)</span>        x <span class="token operator">=</span> self<span class="token punctuation">.</span>relu<span class="token punctuation">(</span>x<span class="token punctuation">)</span>        x <span class="token operator">=</span> self<span class="token punctuation">.</span>linear2<span class="token punctuation">(</span>x<span class="token punctuation">)</span>        y <span class="token operator">=</span> self<span class="token punctuation">.</span>sigmoid<span class="token punctuation">(</span>x<span class="token punctuation">)</span>        <span class="token keyword">return</span> ynet <span class="token operator">=</span> Net<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>net<span class="token punctuation">)</span><span class="token operator">>></span><span class="token operator">></span>Output<span class="token punctuation">:</span>Net<span class="token punctuation">(</span>  <span class="token punctuation">(</span>pool1<span class="token punctuation">)</span><span class="token punctuation">:</span> MaxPool2d<span class="token punctuation">(</span>kernel_size<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span> stride<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">,</span> dilation<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> ceil_mode<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span>  <span class="token punctuation">(</span>conv1<span class="token punctuation">)</span><span class="token punctuation">:</span> Conv2d<span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">32</span><span class="token punctuation">,</span> kernel_size<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">,</span> stride<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span>  <span class="token punctuation">(</span>conv2<span class="token punctuation">)</span><span class="token punctuation">:</span> Conv2d<span class="token punctuation">(</span><span class="token number">32</span><span class="token punctuation">,</span> <span class="token number">64</span><span class="token punctuation">,</span> kernel_size<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">5</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">)</span><span class="token punctuation">,</span> stride<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span>  <span class="token punctuation">(</span>dropout<span class="token punctuation">)</span><span class="token punctuation">:</span> Dropout2d<span class="token punctuation">(</span>p<span class="token operator">=</span><span class="token number">0.1</span><span class="token punctuation">,</span> inplace<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span>  <span class="token punctuation">(</span>adaptive_pool<span class="token punctuation">)</span><span class="token punctuation">:</span> AdaptiveMaxPool2d<span class="token punctuation">(</span>output_size<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span>  <span class="token punctuation">(</span>flatten<span class="token punctuation">)</span><span class="token punctuation">:</span> Flatten<span class="token punctuation">(</span>start_dim<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> end_dim<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>  <span class="token punctuation">(</span>linear1<span class="token punctuation">)</span><span class="token punctuation">:</span> Linear<span class="token punctuation">(</span>in_features<span class="token operator">=</span><span class="token number">64</span><span class="token punctuation">,</span> out_features<span class="token operator">=</span><span class="token number">32</span><span class="token punctuation">,</span> bias<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>  <span class="token punctuation">(</span>relu<span class="token punctuation">)</span><span class="token punctuation">:</span> ReLU<span class="token punctuation">(</span><span class="token punctuation">)</span>  <span class="token punctuation">(</span>linear2<span class="token punctuation">)</span><span class="token punctuation">:</span> Linear<span class="token punctuation">(</span>in_features<span class="token operator">=</span><span class="token number">32</span><span class="token punctuation">,</span> out_features<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> bias<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>  <span class="token punctuation">(</span>sigmoid<span class="token punctuation">)</span><span class="token punctuation">:</span> Sigmoid<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h1 id="使用nn-Sequential按层顺序构建模型"><a href="#使用nn-Sequential按层顺序构建模型" class="headerlink" title="使用nn.Sequential按层顺序构建模型"></a>使用nn.Sequential按层顺序构建模型</h1><p>使用<code>nn.Sequential()</code>按层顺序构建模型<strong>无需定义forward方法</strong>。仅仅<strong>适合于简单的顺序模型</strong>。</p><h2 id="add-module方法"><a href="#add-module方法" class="headerlink" title="add_module方法"></a>add_module方法</h2><pre class="line-numbers language-python"><code class="language-python">net <span class="token operator">=</span> nn<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span><span class="token punctuation">)</span>net<span class="token punctuation">.</span>add_module<span class="token punctuation">(</span><span class="token string">"pool1"</span><span class="token punctuation">,</span>nn<span class="token punctuation">.</span>MaxPool2d<span class="token punctuation">(</span>kernel_size <span class="token operator">=</span> <span class="token number">2</span><span class="token punctuation">,</span>stride <span class="token operator">=</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">)</span>net<span class="token punctuation">.</span>add_module<span class="token punctuation">(</span><span class="token string">"conv1"</span><span class="token punctuation">,</span>nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span>in_channels<span class="token operator">=</span><span class="token number">3</span><span class="token punctuation">,</span>out_channels<span class="token operator">=</span><span class="token number">32</span><span class="token punctuation">,</span>kernel_size <span class="token operator">=</span> <span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">)</span>net<span class="token punctuation">.</span>add_module<span class="token punctuation">(</span><span class="token string">"conv2"</span><span class="token punctuation">,</span>nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span>in_channels<span class="token operator">=</span><span class="token number">32</span><span class="token punctuation">,</span>out_channels<span class="token operator">=</span><span class="token number">64</span><span class="token punctuation">,</span>kernel_size <span class="token operator">=</span> <span class="token number">5</span><span class="token punctuation">)</span><span class="token punctuation">)</span>net<span class="token punctuation">.</span>add_module<span class="token punctuation">(</span><span class="token string">"dropout"</span><span class="token punctuation">,</span>nn<span class="token punctuation">.</span>Dropout2d<span class="token punctuation">(</span>p <span class="token operator">=</span> <span class="token number">0.1</span><span class="token punctuation">)</span><span class="token punctuation">)</span>net<span class="token punctuation">.</span>add_module<span class="token punctuation">(</span><span class="token string">"adaptive_pool"</span><span class="token punctuation">,</span>nn<span class="token punctuation">.</span>AdaptiveMaxPool2d<span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>net<span class="token punctuation">.</span>add_module<span class="token punctuation">(</span><span class="token string">"flatten"</span><span class="token punctuation">,</span>nn<span class="token punctuation">.</span>Flatten<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>net<span class="token punctuation">.</span>add_module<span class="token punctuation">(</span><span class="token string">"linear1"</span><span class="token punctuation">,</span>nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">64</span><span class="token punctuation">,</span><span class="token number">32</span><span class="token punctuation">)</span><span class="token punctuation">)</span>net<span class="token punctuation">.</span>add_module<span class="token punctuation">(</span><span class="token string">"relu"</span><span class="token punctuation">,</span>nn<span class="token punctuation">.</span>ReLU<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>net<span class="token punctuation">.</span>add_module<span class="token punctuation">(</span><span class="token string">"linear2"</span><span class="token punctuation">,</span>nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">32</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span>net<span class="token punctuation">.</span>add_module<span class="token punctuation">(</span><span class="token string">"sigmoid"</span><span class="token punctuation">,</span>nn<span class="token punctuation">.</span>Sigmoid<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="利用变长参数"><a href="#利用变长参数" class="headerlink" title="利用变长参数"></a>利用变长参数</h2><p>这种方式构建时<strong>不能给每个层指定名称</strong>。</p><pre class="line-numbers language-python"><code class="language-python">net <span class="token operator">=</span> nn<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span>    nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span>in_channels<span class="token operator">=</span><span class="token number">3</span><span class="token punctuation">,</span>out_channels<span class="token operator">=</span><span class="token number">32</span><span class="token punctuation">,</span>kernel_size <span class="token operator">=</span> <span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">,</span>    nn<span class="token punctuation">.</span>MaxPool2d<span class="token punctuation">(</span>kernel_size <span class="token operator">=</span> <span class="token number">2</span><span class="token punctuation">,</span>stride <span class="token operator">=</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">,</span>    nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span>in_channels<span class="token operator">=</span><span class="token number">32</span><span class="token punctuation">,</span>out_channels<span class="token operator">=</span><span class="token number">64</span><span class="token punctuation">,</span>kernel_size <span class="token operator">=</span> <span class="token number">5</span><span class="token punctuation">)</span><span class="token punctuation">,</span>    nn<span class="token punctuation">.</span>MaxPool2d<span class="token punctuation">(</span>kernel_size <span class="token operator">=</span> <span class="token number">2</span><span class="token punctuation">,</span>stride <span class="token operator">=</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">,</span>    nn<span class="token punctuation">.</span>Dropout2d<span class="token punctuation">(</span>p <span class="token operator">=</span> <span class="token number">0.1</span><span class="token punctuation">)</span><span class="token punctuation">,</span>    nn<span class="token punctuation">.</span>AdaptiveMaxPool2d<span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span>    nn<span class="token punctuation">.</span>Flatten<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>    nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">64</span><span class="token punctuation">,</span><span class="token number">32</span><span class="token punctuation">)</span><span class="token punctuation">,</span>    nn<span class="token punctuation">.</span>ReLU<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>    nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">32</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span>    nn<span class="token punctuation">.</span>Sigmoid<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>net<span class="token punctuation">)</span><span class="token operator">>></span><span class="token operator">></span>Output<span class="token punctuation">:</span>Sequential<span class="token punctuation">(</span>  <span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">:</span> Conv2d<span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">32</span><span class="token punctuation">,</span> kernel_size<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">,</span> stride<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span>  <span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">:</span> MaxPool2d<span class="token punctuation">(</span>kernel_size<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span> stride<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">,</span> dilation<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> ceil_mode<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span>  <span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">:</span> Conv2d<span class="token punctuation">(</span><span class="token number">32</span><span class="token punctuation">,</span> <span class="token number">64</span><span class="token punctuation">,</span> kernel_size<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">5</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">)</span><span class="token punctuation">,</span> stride<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span>  <span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">:</span> MaxPool2d<span class="token punctuation">(</span>kernel_size<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span> stride<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">,</span> dilation<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> ceil_mode<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span>  <span class="token punctuation">(</span><span class="token number">4</span><span class="token punctuation">)</span><span class="token punctuation">:</span> Dropout2d<span class="token punctuation">(</span>p<span class="token operator">=</span><span class="token number">0.1</span><span class="token punctuation">,</span> inplace<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span>  <span class="token punctuation">(</span><span class="token number">5</span><span class="token punctuation">)</span><span class="token punctuation">:</span> AdaptiveMaxPool2d<span class="token punctuation">(</span>output_size<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span>  <span class="token punctuation">(</span><span class="token number">6</span><span class="token punctuation">)</span><span class="token punctuation">:</span> Flatten<span class="token punctuation">(</span>start_dim<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> end_dim<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>  <span class="token punctuation">(</span><span class="token number">7</span><span class="token punctuation">)</span><span class="token punctuation">:</span> Linear<span class="token punctuation">(</span>in_features<span class="token operator">=</span><span class="token number">64</span><span class="token punctuation">,</span> out_features<span class="token operator">=</span><span class="token number">32</span><span class="token punctuation">,</span> bias<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>  <span class="token punctuation">(</span><span class="token number">8</span><span class="token punctuation">)</span><span class="token punctuation">:</span> ReLU<span class="token punctuation">(</span><span class="token punctuation">)</span>  <span class="token punctuation">(</span><span class="token number">9</span><span class="token punctuation">)</span><span class="token punctuation">:</span> Linear<span class="token punctuation">(</span>in_features<span class="token operator">=</span><span class="token number">32</span><span class="token punctuation">,</span> out_features<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> bias<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>  <span class="token punctuation">(</span><span class="token number">10</span><span class="token punctuation">)</span><span class="token punctuation">:</span> Sigmoid<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="利用OrderedDict"><a href="#利用OrderedDict" class="headerlink" title="利用OrderedDict"></a>利用OrderedDict</h2><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">from</span> collections <span class="token keyword">import</span> OrderedDictnet <span class="token operator">=</span> nn<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span>        OrderedDict<span class="token punctuation">(</span>          <span class="token punctuation">[</span><span class="token punctuation">(</span><span class="token string">"conv1"</span><span class="token punctuation">,</span>nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span>in_channels<span class="token operator">=</span><span class="token number">3</span><span class="token punctuation">,</span>out_channels<span class="token operator">=</span><span class="token number">32</span><span class="token punctuation">,</span>kernel_size <span class="token operator">=</span> <span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span>            <span class="token punctuation">(</span><span class="token string">"pool1"</span><span class="token punctuation">,</span>nn<span class="token punctuation">.</span>MaxPool2d<span class="token punctuation">(</span>kernel_size <span class="token operator">=</span> <span class="token number">2</span><span class="token punctuation">,</span>stride <span class="token operator">=</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span>            <span class="token punctuation">(</span><span class="token string">"conv2"</span><span class="token punctuation">,</span>nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span>in_channels<span class="token operator">=</span><span class="token number">32</span><span class="token punctuation">,</span>out_channels<span class="token operator">=</span><span class="token number">64</span><span class="token punctuation">,</span>kernel_size <span class="token operator">=</span> <span class="token number">5</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span>            <span class="token punctuation">(</span><span class="token string">"pool2"</span><span class="token punctuation">,</span>nn<span class="token punctuation">.</span>MaxPool2d<span class="token punctuation">(</span>kernel_size <span class="token operator">=</span> <span class="token number">2</span><span class="token punctuation">,</span>stride <span class="token operator">=</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span>            <span class="token punctuation">(</span><span class="token string">"dropout"</span><span class="token punctuation">,</span>nn<span class="token punctuation">.</span>Dropout2d<span class="token punctuation">(</span>p <span class="token operator">=</span> <span class="token number">0.1</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span>            <span class="token punctuation">(</span><span class="token string">"adaptive_pool"</span><span class="token punctuation">,</span>nn<span class="token punctuation">.</span>AdaptiveMaxPool2d<span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span>            <span class="token punctuation">(</span><span class="token string">"flatten"</span><span class="token punctuation">,</span>nn<span class="token punctuation">.</span>Flatten<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span>            <span class="token punctuation">(</span><span class="token string">"linear1"</span><span class="token punctuation">,</span>nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">64</span><span class="token punctuation">,</span><span class="token number">32</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span>            <span class="token punctuation">(</span><span class="token string">"relu"</span><span class="token punctuation">,</span>nn<span class="token punctuation">.</span>ReLU<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span>            <span class="token punctuation">(</span><span class="token string">"linear2"</span><span class="token punctuation">,</span>nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">32</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span>            <span class="token punctuation">(</span><span class="token string">"sigmoid"</span><span class="token punctuation">,</span>nn<span class="token punctuation">.</span>Sigmoid<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>          <span class="token punctuation">]</span>        <span class="token punctuation">)</span>    <span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h1 id="继承nn-Module基类构建模型并应用模型容器进行封装"><a href="#继承nn-Module基类构建模型并应用模型容器进行封装" class="headerlink" title="继承nn.Module基类构建模型并应用模型容器进行封装"></a>继承nn.Module基类构建模型并应用模型容器进行封装</h1><p>当模型的结构比较复杂时，我们可以应用模型容器(<code>nn.Sequential</code>，<code>nn.ModuleList</code>，<code>nn.ModuleDict</code>)对模型的部分结构进行封装。这样做会让模型整体更加有层次感，有时候也能减少代码量。<br>注意，在下面的范例中我们每次仅仅使用一种模型容器，但实际上这些模型容器的使用是非常灵活的，可以在一个模型中任意组合任意嵌套使用。</p><h2 id="nn-Sequential作为模型容器"><a href="#nn-Sequential作为模型容器" class="headerlink" title="nn.Sequential作为模型容器"></a>nn.Sequential作为模型容器</h2><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">Net</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>        super<span class="token punctuation">(</span>Net<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>conv <span class="token operator">=</span> nn<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span>            nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span>in_channels<span class="token operator">=</span><span class="token number">3</span><span class="token punctuation">,</span>out_channels<span class="token operator">=</span><span class="token number">32</span><span class="token punctuation">,</span>kernel_size <span class="token operator">=</span> <span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">,</span>            nn<span class="token punctuation">.</span>MaxPool2d<span class="token punctuation">(</span>kernel_size <span class="token operator">=</span> <span class="token number">2</span><span class="token punctuation">,</span>stride <span class="token operator">=</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">,</span>            nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span>in_channels<span class="token operator">=</span><span class="token number">32</span><span class="token punctuation">,</span>out_channels<span class="token operator">=</span><span class="token number">64</span><span class="token punctuation">,</span>kernel_size <span class="token operator">=</span> <span class="token number">5</span><span class="token punctuation">)</span><span class="token punctuation">,</span>            nn<span class="token punctuation">.</span>MaxPool2d<span class="token punctuation">(</span>kernel_size <span class="token operator">=</span> <span class="token number">2</span><span class="token punctuation">,</span>stride <span class="token operator">=</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">,</span>            nn<span class="token punctuation">.</span>Dropout2d<span class="token punctuation">(</span>p <span class="token operator">=</span> <span class="token number">0.1</span><span class="token punctuation">)</span><span class="token punctuation">,</span>            nn<span class="token punctuation">.</span>AdaptiveMaxPool2d<span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span>        <span class="token punctuation">)</span>        self<span class="token punctuation">.</span>dense <span class="token operator">=</span> nn<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span>            nn<span class="token punctuation">.</span>Flatten<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>            nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">64</span><span class="token punctuation">,</span><span class="token number">32</span><span class="token punctuation">)</span><span class="token punctuation">,</span>            nn<span class="token punctuation">.</span>ReLU<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>            nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">32</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span>            nn<span class="token punctuation">.</span>Sigmoid<span class="token punctuation">(</span><span class="token punctuation">)</span>        <span class="token punctuation">)</span>    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span>x<span class="token punctuation">)</span><span class="token punctuation">:</span>        x <span class="token operator">=</span> self<span class="token punctuation">.</span>conv<span class="token punctuation">(</span>x<span class="token punctuation">)</span>        y <span class="token operator">=</span> self<span class="token punctuation">.</span>dense<span class="token punctuation">(</span>x<span class="token punctuation">)</span>        <span class="token keyword">return</span> y net <span class="token operator">=</span> Net<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>net<span class="token punctuation">)</span><span class="token operator">>></span><span class="token operator">></span>Output<span class="token punctuation">:</span>Net<span class="token punctuation">(</span>  <span class="token punctuation">(</span>conv<span class="token punctuation">)</span><span class="token punctuation">:</span> Sequential<span class="token punctuation">(</span>    <span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">:</span> Conv2d<span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">32</span><span class="token punctuation">,</span> kernel_size<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">,</span> stride<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span>    <span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">:</span> MaxPool2d<span class="token punctuation">(</span>kernel_size<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span> stride<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">,</span> dilation<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> ceil_mode<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span>    <span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">:</span> Conv2d<span class="token punctuation">(</span><span class="token number">32</span><span class="token punctuation">,</span> <span class="token number">64</span><span class="token punctuation">,</span> kernel_size<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">5</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">)</span><span class="token punctuation">,</span> stride<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span>    <span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">:</span> MaxPool2d<span class="token punctuation">(</span>kernel_size<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span> stride<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">,</span> dilation<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> ceil_mode<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span>    <span class="token punctuation">(</span><span class="token number">4</span><span class="token punctuation">)</span><span class="token punctuation">:</span> Dropout2d<span class="token punctuation">(</span>p<span class="token operator">=</span><span class="token number">0.1</span><span class="token punctuation">,</span> inplace<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span>    <span class="token punctuation">(</span><span class="token number">5</span><span class="token punctuation">)</span><span class="token punctuation">:</span> AdaptiveMaxPool2d<span class="token punctuation">(</span>output_size<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span>  <span class="token punctuation">)</span>  <span class="token punctuation">(</span>dense<span class="token punctuation">)</span><span class="token punctuation">:</span> Sequential<span class="token punctuation">(</span>    <span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">:</span> Flatten<span class="token punctuation">(</span><span class="token punctuation">)</span>    <span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">:</span> Linear<span class="token punctuation">(</span>in_features<span class="token operator">=</span><span class="token number">64</span><span class="token punctuation">,</span> out_features<span class="token operator">=</span><span class="token number">32</span><span class="token punctuation">,</span> bias<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>    <span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">:</span> ReLU<span class="token punctuation">(</span><span class="token punctuation">)</span>    <span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">:</span> Linear<span class="token punctuation">(</span>in_features<span class="token operator">=</span><span class="token number">32</span><span class="token punctuation">,</span> out_features<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> bias<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>    <span class="token punctuation">(</span><span class="token number">4</span><span class="token punctuation">)</span><span class="token punctuation">:</span> Sigmoid<span class="token punctuation">(</span><span class="token punctuation">)</span>  <span class="token punctuation">)</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="nn-ModuleList作为模型容器"><a href="#nn-ModuleList作为模型容器" class="headerlink" title="nn.ModuleList作为模型容器"></a>nn.ModuleList作为模型容器</h2><p>注意<code>ModuleList</code>不能用Python中的列表代替。</p><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">Net</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>        super<span class="token punctuation">(</span>Net<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>layers <span class="token operator">=</span> nn<span class="token punctuation">.</span>ModuleList<span class="token punctuation">(</span><span class="token punctuation">[</span>            nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span>in_channels<span class="token operator">=</span><span class="token number">3</span><span class="token punctuation">,</span>out_channels<span class="token operator">=</span><span class="token number">32</span><span class="token punctuation">,</span>kernel_size <span class="token operator">=</span> <span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">,</span>            nn<span class="token punctuation">.</span>MaxPool2d<span class="token punctuation">(</span>kernel_size <span class="token operator">=</span> <span class="token number">2</span><span class="token punctuation">,</span>stride <span class="token operator">=</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">,</span>            nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span>in_channels<span class="token operator">=</span><span class="token number">32</span><span class="token punctuation">,</span>out_channels<span class="token operator">=</span><span class="token number">64</span><span class="token punctuation">,</span>kernel_size <span class="token operator">=</span> <span class="token number">5</span><span class="token punctuation">)</span><span class="token punctuation">,</span>            nn<span class="token punctuation">.</span>MaxPool2d<span class="token punctuation">(</span>kernel_size <span class="token operator">=</span> <span class="token number">2</span><span class="token punctuation">,</span>stride <span class="token operator">=</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">,</span>            nn<span class="token punctuation">.</span>Dropout2d<span class="token punctuation">(</span>p <span class="token operator">=</span> <span class="token number">0.1</span><span class="token punctuation">)</span><span class="token punctuation">,</span>            nn<span class="token punctuation">.</span>AdaptiveMaxPool2d<span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span>            nn<span class="token punctuation">.</span>Flatten<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>            nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">64</span><span class="token punctuation">,</span><span class="token number">32</span><span class="token punctuation">)</span><span class="token punctuation">,</span>            nn<span class="token punctuation">.</span>ReLU<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>            nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">32</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span>            nn<span class="token punctuation">.</span>Sigmoid<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">]</span>        <span class="token punctuation">)</span>    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span>x<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token keyword">for</span> layer <span class="token keyword">in</span> self<span class="token punctuation">.</span>layers<span class="token punctuation">:</span>            x <span class="token operator">=</span> layer<span class="token punctuation">(</span>x<span class="token punctuation">)</span>        <span class="token keyword">return</span> xnet <span class="token operator">=</span> Net<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>net<span class="token punctuation">)</span><span class="token operator">>></span><span class="token operator">></span>Output<span class="token punctuation">:</span>Net<span class="token punctuation">(</span>  <span class="token punctuation">(</span>layers<span class="token punctuation">)</span><span class="token punctuation">:</span> ModuleList<span class="token punctuation">(</span>    <span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">:</span> Conv2d<span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">32</span><span class="token punctuation">,</span> kernel_size<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">,</span> stride<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span>    <span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">:</span> MaxPool2d<span class="token punctuation">(</span>kernel_size<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span> stride<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">,</span> dilation<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> ceil_mode<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span>    <span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">:</span> Conv2d<span class="token punctuation">(</span><span class="token number">32</span><span class="token punctuation">,</span> <span class="token number">64</span><span class="token punctuation">,</span> kernel_size<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">5</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">)</span><span class="token punctuation">,</span> stride<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span>    <span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">:</span> MaxPool2d<span class="token punctuation">(</span>kernel_size<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span> stride<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">,</span> dilation<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> ceil_mode<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span>    <span class="token punctuation">(</span><span class="token number">4</span><span class="token punctuation">)</span><span class="token punctuation">:</span> Dropout2d<span class="token punctuation">(</span>p<span class="token operator">=</span><span class="token number">0.1</span><span class="token punctuation">,</span> inplace<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span>    <span class="token punctuation">(</span><span class="token number">5</span><span class="token punctuation">)</span><span class="token punctuation">:</span> AdaptiveMaxPool2d<span class="token punctuation">(</span>output_size<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span>    <span class="token punctuation">(</span><span class="token number">6</span><span class="token punctuation">)</span><span class="token punctuation">:</span> Flatten<span class="token punctuation">(</span><span class="token punctuation">)</span>    <span class="token punctuation">(</span><span class="token number">7</span><span class="token punctuation">)</span><span class="token punctuation">:</span> Linear<span class="token punctuation">(</span>in_features<span class="token operator">=</span><span class="token number">64</span><span class="token punctuation">,</span> out_features<span class="token operator">=</span><span class="token number">32</span><span class="token punctuation">,</span> bias<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>    <span class="token punctuation">(</span><span class="token number">8</span><span class="token punctuation">)</span><span class="token punctuation">:</span> ReLU<span class="token punctuation">(</span><span class="token punctuation">)</span>    <span class="token punctuation">(</span><span class="token number">9</span><span class="token punctuation">)</span><span class="token punctuation">:</span> Linear<span class="token punctuation">(</span>in_features<span class="token operator">=</span><span class="token number">32</span><span class="token punctuation">,</span> out_features<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> bias<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>    <span class="token punctuation">(</span><span class="token number">10</span><span class="token punctuation">)</span><span class="token punctuation">:</span> Sigmoid<span class="token punctuation">(</span><span class="token punctuation">)</span>  <span class="token punctuation">)</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="nn-ModuleDict作为模型容器"><a href="#nn-ModuleDict作为模型容器" class="headerlink" title="nn.ModuleDict作为模型容器"></a>nn.ModuleDict作为模型容器</h2><p>注意<code>的ModuleDict</code>不能用Python中的字典代替。</p><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">Net</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>        super<span class="token punctuation">(</span>Net<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>layers_dict <span class="token operator">=</span> nn<span class="token punctuation">.</span>ModuleDict<span class="token punctuation">(</span>            <span class="token punctuation">{</span><span class="token string">"conv1"</span><span class="token punctuation">:</span>nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span>in_channels<span class="token operator">=</span><span class="token number">3</span><span class="token punctuation">,</span>out_channels<span class="token operator">=</span><span class="token number">32</span><span class="token punctuation">,</span>kernel_size <span class="token operator">=</span> <span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">,</span>               <span class="token string">"pool"</span><span class="token punctuation">:</span> nn<span class="token punctuation">.</span>MaxPool2d<span class="token punctuation">(</span>kernel_size <span class="token operator">=</span> <span class="token number">2</span><span class="token punctuation">,</span>stride <span class="token operator">=</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">,</span>               <span class="token string">"conv2"</span><span class="token punctuation">:</span>nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span>in_channels<span class="token operator">=</span><span class="token number">32</span><span class="token punctuation">,</span>out_channels<span class="token operator">=</span><span class="token number">64</span><span class="token punctuation">,</span>kernel_size <span class="token operator">=</span> <span class="token number">5</span><span class="token punctuation">)</span><span class="token punctuation">,</span>               <span class="token string">"dropout"</span><span class="token punctuation">:</span> nn<span class="token punctuation">.</span>Dropout2d<span class="token punctuation">(</span>p <span class="token operator">=</span> <span class="token number">0.1</span><span class="token punctuation">)</span><span class="token punctuation">,</span>               <span class="token string">"adaptive"</span><span class="token punctuation">:</span>nn<span class="token punctuation">.</span>AdaptiveMaxPool2d<span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span>               <span class="token string">"flatten"</span><span class="token punctuation">:</span> nn<span class="token punctuation">.</span>Flatten<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>               <span class="token string">"linear1"</span><span class="token punctuation">:</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">64</span><span class="token punctuation">,</span><span class="token number">32</span><span class="token punctuation">)</span><span class="token punctuation">,</span>               <span class="token string">"relu"</span><span class="token punctuation">:</span>nn<span class="token punctuation">.</span>ReLU<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>               <span class="token string">"linear2"</span><span class="token punctuation">:</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">32</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span>               <span class="token string">"sigmoid"</span><span class="token punctuation">:</span> nn<span class="token punctuation">.</span>Sigmoid<span class="token punctuation">(</span><span class="token punctuation">)</span>              <span class="token punctuation">}</span><span class="token punctuation">)</span>    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span>x<span class="token punctuation">)</span><span class="token punctuation">:</span>        layers <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token string">"conv1"</span><span class="token punctuation">,</span><span class="token string">"pool"</span><span class="token punctuation">,</span><span class="token string">"conv2"</span><span class="token punctuation">,</span><span class="token string">"pool"</span><span class="token punctuation">,</span><span class="token string">"dropout"</span><span class="token punctuation">,</span><span class="token string">"adaptive"</span><span class="token punctuation">,</span>                  <span class="token string">"flatten"</span><span class="token punctuation">,</span><span class="token string">"linear1"</span><span class="token punctuation">,</span><span class="token string">"relu"</span><span class="token punctuation">,</span><span class="token string">"linear2"</span><span class="token punctuation">,</span><span class="token string">"sigmoid"</span><span class="token punctuation">]</span>        <span class="token keyword">for</span> layer <span class="token keyword">in</span> layers<span class="token punctuation">:</span>            x <span class="token operator">=</span> self<span class="token punctuation">.</span>layers_dict<span class="token punctuation">[</span>layer<span class="token punctuation">]</span><span class="token punctuation">(</span>x<span class="token punctuation">)</span>        <span class="token keyword">return</span> xnet <span class="token operator">=</span> Net<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>net<span class="token punctuation">)</span><span class="token operator">>></span><span class="token operator">></span>Output<span class="token punctuation">:</span>Net<span class="token punctuation">(</span>  <span class="token punctuation">(</span>layers_dict<span class="token punctuation">)</span><span class="token punctuation">:</span> ModuleDict<span class="token punctuation">(</span>    <span class="token punctuation">(</span>adaptive<span class="token punctuation">)</span><span class="token punctuation">:</span> AdaptiveMaxPool2d<span class="token punctuation">(</span>output_size<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span>    <span class="token punctuation">(</span>conv1<span class="token punctuation">)</span><span class="token punctuation">:</span> Conv2d<span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">32</span><span class="token punctuation">,</span> kernel_size<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">,</span> stride<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span>    <span class="token punctuation">(</span>conv2<span class="token punctuation">)</span><span class="token punctuation">:</span> Conv2d<span class="token punctuation">(</span><span class="token number">32</span><span class="token punctuation">,</span> <span class="token number">64</span><span class="token punctuation">,</span> kernel_size<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">5</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">)</span><span class="token punctuation">,</span> stride<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span>    <span class="token punctuation">(</span>dropout<span class="token punctuation">)</span><span class="token punctuation">:</span> Dropout2d<span class="token punctuation">(</span>p<span class="token operator">=</span><span class="token number">0.1</span><span class="token punctuation">,</span> inplace<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span>    <span class="token punctuation">(</span>flatten<span class="token punctuation">)</span><span class="token punctuation">:</span> Flatten<span class="token punctuation">(</span><span class="token punctuation">)</span>    <span class="token punctuation">(</span>linear1<span class="token punctuation">)</span><span class="token punctuation">:</span> Linear<span class="token punctuation">(</span>in_features<span class="token operator">=</span><span class="token number">64</span><span class="token punctuation">,</span> out_features<span class="token operator">=</span><span class="token number">32</span><span class="token punctuation">,</span> bias<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>    <span class="token punctuation">(</span>linear2<span class="token punctuation">)</span><span class="token punctuation">:</span> Linear<span class="token punctuation">(</span>in_features<span class="token operator">=</span><span class="token number">32</span><span class="token punctuation">,</span> out_features<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> bias<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>    <span class="token punctuation">(</span>pool<span class="token punctuation">)</span><span class="token punctuation">:</span> MaxPool2d<span class="token punctuation">(</span>kernel_size<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span> stride<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">,</span> dilation<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> ceil_mode<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span>    <span class="token punctuation">(</span>relu<span class="token punctuation">)</span><span class="token punctuation">:</span> ReLU<span class="token punctuation">(</span><span class="token punctuation">)</span>    <span class="token punctuation">(</span>sigmoid<span class="token punctuation">)</span><span class="token punctuation">:</span> Sigmoid<span class="token punctuation">(</span><span class="token punctuation">)</span>  <span class="token punctuation">)</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>]]></content>
      
      
      <categories>
          
          <category> Pytorch </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 基础知识 </tag>
            
            <tag> pytorch </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Pytorch|张量和张量操作</title>
      <link href="/2023/04/01/01-zhang-liang-he-zhang-liang-cao-zuo/"/>
      <url>/2023/04/01/01-zhang-liang-he-zhang-liang-cao-zuo/</url>
      
        <content type="html"><![CDATA[<h1 id="张量"><a href="#张量" class="headerlink" title="张量"></a>张量</h1><h2 id="创建张量"><a href="#创建张量" class="headerlink" title="创建张量"></a>创建张量</h2><h3 id="常规方式创建"><a href="#常规方式创建" class="headerlink" title="常规方式创建"></a>常规方式创建</h3><p>常见的初始化有<code>torch.tensor</code>和<code>torch.Tensor</code>，其中的区别是</p><ul><li>tensor()：接收现有数据，通过numpy 或 list 的现有数据初始化</li><li>Tensor()：1. 接收数据的维度(, )生成随机数张量；2. 接收现有的数据[, ]生成指定数据张量<pre class="line-numbers language-python"><code class="language-python">tensor<span class="token operator">=</span>torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">3</span><span class="token punctuation">,</span><span class="token number">4</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre></li></ul><p>tensor=torch.Tensor([2,3,4])</p><pre><code>常见的创建方式还有`torch.as_tensor()`，`torch.from_numpy()`。这两种方式都是接收现有数据的，而且生成的张量与原有数据是**内存共享**的。```pythona = np.array([1,2,3])tensor = torch.from_numpy(a)tensor = torch.as_tensor(a)</code></pre><p>在不考虑性能方面，一般情况下使用torch.tensor()方法居多，那么如果要考虑性能方面，首先肯定是要从<strong>torch.as_tensor()和torch.from_numpy()两种方法中选择，因为在创建tensor的过程中，它俩是共享内存的，不需要额外创建一份数据</strong>。<br>两者的区别是torch.from_numpy()只能接收numpy数组，而torch.as_tensor()不仅可以接收numpy数组，还可以接收python的list类型数据。</p><h3 id="序列生成"><a href="#序列生成" class="headerlink" title="序列生成"></a>序列生成</h3><ul><li><code>torch.arange()</code>：接收参数：a,b,step，输出[a,b)范围内step步长的等差序列组成的tensor，数据类型为int。</li><li><code>torc.range()</code>：接收参数：a,b,step，输出[a,b]范围内step步长的等差序列组成的tensor，数据类型为float。</li></ul><p>arrange()方法和range()方法的区别在于：range()方法可以输出结果包含区间右侧b这个数值，且range()方法的数据类型为float。</p><pre class="line-numbers language-python"><code class="language-python">a <span class="token operator">=</span> torch<span class="token punctuation">.</span>arange<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span><span class="token number">10</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>a<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>a<span class="token punctuation">.</span>type<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>b <span class="token operator">=</span> torch<span class="token punctuation">.</span>range<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span><span class="token number">10</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>b<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>b<span class="token punctuation">.</span>type<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token operator">>></span><span class="token operator">></span>Output<span class="token punctuation">:</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">6</span><span class="token punctuation">,</span> <span class="token number">8</span><span class="token punctuation">]</span><span class="token punctuation">)</span>torch<span class="token punctuation">.</span>LongTensortensor<span class="token punctuation">(</span><span class="token punctuation">[</span> <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span>  <span class="token number">2</span><span class="token punctuation">.</span><span class="token punctuation">,</span>  <span class="token number">4</span><span class="token punctuation">.</span><span class="token punctuation">,</span>  <span class="token number">6</span><span class="token punctuation">.</span><span class="token punctuation">,</span>  <span class="token number">8</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">10</span><span class="token punctuation">.</span><span class="token punctuation">]</span><span class="token punctuation">)</span>torch<span class="token punctuation">.</span>FloatTensor<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>生成一个0到n-1的n-1个整数的随机排列</p><pre class="line-numbers language-python"><code class="language-python">a <span class="token operator">=</span> torch<span class="token punctuation">.</span>randperm<span class="token punctuation">(</span><span class="token number">10</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>a<span class="token punctuation">)</span><span class="token operator">>></span><span class="token operator">></span>Output<span class="token punctuation">:</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">7</span><span class="token punctuation">,</span> <span class="token number">9</span><span class="token punctuation">,</span> <span class="token number">8</span><span class="token punctuation">,</span> <span class="token number">6</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="使用随机数据创建"><a href="#使用随机数据创建" class="headerlink" title="使用随机数据创建"></a>使用随机数据创建</h3><ul><li><code>torch.rand()</code>：输入参数为一个shape，创建指定形状大小的tensor，数据为float32类型的随机数。产生[0,1]均匀分布的数据。</li><li><code>torch.randint()</code>：指定数据范围为[a, b)的随机tensor创建。输入参数为一个a,b,(x,y,…)，创建(x,y,…)大小的tensor，数据的范围为[a,b)，数据类型为整数值。</li><li><code>torch.rand_like()</code>：输入参数为一个<strong>浮点型</strong>的tensor，创建一个与输入tensor数据同大小的矩阵，数据为<strong>与原始tensor相同类型</strong>的<strong>浮点型</strong>随机数。产生[0,1]均匀分布的数据。<pre class="line-numbers language-python"><code class="language-python"><span class="token comment" spellcheck="true"># 等价于 a = torch.rand(2,3)</span>a <span class="token operator">=</span> torch<span class="token punctuation">.</span>rand<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">3</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>a<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre></li></ul><p>c = torch.rand_like(a)<br>print(c)<br>print(c.type())</p><p>b = torch.randint(1,10,(2,3))<br>print(b)<br>print(b.type())</p><blockquote><blockquote><blockquote><p>Output:<br>tensor([[0.8703, 0.8061, 0.5126],<br>        [0.6069, 0.5985, 0.4657]])</p></blockquote></blockquote></blockquote><p>tensor([[0.0937, 0.1968, 0.2269],<br>        [0.3653, 0.9386, 0.9892]])<br>torch.FloatTensor</p><p>tensor([[9, 3, 6],<br>        [4, 1, 1]])<br>torch.LongTensor</p><pre><code>- `torch.randn()`：接受参数为shape，输出一个数据满足标准正态分布`N(0,1)`的随机数tensor。- `torch.normal()`：接受参数为：mean, std, shape，分别为所创建数据的均值，标准差和形状，输出一个满足上述参数的广义的正态分布tensor。```pythona = torch.randn(3,3)b = torch.normal(mean = torch.zeros(3,3), std = torch.ones(3,3))&gt;&gt;&gt;Output:tensor([[-0.6225, -0.1253, -0.1083],        [-0.3199, -0.5670,  0.2898],        [-0.6500,  0.9275,  1.0377]])tensor([[ 0.5507,  0.2704,  0.6472],        [ 0.2490, -0.3354,  0.4564],        [-0.6255,  0.4539, -1.3740]])</code></pre><p>使用<code>torch.Tensor()</code>创建随机数张量。</p><pre class="line-numbers language-python"><code class="language-python">a <span class="token operator">=</span> torch<span class="token punctuation">.</span>IntTensor<span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span><span class="token number">4</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>a<span class="token punctuation">)</span><span class="token operator">>></span><span class="token operator">></span>Output<span class="token punctuation">:</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span> <span class="token number">241405024</span><span class="token punctuation">,</span>      <span class="token number">32635</span><span class="token punctuation">,</span> <span class="token number">1567730800</span><span class="token punctuation">]</span><span class="token punctuation">,</span>        <span class="token punctuation">[</span>     <span class="token number">22007</span><span class="token punctuation">,</span>         <span class="token number">32</span><span class="token punctuation">,</span>          <span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">,</span> dtype<span class="token operator">=</span>torch<span class="token punctuation">.</span>int32<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="特殊矩阵的创建"><a href="#特殊矩阵的创建" class="headerlink" title="特殊矩阵的创建"></a>特殊矩阵的创建</h3><p>在数学计算中会经常使用到全0矩阵、全1矩阵、单位矩阵。其创建方法如下：</p><ul><li>torch.zeros()：接收参数为shape，输出一个shape大小的全0 Tensor。torch.zeros_like(input, dtype)</li><li>torch.ones()：接收参数为shape，输出一个shape大小的全1 Tensor。torch.ones_like(input, dtype)</li><li>torch.eye()：接收参数为shape，输出一个shape大小的单位矩阵Tensor。<pre class="line-numbers language-python"><code class="language-python">a <span class="token operator">=</span> torch<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span><span class="token number">3</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>a<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>a<span class="token punctuation">.</span>type<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre></li></ul><p>b = torch.ones(3,3)<br>print(b)<br>print(b.type())</p><p>c = torch.eye(3,3)<br>print(c)<br>print(c.type())</p><p>print(torch.eye(2))</p><blockquote><blockquote><blockquote><p>Output:<br>tensor([[0., 0., 0.],<br>        [0., 0., 0.],<br>        [0., 0., 0.]])<br>torch.FloatTensor</p></blockquote></blockquote></blockquote><p>tensor([[1., 1., 1.],<br>        [1., 1., 1.],<br>        [1., 1., 1.]])<br>torch.FloatTensor</p><p>tensor([[1., 0., 0.],<br>        [0., 1., 0.],<br>        [0., 0., 1.]])<br>torch.FloatTensor</p><p>tensor([[1., 0.],<br>        [0., 1.]])</p><pre><code>`torch.full()`：接收参数为shape, x，输出一个shape大小的元素全为x的tensor。```pythona = torch.full((2,3),1)print(a)print(a.type())a = torch.ones((3,3), dtype = torch.int)torch.fill_(a,5) print(a)&gt;&gt;&gt;Output:tensor([[1, 1, 1],        [1, 1, 1]])torch.LongTensortensor([[5., 5., 5.],        [5., 5., 5.],        [5., 5., 5.]])</code></pre><p><code>torch.empty()</code>接收参数为shape，表示创建一个<strong>未初始化</strong>的张量。torch.empty_like(input, dtype)</p><pre class="line-numbers language-python"><code class="language-python">a <span class="token operator">=</span> torch<span class="token punctuation">.</span>empty<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">3</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>a<span class="token punctuation">)</span><span class="token comment" spellcheck="true"># 等价于 a = torch.empty([2,3])</span><span class="token operator">>></span><span class="token operator">></span>Output<span class="token punctuation">:</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">8.8319e+17</span><span class="token punctuation">,</span> <span class="token number">3.0838e-41</span><span class="token punctuation">,</span> <span class="token number">9.5811e+17</span><span class="token punctuation">]</span><span class="token punctuation">,</span>        <span class="token punctuation">[</span><span class="token number">3.0838e-41</span><span class="token punctuation">,</span> <span class="token number">9.1995e-41</span><span class="token punctuation">,</span> <span class="token number">4.5989e-40</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="张量的数据类型"><a href="#张量的数据类型" class="headerlink" title="张量的数据类型"></a>张量的数据类型</h2><p>Pytorch的基本数据结构是张量Tensor。张量即多维数组。Pytorch的张量和numpy中的array很类似。</p><table><thead><tr><th>Data type</th><th>dtype</th><th>CPU tensor</th><th>GPU tensor</th></tr></thead><tbody><tr><td>16-bit floating point</td><td>torch.float16 or torch.half</td><td>torch.HalfTensor</td><td>torch.cuda.HalfTensor</td></tr><tr><td>32-bit floating point</td><td>torch.float32 or torch.float</td><td>torch.FloatTensor</td><td>torch.cuda.FloatTensor</td></tr><tr><td>64-bit floating point</td><td>torch.float64 or torch.double</td><td>torch.DoubleTensor</td><td>torch.cuda.DoubleTensor</td></tr><tr><td>8-bit integer (signed)</td><td>torch.int8</td><td>torch.CharTensor</td><td>torch.cuda.CharTensor</td></tr><tr><td>16-bit integer (signed)</td><td>torch.int16 or torch.short</td><td>torch.ShortTensor</td><td>torch.cuda.ShortTensor</td></tr><tr><td>32-bit integer (signed)</td><td>torch.int32 or torch.int</td><td>torch.IntTensor</td><td>torch.cuda.IntTensor</td></tr><tr><td>64-bit integer (signed)</td><td>torch.int64 or torch.long</td><td>torch.LongTensor</td><td>torch.cuda.LongTensor</td></tr><tr><td>8-bit integer (unsigned)</td><td>torch.uint8</td><td>torch.ByteTensor</td><td>torch.cuda.ByteTensor</td></tr></tbody></table><p>张量的数据类型和numpy.array基本一一对应，但是不支持str类型。包括:</p><ul><li>torch.float64(torch.double), </li><li><strong>torch.float32(torch.float)</strong>, </li><li>torch.float16, </li><li>torch.int64(torch.long), </li><li>torch.int32(torch.int), </li><li>torch.int16, </li><li>torch.int8, </li><li>torch.uint8, </li><li>torch.bool</li></ul><p>一般神经网络建模使用的都是torch.float32类型。</p><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">import</span> numpy <span class="token keyword">as</span> np<span class="token keyword">import</span> torch <span class="token comment" spellcheck="true"># 自动推断数据类型</span>i <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token keyword">print</span><span class="token punctuation">(</span>i<span class="token punctuation">,</span> i<span class="token punctuation">.</span>dtype<span class="token punctuation">)</span>x <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token number">2.0</span><span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token keyword">print</span><span class="token punctuation">(</span>x<span class="token punctuation">,</span> x<span class="token punctuation">.</span>dtype<span class="token punctuation">)</span>b <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token keyword">print</span><span class="token punctuation">(</span>b<span class="token punctuation">,</span> b<span class="token punctuation">.</span>dtype<span class="token punctuation">)</span><span class="token operator">>></span><span class="token operator">></span>Output<span class="token punctuation">:</span>tensor<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>int64tensor<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">.</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float32tensor<span class="token punctuation">(</span><span class="token boolean">True</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>bool<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><pre class="line-numbers language-python"><code class="language-python"><span class="token comment" spellcheck="true"># 指定数据类型</span>i <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> dtype <span class="token operator">=</span> torch<span class="token punctuation">.</span>int32<span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token keyword">print</span><span class="token punctuation">(</span>i<span class="token punctuation">,</span>i<span class="token punctuation">.</span>dtype<span class="token punctuation">)</span>x <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token number">2.0</span><span class="token punctuation">,</span> dtype <span class="token operator">=</span> torch<span class="token punctuation">.</span>double<span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token keyword">print</span><span class="token punctuation">(</span>x<span class="token punctuation">,</span>x<span class="token punctuation">.</span>dtype<span class="token punctuation">)</span><span class="token operator">>></span><span class="token operator">></span>Output<span class="token punctuation">:</span>tensor<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> dtype<span class="token operator">=</span>torch<span class="token punctuation">.</span>int32<span class="token punctuation">)</span> torch<span class="token punctuation">.</span>int32tensor<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">.</span><span class="token punctuation">,</span> dtype<span class="token operator">=</span>torch<span class="token punctuation">.</span>float64<span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float64<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><pre class="line-numbers language-python"><code class="language-python"><span class="token comment" spellcheck="true"># 使用特定类型构造函数</span>i <span class="token operator">=</span> torch<span class="token punctuation">.</span>IntTensor<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token keyword">print</span><span class="token punctuation">(</span>i<span class="token punctuation">,</span>i<span class="token punctuation">.</span>dtype<span class="token punctuation">)</span><span class="token comment" spellcheck="true"># 等价于torch.FloatTensor</span>x <span class="token operator">=</span> torch<span class="token punctuation">.</span>Tensor<span class="token punctuation">(</span>np<span class="token punctuation">.</span>array<span class="token punctuation">(</span><span class="token number">2.0</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token keyword">print</span><span class="token punctuation">(</span>x<span class="token punctuation">,</span>x<span class="token punctuation">.</span>dtype<span class="token punctuation">)</span> b <span class="token operator">=</span> torch<span class="token punctuation">.</span>BoolTensor<span class="token punctuation">(</span>np<span class="token punctuation">.</span>array<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">0</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">;</span> <span class="token keyword">print</span><span class="token punctuation">(</span>b<span class="token punctuation">,</span>b<span class="token punctuation">.</span>dtype<span class="token punctuation">)</span><span class="token operator">>></span><span class="token operator">></span>Output<span class="token punctuation">:</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">5</span><span class="token punctuation">]</span><span class="token punctuation">,</span> dtype<span class="token operator">=</span>torch<span class="token punctuation">.</span>int32<span class="token punctuation">)</span> torch<span class="token punctuation">.</span>int32tensor<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">.</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float32tensor<span class="token punctuation">(</span><span class="token punctuation">[</span> <span class="token boolean">True</span><span class="token punctuation">,</span> <span class="token boolean">False</span><span class="token punctuation">,</span>  <span class="token boolean">True</span><span class="token punctuation">,</span> <span class="token boolean">False</span><span class="token punctuation">]</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>bool<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><pre class="line-numbers language-python"><code class="language-python"><span class="token comment" spellcheck="true"># 不同类型进行转换</span>i <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">;</span> <span class="token keyword">print</span><span class="token punctuation">(</span>i<span class="token punctuation">,</span>i<span class="token punctuation">.</span>dtype<span class="token punctuation">)</span><span class="token comment" spellcheck="true"># 调用 float方法转换成浮点类型</span>x <span class="token operator">=</span> i<span class="token punctuation">.</span>float<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">;</span> <span class="token keyword">print</span><span class="token punctuation">(</span>x<span class="token punctuation">,</span>x<span class="token punctuation">.</span>dtype<span class="token punctuation">)</span> <span class="token comment" spellcheck="true"># 使用type函数转换成浮点类型</span>y <span class="token operator">=</span> i<span class="token punctuation">.</span>type<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>float<span class="token punctuation">)</span><span class="token punctuation">;</span> <span class="token keyword">print</span><span class="token punctuation">(</span>y<span class="token punctuation">,</span>y<span class="token punctuation">.</span>dtype<span class="token punctuation">)</span> <span class="token comment" spellcheck="true"># 使用type_as方法转换成某个Tensor相同类型</span>z <span class="token operator">=</span> i<span class="token punctuation">.</span>type_as<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">;</span><span class="token keyword">print</span><span class="token punctuation">(</span>z<span class="token punctuation">,</span>z<span class="token punctuation">.</span>dtype<span class="token punctuation">)</span> <span class="token operator">>></span><span class="token operator">></span>Output<span class="token punctuation">:</span>tensor<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>int64tensor<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">.</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float32tensor<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">.</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float32tensor<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">.</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>float32<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="张量的维度"><a href="#张量的维度" class="headerlink" title="张量的维度"></a>张量的维度</h2><p>不同类型的数据可以用不同维度(dimension)的张量来表示。标量为0维张量，向量为1维张量，矩阵为2维张量。彩色图像有rgb三个通道，可以表示为3维张量。视频还有时间维，可以表示为4维张量。<br>可以简单地总结为：有几层中括号，就是多少维的张量。</p><pre class="line-numbers language-python"><code class="language-python">scalar <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>scalar<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>scalar<span class="token punctuation">.</span>dim<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>  <span class="token comment" spellcheck="true"># 标量，0维张量</span><span class="token operator">>></span><span class="token operator">></span>Output<span class="token punctuation">:</span>tensor<span class="token punctuation">(</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token number">0</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><pre class="line-numbers language-python"><code class="language-python">vector <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">1.0</span><span class="token punctuation">,</span><span class="token number">2.0</span><span class="token punctuation">,</span><span class="token number">3.0</span><span class="token punctuation">,</span><span class="token number">4.0</span><span class="token punctuation">]</span><span class="token punctuation">)</span> <span class="token comment" spellcheck="true">#向量，1维张量</span><span class="token keyword">print</span><span class="token punctuation">(</span>vector<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>vector<span class="token punctuation">.</span>dim<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token operator">>></span><span class="token operator">></span>Output<span class="token punctuation">:</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">.</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token number">1</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><pre class="line-numbers language-python"><code class="language-python">matrix <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">1.0</span><span class="token punctuation">,</span><span class="token number">2.0</span><span class="token punctuation">]</span><span class="token punctuation">,</span><span class="token punctuation">[</span><span class="token number">3.0</span><span class="token punctuation">,</span><span class="token number">4.0</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span> <span class="token comment" spellcheck="true">#矩阵, 2维张量</span><span class="token keyword">print</span><span class="token punctuation">(</span>matrix<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>matrix<span class="token punctuation">.</span>dim<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token operator">>></span><span class="token operator">></span>Output<span class="token punctuation">:</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">.</span><span class="token punctuation">]</span><span class="token punctuation">,</span>        <span class="token punctuation">[</span><span class="token number">3</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">.</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token number">2</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><pre class="line-numbers language-python"><code class="language-python">tensor3 <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">1.0</span><span class="token punctuation">,</span><span class="token number">2.0</span><span class="token punctuation">]</span><span class="token punctuation">,</span><span class="token punctuation">[</span><span class="token number">3.0</span><span class="token punctuation">,</span><span class="token number">4.0</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">,</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">5.0</span><span class="token punctuation">,</span><span class="token number">6.0</span><span class="token punctuation">]</span><span class="token punctuation">,</span><span class="token punctuation">[</span><span class="token number">7.0</span><span class="token punctuation">,</span><span class="token number">8.0</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span>  <span class="token comment" spellcheck="true"># 3维张量</span><span class="token keyword">print</span><span class="token punctuation">(</span>tensor3<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>tensor3<span class="token punctuation">.</span>dim<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token operator">>></span><span class="token operator">></span>Output<span class="token punctuation">:</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">.</span><span class="token punctuation">]</span><span class="token punctuation">,</span>         <span class="token punctuation">[</span><span class="token number">3</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">.</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">,</span>        <span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">5</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">6</span><span class="token punctuation">.</span><span class="token punctuation">]</span><span class="token punctuation">,</span>         <span class="token punctuation">[</span><span class="token number">7</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">8</span><span class="token punctuation">.</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token number">3</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><pre class="line-numbers language-python"><code class="language-python">tensor4 <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">1.0</span><span class="token punctuation">,</span><span class="token number">1.0</span><span class="token punctuation">]</span><span class="token punctuation">,</span><span class="token punctuation">[</span><span class="token number">2.0</span><span class="token punctuation">,</span><span class="token number">2.0</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">,</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">3.0</span><span class="token punctuation">,</span><span class="token number">3.0</span><span class="token punctuation">]</span><span class="token punctuation">,</span><span class="token punctuation">[</span><span class="token number">4.0</span><span class="token punctuation">,</span><span class="token number">4.0</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">,</span>                        <span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">5.0</span><span class="token punctuation">,</span><span class="token number">5.0</span><span class="token punctuation">]</span><span class="token punctuation">,</span><span class="token punctuation">[</span><span class="token number">6.0</span><span class="token punctuation">,</span><span class="token number">6.0</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">,</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">7.0</span><span class="token punctuation">,</span><span class="token number">7.0</span><span class="token punctuation">]</span><span class="token punctuation">,</span><span class="token punctuation">[</span><span class="token number">8.0</span><span class="token punctuation">,</span><span class="token number">8.0</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span>  <span class="token comment" spellcheck="true"># 4维张量</span><span class="token keyword">print</span><span class="token punctuation">(</span>tensor4<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>tensor4<span class="token punctuation">.</span>dim<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token operator">>></span><span class="token operator">></span>Output<span class="token punctuation">:</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">.</span><span class="token punctuation">]</span><span class="token punctuation">,</span>          <span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">.</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">,</span>         <span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">3</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">.</span><span class="token punctuation">]</span><span class="token punctuation">,</span>          <span class="token punctuation">[</span><span class="token number">4</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">.</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">,</span>        <span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">5</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">.</span><span class="token punctuation">]</span><span class="token punctuation">,</span>          <span class="token punctuation">[</span><span class="token number">6</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">6</span><span class="token punctuation">.</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">,</span>         <span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">7</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">7</span><span class="token punctuation">.</span><span class="token punctuation">]</span><span class="token punctuation">,</span>          <span class="token punctuation">[</span><span class="token number">8</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">8</span><span class="token punctuation">.</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token number">4</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="张量的尺寸"><a href="#张量的尺寸" class="headerlink" title="张量的尺寸"></a>张量的尺寸</h2><p>可以使用<code>**shape**</code><strong>属性</strong>或者<code>**size()**</code><strong>方法</strong>查看张量在每一维的长度。<br>可以使用<code>view()</code>方法改变张量的尺寸。如果<code>view()</code>方法改变尺寸失败，可以使用<code>reshape()</code>方法。</p><pre class="line-numbers language-python"><code class="language-python">scalar <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>scalar<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>scalar<span class="token punctuation">.</span>shape<span class="token punctuation">)</span><span class="token operator">>></span><span class="token operator">></span>Output<span class="token punctuation">:</span>torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">]</span><span class="token punctuation">)</span>torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><pre class="line-numbers language-python"><code class="language-python">vector <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">1.0</span><span class="token punctuation">,</span> <span class="token number">2.0</span><span class="token punctuation">,</span> <span class="token number">3.0</span><span class="token punctuation">,</span> <span class="token number">4.0</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>vector<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>vector<span class="token punctuation">.</span>shape<span class="token punctuation">)</span><span class="token operator">>></span><span class="token operator">></span>Output<span class="token punctuation">:</span>torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">4</span><span class="token punctuation">]</span><span class="token punctuation">)</span>torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">4</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><pre class="line-numbers language-python"><code class="language-python">matrix <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">1.0</span><span class="token punctuation">,</span><span class="token number">2.0</span><span class="token punctuation">]</span><span class="token punctuation">,</span><span class="token punctuation">[</span><span class="token number">3.0</span><span class="token punctuation">,</span><span class="token number">4.0</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>matrix<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>matrix<span class="token punctuation">.</span>shape<span class="token punctuation">)</span><span class="token operator">>></span><span class="token operator">></span>Output<span class="token punctuation">:</span>torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">)</span>torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>使用view可以改变张量尺寸</p><pre class="line-numbers language-python"><code class="language-python">vector <span class="token operator">=</span> torch<span class="token punctuation">.</span>arange<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span><span class="token number">12</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>vector<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>vector<span class="token punctuation">.</span>shape<span class="token punctuation">)</span>matrix34 <span class="token operator">=</span> vector<span class="token punctuation">.</span>view<span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span><span class="token number">4</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>matrix34<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>matrix34<span class="token punctuation">.</span>shape<span class="token punctuation">)</span>matrix43 <span class="token operator">=</span> vector<span class="token punctuation">.</span>view<span class="token punctuation">(</span><span class="token number">4</span><span class="token punctuation">,</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span> <span class="token comment" spellcheck="true">#-1表示该位置长度由程序自动推断</span><span class="token keyword">print</span><span class="token punctuation">(</span>matrix43<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>matrix43<span class="token punctuation">.</span>shape<span class="token punctuation">)</span><span class="token operator">>></span><span class="token operator">></span>Output<span class="token punctuation">:</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span> <span class="token number">0</span><span class="token punctuation">,</span>  <span class="token number">1</span><span class="token punctuation">,</span>  <span class="token number">2</span><span class="token punctuation">,</span>  <span class="token number">3</span><span class="token punctuation">,</span>  <span class="token number">4</span><span class="token punctuation">,</span>  <span class="token number">5</span><span class="token punctuation">,</span>  <span class="token number">6</span><span class="token punctuation">,</span>  <span class="token number">7</span><span class="token punctuation">,</span>  <span class="token number">8</span><span class="token punctuation">,</span>  <span class="token number">9</span><span class="token punctuation">,</span> <span class="token number">10</span><span class="token punctuation">,</span> <span class="token number">11</span><span class="token punctuation">]</span><span class="token punctuation">)</span>torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">12</span><span class="token punctuation">]</span><span class="token punctuation">)</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span> <span class="token number">0</span><span class="token punctuation">,</span>  <span class="token number">1</span><span class="token punctuation">,</span>  <span class="token number">2</span><span class="token punctuation">,</span>  <span class="token number">3</span><span class="token punctuation">]</span><span class="token punctuation">,</span>        <span class="token punctuation">[</span> <span class="token number">4</span><span class="token punctuation">,</span>  <span class="token number">5</span><span class="token punctuation">,</span>  <span class="token number">6</span><span class="token punctuation">,</span>  <span class="token number">7</span><span class="token punctuation">]</span><span class="token punctuation">,</span>        <span class="token punctuation">[</span> <span class="token number">8</span><span class="token punctuation">,</span>  <span class="token number">9</span><span class="token punctuation">,</span> <span class="token number">10</span><span class="token punctuation">,</span> <span class="token number">11</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span>torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">]</span><span class="token punctuation">)</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span> <span class="token number">0</span><span class="token punctuation">,</span>  <span class="token number">1</span><span class="token punctuation">,</span>  <span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">,</span>        <span class="token punctuation">[</span> <span class="token number">3</span><span class="token punctuation">,</span>  <span class="token number">4</span><span class="token punctuation">,</span>  <span class="token number">5</span><span class="token punctuation">]</span><span class="token punctuation">,</span>        <span class="token punctuation">[</span> <span class="token number">6</span><span class="token punctuation">,</span>  <span class="token number">7</span><span class="token punctuation">,</span>  <span class="token number">8</span><span class="token punctuation">]</span><span class="token punctuation">,</span>        <span class="token punctuation">[</span> <span class="token number">9</span><span class="token punctuation">,</span> <span class="token number">10</span><span class="token punctuation">,</span> <span class="token number">11</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span>torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>有些操作会让张量存储结构扭曲，直接使用view会失败，可以用reshape方法。</p><pre class="line-numbers language-python"><code class="language-python">matrix26 <span class="token operator">=</span> torch<span class="token punctuation">.</span>arange<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span><span class="token number">12</span><span class="token punctuation">)</span><span class="token punctuation">.</span>view<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">6</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>matrix26<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>matrix26<span class="token punctuation">.</span>shape<span class="token punctuation">)</span><span class="token comment" spellcheck="true"># 转置操作让张量存储结构扭曲</span>matrix62 <span class="token operator">=</span> matrix26<span class="token punctuation">.</span>t<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>matrix62<span class="token punctuation">.</span>is_contiguous<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true"># 直接使用view方法会失败，可以使用reshape方法</span><span class="token comment" spellcheck="true"># matrix34 = matrix62.view(3,4) # error!</span><span class="token comment" spellcheck="true"># 等价于matrix34 = matrix62.contiguous().view(3,4)。</span><span class="token comment" spellcheck="true"># contiguous()函数，把tensor变成在内存中连续分布的形式。</span>matrix34 <span class="token operator">=</span> matrix62<span class="token punctuation">.</span>reshape<span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span><span class="token number">4</span><span class="token punctuation">)</span> <span class="token keyword">print</span><span class="token punctuation">(</span>matrix34<span class="token punctuation">)</span><span class="token operator">>></span><span class="token operator">></span>Output<span class="token punctuation">:</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span> <span class="token number">0</span><span class="token punctuation">,</span>  <span class="token number">1</span><span class="token punctuation">,</span>  <span class="token number">2</span><span class="token punctuation">,</span>  <span class="token number">3</span><span class="token punctuation">,</span>  <span class="token number">4</span><span class="token punctuation">,</span>  <span class="token number">5</span><span class="token punctuation">]</span><span class="token punctuation">,</span>        <span class="token punctuation">[</span> <span class="token number">6</span><span class="token punctuation">,</span>  <span class="token number">7</span><span class="token punctuation">,</span>  <span class="token number">8</span><span class="token punctuation">,</span>  <span class="token number">9</span><span class="token punctuation">,</span> <span class="token number">10</span><span class="token punctuation">,</span> <span class="token number">11</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span>torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">6</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token boolean">False</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span> <span class="token number">0</span><span class="token punctuation">,</span>  <span class="token number">6</span><span class="token punctuation">,</span>  <span class="token number">1</span><span class="token punctuation">,</span>  <span class="token number">7</span><span class="token punctuation">]</span><span class="token punctuation">,</span>        <span class="token punctuation">[</span> <span class="token number">2</span><span class="token punctuation">,</span>  <span class="token number">8</span><span class="token punctuation">,</span>  <span class="token number">3</span><span class="token punctuation">,</span>  <span class="token number">9</span><span class="token punctuation">]</span><span class="token punctuation">,</span>        <span class="token punctuation">[</span> <span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">10</span><span class="token punctuation">,</span>  <span class="token number">5</span><span class="token punctuation">,</span> <span class="token number">11</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="张量和numpy数组"><a href="#张量和numpy数组" class="headerlink" title="张量和numpy数组"></a>张量和numpy数组</h2><p>可以用<code>numpy()</code>方法从Tensor得到numpy数组，也可以用<code>torch.from_numpy()</code>从numpy数组得到Tensor。这两种方法关联的Tensor和numpy数组是<strong>共享数据内存</strong>的。如果改变其中一个，另外一个的值也会发生改变。<br>如果有需要，可以用张量的<code>clone()</code>方法拷贝张量，中断这种关联。<br>此外，还可以使用<code>item()</code>方法从标量张量得到对应的Python数值。使用<code>tolist()</code>方法从张量得到对应的Python数值列表。</p><pre class="line-numbers language-python"><code class="language-python"><span class="token comment" spellcheck="true">#torch.from_numpy函数从numpy数组得到Tensor</span>arr <span class="token operator">=</span> np<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">)</span>tensor <span class="token operator">=</span> torch<span class="token punctuation">.</span>from_numpy<span class="token punctuation">(</span>arr<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"before add 1:"</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>arr<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>tensor<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"\nafter add 1:"</span><span class="token punctuation">)</span>np<span class="token punctuation">.</span>add<span class="token punctuation">(</span>arr<span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">,</span> out <span class="token operator">=</span> arr<span class="token punctuation">)</span> <span class="token comment" spellcheck="true"># 给 arr增加1，tensor也随之改变</span><span class="token keyword">print</span><span class="token punctuation">(</span>arr<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>tensor<span class="token punctuation">)</span><span class="token operator">>></span><span class="token operator">></span>Output<span class="token punctuation">:</span>before add <span class="token number">1</span><span class="token punctuation">:</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">.</span> <span class="token number">0</span><span class="token punctuation">.</span> <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">]</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">]</span><span class="token punctuation">,</span> dtype<span class="token operator">=</span>torch<span class="token punctuation">.</span>float64<span class="token punctuation">)</span>after add <span class="token number">1</span><span class="token punctuation">:</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">.</span> <span class="token number">1</span><span class="token punctuation">.</span> <span class="token number">1</span><span class="token punctuation">.</span><span class="token punctuation">]</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">.</span><span class="token punctuation">]</span><span class="token punctuation">,</span> dtype<span class="token operator">=</span>torch<span class="token punctuation">.</span>float64<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><pre class="line-numbers language-python"><code class="language-python"><span class="token comment" spellcheck="true"># numpy方法从Tensor得到numpy数组</span>tensor <span class="token operator">=</span> torch<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">)</span>arr <span class="token operator">=</span> tensor<span class="token punctuation">.</span>numpy<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"before add 1:"</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>tensor<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>arr<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"\nafter add 1:"</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true"># 使用带下划线的方法表示计算结果会返回给调用 张量</span>tensor<span class="token punctuation">.</span>add_<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span> <span class="token comment" spellcheck="true">#给 tensor增加1，arr也随之改变 </span><span class="token comment" spellcheck="true"># 或： torch.add(tensor,1,out = tensor)</span><span class="token keyword">print</span><span class="token punctuation">(</span>tensor<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>arr<span class="token punctuation">)</span><span class="token operator">>></span><span class="token operator">></span>Output<span class="token punctuation">:</span>before add <span class="token number">1</span><span class="token punctuation">:</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">.</span> <span class="token number">0</span><span class="token punctuation">.</span> <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">]</span>after add <span class="token number">1</span><span class="token punctuation">:</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">.</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">.</span> <span class="token number">1</span><span class="token punctuation">.</span> <span class="token number">1</span><span class="token punctuation">.</span><span class="token punctuation">]</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><pre class="line-numbers language-python"><code class="language-python"><span class="token comment" spellcheck="true"># 可以用clone() 方法拷贝张量，中断这种关联</span>tensor <span class="token operator">=</span> torch<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true"># 使用clone方法拷贝张量, 拷贝后的张量和原始张量内存独立</span>arr <span class="token operator">=</span> tensor<span class="token punctuation">.</span>clone<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>numpy<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token comment" spellcheck="true"># 也可以使用tensor.data.numpy()</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"before add 1:"</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>tensor<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>arr<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"\nafter add 1:"</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true">#使用 带下划线的方法表示计算结果会返回给调用 张量</span>tensor<span class="token punctuation">.</span>add_<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span> <span class="token comment" spellcheck="true">#给 tensor增加1，arr不再随之改变</span><span class="token keyword">print</span><span class="token punctuation">(</span>tensor<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>arr<span class="token punctuation">)</span><span class="token operator">>></span><span class="token operator">></span>Output<span class="token punctuation">:</span>before add <span class="token number">1</span><span class="token punctuation">:</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">.</span> <span class="token number">0</span><span class="token punctuation">.</span> <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">]</span>after add <span class="token number">1</span><span class="token punctuation">:</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">.</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">.</span> <span class="token number">0</span><span class="token punctuation">.</span> <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">]</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><pre class="line-numbers language-python"><code class="language-python"><span class="token comment" spellcheck="true"># item方法和tolist方法可以将张量转换成Python数值和数值列表</span>scalar <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token number">1.0</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>scalar<span class="token punctuation">)</span>s <span class="token operator">=</span> scalar<span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>s<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>type<span class="token punctuation">(</span>s<span class="token punctuation">)</span><span class="token punctuation">)</span>tensor <span class="token operator">=</span> torch<span class="token punctuation">.</span>rand<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">)</span>t <span class="token operator">=</span> tensor<span class="token punctuation">.</span>tolist<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>t<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>type<span class="token punctuation">(</span>t<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token operator">>></span><span class="token operator">></span>Output<span class="token punctuation">:</span>tensor<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">.</span><span class="token punctuation">)</span><span class="token number">1.0</span><span class="token operator">&lt;</span><span class="token keyword">class</span> <span class="token string">'float'</span><span class="token operator">></span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">0.8211846351623535</span><span class="token punctuation">,</span> <span class="token number">0.20020723342895508</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token number">0.011571824550628662</span><span class="token punctuation">,</span> <span class="token number">0.2906131148338318</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token operator">&lt;</span><span class="token keyword">class</span> <span class="token string">'list'</span><span class="token operator">></span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h1 id="张量的操作"><a href="#张量的操作" class="headerlink" title="张量的操作"></a>张量的操作</h1><p>张量的操作主要包括张量的结构操作和张量的数学运算。</p><ul><li>张量结构操作诸如：张量创建，索引切片，维度变换，合并分割。</li><li>张量数学运算主要有：标量运算，向量运算，矩阵运算，张量运算的广播机制。<h2 id="张量的结构操作"><a href="#张量的结构操作" class="headerlink" title="张量的结构操作"></a>张量的结构操作</h2><h3 id="张量创建"><a href="#张量创建" class="headerlink" title="张量创建"></a>张量创建</h3>张量创建的许多方法和numpy中创建array的方法很像。也可参考【创建张量】<pre class="line-numbers language-python"><code class="language-python">a <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">3</span><span class="token punctuation">]</span><span class="token punctuation">,</span>dtype <span class="token operator">=</span> torch<span class="token punctuation">.</span>float<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>a<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre></li></ul><p>b = torch.arange(1,10,step = 2)<br>print(b)</p><p>c = torch.linspace(0.0,2*3.14,10)<br>print(c)</p><p>d = torch.zeros((3,3))<br>print(d)</p><blockquote><blockquote><blockquote><p>Output:<br>tensor([1., 2., 3.])<br>tensor([1, 3, 5, 7, 9])<br>tensor([0.0000, 0.6978, 1.3956, 2.0933, 2.7911, 3.4889, 4.1867, 4.8844, 5.5822,<br>        6.2800])<br>tensor([[0., 0., 0.],<br>        [0., 0., 0.],<br>        [0., 0., 0.]])</p><pre><code>```pythona = torch.ones((3,3), dtype = torch.int)b = torch.zeros_like(a, dtype = torch.float)print(a)print(b)</code></pre></blockquote></blockquote></blockquote><h1 id="等价于-b-fill-5"><a href="#等价于-b-fill-5" class="headerlink" title="等价于 b.fill_(5)"></a>等价于 b.fill_(5)</h1><p>torch.fill_(b,5)<br>print(b)</p><blockquote><blockquote><blockquote><p>Output:<br>tensor([[1, 1, 1],<br>        [1, 1, 1],<br>        [1, 1, 1]], dtype=torch.int32)<br>tensor([[0., 0., 0.],<br>        [0., 0., 0.],<br>        [0., 0., 0.]])<br>tensor([[5., 5., 5.],<br>        [5., 5., 5.],<br>        [5., 5., 5.]])</p><pre><code>```python</code></pre></blockquote></blockquote></blockquote><h1 id="均匀随机分布"><a href="#均匀随机分布" class="headerlink" title="均匀随机分布"></a>均匀随机分布</h1><p>minval,maxval = 0,10<br>a = minval + (maxval-minval)*torch.rand([5])<br>print(a)</p><h1 id="正态分布随机"><a href="#正态分布随机" class="headerlink" title="正态分布随机"></a>正态分布随机</h1><p>b = torch.normal(mean = torch.zeros(3,3), std = torch.ones(3,3))<br>print(b)</p><h1 id="正态分布随机-1"><a href="#正态分布随机-1" class="headerlink" title="正态分布随机"></a>正态分布随机</h1><p>mean,std = 2,5<br>c = std*torch.randn((3,3))+mean<br>print(c)</p><h1 id="整数随机排列"><a href="#整数随机排列" class="headerlink" title="整数随机排列"></a>整数随机排列</h1><p>d = torch.randperm(20)<br>print(d)</p><blockquote><blockquote><blockquote><p>Output:<br>tensor([4.9626, 7.6822, 0.8848, 1.3203, 3.0742])<br>tensor([[ 0.5507,  0.2704,  0.6472],<br>        [ 0.2490, -0.3354,  0.4564],<br>        [-0.6255,  0.4539, -1.3740]])<br>tensor([[16.2371, -1.6612,  3.9163],<br>        [ 7.4999,  1.5616,  4.0768],<br>        [ 5.2128, -8.9407,  6.4601]])<br>tensor([ 3, 17,  9, 19,  1, 18,  4, 13, 15, 12,  0, 16,  7, 11,  2,  5,  8, 10,<br>         6, 14])</p><pre><code>```pythonI = torch.eye(3,3) #单位矩阵print(I)t = torch.diag(torch.tensor([1,2,3])) #对角矩阵print(t)</code></pre></blockquote></blockquote></blockquote><blockquote><blockquote><blockquote><p>Output:<br>tensor([[1., 0., 0.],<br>        [0., 1., 0.],<br>        [0., 0., 1.]])<br>tensor([[1, 0, 0],<br>        [0, 2, 0],<br>        [0, 0, 3]])</p><pre><code></code></pre></blockquote></blockquote></blockquote><h3 id="索引切片"><a href="#索引切片" class="headerlink" title="索引切片"></a>索引切片</h3><p>张量的索引切片方式和numpy几乎是一样的。切片时支持缺省参数和省略号。可以通过索引和切片对部分元素进行修改。<br>此外，对于不规则的切片提取，可以使用<code>torch.index_select</code>，<code>torch.masked_select</code>，<code>torch.take</code>。<br>如果要通过修改张量的某些元素得到新的张量，可以使用<code>torch.where</code>，<code>torch.masked_fill</code>，<code>torch.index_fill</code>。</p><pre class="line-numbers language-python"><code class="language-python"><span class="token comment" spellcheck="true"># 均匀随机分布</span>torch<span class="token punctuation">.</span>manual_seed<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span>minval<span class="token punctuation">,</span>maxval <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">,</span><span class="token number">10</span>t <span class="token operator">=</span> torch<span class="token punctuation">.</span>floor<span class="token punctuation">(</span>minval <span class="token operator">+</span> <span class="token punctuation">(</span>maxval<span class="token operator">-</span>minval<span class="token punctuation">)</span><span class="token operator">*</span>torch<span class="token punctuation">.</span>rand<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">5</span><span class="token punctuation">,</span><span class="token number">5</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">.</span>int<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>t<span class="token punctuation">)</span><span class="token comment" spellcheck="true">#第1行</span><span class="token keyword">print</span><span class="token punctuation">(</span>t<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true">#倒数第一行</span><span class="token keyword">print</span><span class="token punctuation">(</span>t<span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true">#第2行第4列</span><span class="token keyword">print</span><span class="token punctuation">(</span>t<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">3</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>t<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token number">3</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true">#第1行至第3行</span><span class="token keyword">print</span><span class="token punctuation">(</span>t<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">:</span><span class="token number">4</span><span class="token punctuation">,</span><span class="token punctuation">:</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true">#第1行至最后一行，第0列到最后一列每隔两列取一列</span><span class="token keyword">print</span><span class="token punctuation">(</span>t<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">:</span><span class="token number">4</span><span class="token punctuation">,</span><span class="token punctuation">:</span><span class="token number">4</span><span class="token punctuation">:</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token operator">>></span><span class="token operator">></span>Output<span class="token punctuation">:</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">7</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">]</span><span class="token punctuation">,</span>        <span class="token punctuation">[</span><span class="token number">6</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">8</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">6</span><span class="token punctuation">]</span><span class="token punctuation">,</span>        <span class="token punctuation">[</span><span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">,</span>        <span class="token punctuation">[</span><span class="token number">5</span><span class="token punctuation">,</span> <span class="token number">6</span><span class="token punctuation">,</span> <span class="token number">8</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">,</span>        <span class="token punctuation">[</span><span class="token number">6</span><span class="token punctuation">,</span> <span class="token number">9</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">8</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">,</span> dtype<span class="token operator">=</span>torch<span class="token punctuation">.</span>int32<span class="token punctuation">)</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">7</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">]</span><span class="token punctuation">,</span> dtype<span class="token operator">=</span>torch<span class="token punctuation">.</span>int32<span class="token punctuation">)</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">6</span><span class="token punctuation">,</span> <span class="token number">9</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">8</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">]</span><span class="token punctuation">,</span> dtype<span class="token operator">=</span>torch<span class="token punctuation">.</span>int32<span class="token punctuation">)</span>tensor<span class="token punctuation">(</span><span class="token number">4</span><span class="token punctuation">,</span> dtype<span class="token operator">=</span>torch<span class="token punctuation">.</span>int32<span class="token punctuation">)</span>tensor<span class="token punctuation">(</span><span class="token number">4</span><span class="token punctuation">,</span> dtype<span class="token operator">=</span>torch<span class="token punctuation">.</span>int32<span class="token punctuation">)</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">6</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">8</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">6</span><span class="token punctuation">]</span><span class="token punctuation">,</span>        <span class="token punctuation">[</span><span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">,</span>        <span class="token punctuation">[</span><span class="token number">5</span><span class="token punctuation">,</span> <span class="token number">6</span><span class="token punctuation">,</span> <span class="token number">8</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">,</span> dtype<span class="token operator">=</span>torch<span class="token punctuation">.</span>int32<span class="token punctuation">)</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">6</span><span class="token punctuation">,</span> <span class="token number">8</span><span class="token punctuation">]</span><span class="token punctuation">,</span>        <span class="token punctuation">[</span><span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span>        <span class="token punctuation">[</span><span class="token number">5</span><span class="token punctuation">,</span> <span class="token number">8</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">,</span> dtype<span class="token operator">=</span>torch<span class="token punctuation">.</span>int32<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><pre class="line-numbers language-python"><code class="language-python"><span class="token comment" spellcheck="true">#可以使用索引和切片修改部分元素</span>x <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">,</span><span class="token punctuation">[</span><span class="token number">3</span><span class="token punctuation">,</span><span class="token number">4</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">,</span>dtype <span class="token operator">=</span> torch<span class="token punctuation">.</span>float32<span class="token punctuation">,</span>requires_grad<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>x<span class="token punctuation">.</span>data<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token punctuation">:</span><span class="token punctuation">]</span> <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">0.0</span><span class="token punctuation">,</span><span class="token number">0.0</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token operator">>></span><span class="token operator">></span>Output<span class="token punctuation">:</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">.</span><span class="token punctuation">]</span><span class="token punctuation">,</span>        <span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">.</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">,</span> requires_grad<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><pre class="line-numbers language-python"><code class="language-python">a <span class="token operator">=</span> torch<span class="token punctuation">.</span>arange<span class="token punctuation">(</span><span class="token number">27</span><span class="token punctuation">)</span><span class="token punctuation">.</span>view<span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span><span class="token number">3</span><span class="token punctuation">,</span><span class="token number">3</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>a<span class="token punctuation">)</span><span class="token comment" spellcheck="true">#省略号可以表示多个冒号</span><span class="token keyword">print</span><span class="token punctuation">(</span>a<span class="token punctuation">[</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token operator">>></span><span class="token operator">></span>Output<span class="token punctuation">:</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token punctuation">[</span> <span class="token number">0</span><span class="token punctuation">,</span>  <span class="token number">1</span><span class="token punctuation">,</span>  <span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">,</span>         <span class="token punctuation">[</span> <span class="token number">3</span><span class="token punctuation">,</span>  <span class="token number">4</span><span class="token punctuation">,</span>  <span class="token number">5</span><span class="token punctuation">]</span><span class="token punctuation">,</span>         <span class="token punctuation">[</span> <span class="token number">6</span><span class="token punctuation">,</span>  <span class="token number">7</span><span class="token punctuation">,</span>  <span class="token number">8</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">,</span>        <span class="token punctuation">[</span><span class="token punctuation">[</span> <span class="token number">9</span><span class="token punctuation">,</span> <span class="token number">10</span><span class="token punctuation">,</span> <span class="token number">11</span><span class="token punctuation">]</span><span class="token punctuation">,</span>         <span class="token punctuation">[</span><span class="token number">12</span><span class="token punctuation">,</span> <span class="token number">13</span><span class="token punctuation">,</span> <span class="token number">14</span><span class="token punctuation">]</span><span class="token punctuation">,</span>         <span class="token punctuation">[</span><span class="token number">15</span><span class="token punctuation">,</span> <span class="token number">16</span><span class="token punctuation">,</span> <span class="token number">17</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">,</span>        <span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">18</span><span class="token punctuation">,</span> <span class="token number">19</span><span class="token punctuation">,</span> <span class="token number">20</span><span class="token punctuation">]</span><span class="token punctuation">,</span>         <span class="token punctuation">[</span><span class="token number">21</span><span class="token punctuation">,</span> <span class="token number">22</span><span class="token punctuation">,</span> <span class="token number">23</span><span class="token punctuation">]</span><span class="token punctuation">,</span>         <span class="token punctuation">[</span><span class="token number">24</span><span class="token punctuation">,</span> <span class="token number">25</span><span class="token punctuation">,</span> <span class="token number">26</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span> <span class="token number">1</span><span class="token punctuation">,</span>  <span class="token number">4</span><span class="token punctuation">,</span>  <span class="token number">7</span><span class="token punctuation">]</span><span class="token punctuation">,</span>        <span class="token punctuation">[</span><span class="token number">10</span><span class="token punctuation">,</span> <span class="token number">13</span><span class="token punctuation">,</span> <span class="token number">16</span><span class="token punctuation">]</span><span class="token punctuation">,</span>        <span class="token punctuation">[</span><span class="token number">19</span><span class="token punctuation">,</span> <span class="token number">22</span><span class="token punctuation">,</span> <span class="token number">25</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>以上切片方式相对规则，对于不规则的切片提取，可以使用<code>torch.index_select</code>，<code>torch.masked_select</code>，<code>torch.take</code>，<code>torch.gather</code>。<br>以班级成绩册为例子，有4个班级，每个班级10个学生，每个学生7门科目成绩。可以用一个4×10×7的张量来表示。</p><pre class="line-numbers language-python"><code class="language-python">minval<span class="token operator">=</span><span class="token number">0</span>maxval<span class="token operator">=</span><span class="token number">100</span>scores <span class="token operator">=</span> torch<span class="token punctuation">.</span>floor<span class="token punctuation">(</span>minval <span class="token operator">+</span> <span class="token punctuation">(</span>maxval<span class="token operator">-</span>minval<span class="token punctuation">)</span><span class="token operator">*</span>torch<span class="token punctuation">.</span>rand<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">4</span><span class="token punctuation">,</span><span class="token number">10</span><span class="token punctuation">,</span><span class="token number">7</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">.</span>int<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>scores<span class="token punctuation">)</span><span class="token operator">>></span><span class="token operator">></span>Output<span class="token punctuation">:</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">55</span><span class="token punctuation">,</span> <span class="token number">95</span><span class="token punctuation">,</span>  <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">18</span><span class="token punctuation">,</span> <span class="token number">37</span><span class="token punctuation">,</span> <span class="token number">30</span><span class="token punctuation">,</span> <span class="token number">93</span><span class="token punctuation">]</span><span class="token punctuation">,</span>         <span class="token punctuation">[</span><span class="token number">17</span><span class="token punctuation">,</span> <span class="token number">26</span><span class="token punctuation">,</span> <span class="token number">15</span><span class="token punctuation">,</span>  <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">20</span><span class="token punctuation">,</span> <span class="token number">92</span><span class="token punctuation">,</span> <span class="token number">72</span><span class="token punctuation">]</span><span class="token punctuation">,</span>         <span class="token punctuation">[</span><span class="token number">74</span><span class="token punctuation">,</span> <span class="token number">52</span><span class="token punctuation">,</span> <span class="token number">24</span><span class="token punctuation">,</span> <span class="token number">58</span><span class="token punctuation">,</span>  <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">13</span><span class="token punctuation">,</span> <span class="token number">24</span><span class="token punctuation">]</span><span class="token punctuation">,</span>         <span class="token punctuation">[</span><span class="token number">81</span><span class="token punctuation">,</span> <span class="token number">79</span><span class="token punctuation">,</span> <span class="token number">27</span><span class="token punctuation">,</span> <span class="token number">48</span><span class="token punctuation">,</span> <span class="token number">81</span><span class="token punctuation">,</span> <span class="token number">99</span><span class="token punctuation">,</span> <span class="token number">69</span><span class="token punctuation">]</span><span class="token punctuation">,</span>         <span class="token punctuation">[</span><span class="token number">56</span><span class="token punctuation">,</span> <span class="token number">83</span><span class="token punctuation">,</span> <span class="token number">20</span><span class="token punctuation">,</span> <span class="token number">59</span><span class="token punctuation">,</span> <span class="token number">11</span><span class="token punctuation">,</span> <span class="token number">15</span><span class="token punctuation">,</span> <span class="token number">24</span><span class="token punctuation">]</span><span class="token punctuation">,</span>         <span class="token punctuation">[</span><span class="token number">72</span><span class="token punctuation">,</span> <span class="token number">70</span><span class="token punctuation">,</span> <span class="token number">20</span><span class="token punctuation">,</span> <span class="token number">65</span><span class="token punctuation">,</span> <span class="token number">77</span><span class="token punctuation">,</span> <span class="token number">43</span><span class="token punctuation">,</span> <span class="token number">51</span><span class="token punctuation">]</span><span class="token punctuation">,</span>         <span class="token punctuation">[</span><span class="token number">61</span><span class="token punctuation">,</span> <span class="token number">81</span><span class="token punctuation">,</span> <span class="token number">98</span><span class="token punctuation">,</span> <span class="token number">11</span><span class="token punctuation">,</span> <span class="token number">31</span><span class="token punctuation">,</span> <span class="token number">69</span><span class="token punctuation">,</span> <span class="token number">91</span><span class="token punctuation">]</span><span class="token punctuation">,</span>         <span class="token punctuation">[</span><span class="token number">93</span><span class="token punctuation">,</span> <span class="token number">94</span><span class="token punctuation">,</span> <span class="token number">59</span><span class="token punctuation">,</span>  <span class="token number">6</span><span class="token punctuation">,</span> <span class="token number">54</span><span class="token punctuation">,</span> <span class="token number">18</span><span class="token punctuation">,</span>  <span class="token number">3</span><span class="token punctuation">]</span><span class="token punctuation">,</span>         <span class="token punctuation">[</span><span class="token number">94</span><span class="token punctuation">,</span> <span class="token number">88</span><span class="token punctuation">,</span>  <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">59</span><span class="token punctuation">,</span> <span class="token number">41</span><span class="token punctuation">,</span> <span class="token number">41</span><span class="token punctuation">,</span> <span class="token number">27</span><span class="token punctuation">]</span><span class="token punctuation">,</span>         <span class="token punctuation">[</span><span class="token number">69</span><span class="token punctuation">,</span> <span class="token number">20</span><span class="token punctuation">,</span> <span class="token number">68</span><span class="token punctuation">,</span> <span class="token number">75</span><span class="token punctuation">,</span> <span class="token number">85</span><span class="token punctuation">,</span> <span class="token number">68</span><span class="token punctuation">,</span>  <span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">,</span>        <span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">17</span><span class="token punctuation">,</span> <span class="token number">74</span><span class="token punctuation">,</span> <span class="token number">60</span><span class="token punctuation">,</span> <span class="token number">10</span><span class="token punctuation">,</span> <span class="token number">21</span><span class="token punctuation">,</span> <span class="token number">97</span><span class="token punctuation">,</span> <span class="token number">83</span><span class="token punctuation">]</span><span class="token punctuation">,</span>         <span class="token punctuation">[</span><span class="token number">28</span><span class="token punctuation">,</span> <span class="token number">37</span><span class="token punctuation">,</span>  <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">49</span><span class="token punctuation">,</span> <span class="token number">12</span><span class="token punctuation">,</span> <span class="token number">11</span><span class="token punctuation">,</span> <span class="token number">47</span><span class="token punctuation">]</span><span class="token punctuation">,</span>         <span class="token punctuation">[</span><span class="token number">57</span><span class="token punctuation">,</span> <span class="token number">29</span><span class="token punctuation">,</span> <span class="token number">79</span><span class="token punctuation">,</span> <span class="token number">19</span><span class="token punctuation">,</span> <span class="token number">95</span><span class="token punctuation">,</span> <span class="token number">84</span><span class="token punctuation">,</span>  <span class="token number">7</span><span class="token punctuation">]</span><span class="token punctuation">,</span>         <span class="token punctuation">[</span><span class="token number">37</span><span class="token punctuation">,</span> <span class="token number">52</span><span class="token punctuation">,</span> <span class="token number">57</span><span class="token punctuation">,</span> <span class="token number">61</span><span class="token punctuation">,</span> <span class="token number">69</span><span class="token punctuation">,</span> <span class="token number">52</span><span class="token punctuation">,</span> <span class="token number">25</span><span class="token punctuation">]</span><span class="token punctuation">,</span>         <span class="token punctuation">[</span><span class="token number">73</span><span class="token punctuation">,</span>  <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">20</span><span class="token punctuation">,</span> <span class="token number">37</span><span class="token punctuation">,</span> <span class="token number">25</span><span class="token punctuation">,</span> <span class="token number">32</span><span class="token punctuation">,</span>  <span class="token number">9</span><span class="token punctuation">]</span><span class="token punctuation">,</span>         <span class="token punctuation">[</span><span class="token number">39</span><span class="token punctuation">,</span> <span class="token number">60</span><span class="token punctuation">,</span> <span class="token number">17</span><span class="token punctuation">,</span> <span class="token number">47</span><span class="token punctuation">,</span> <span class="token number">85</span><span class="token punctuation">,</span> <span class="token number">44</span><span class="token punctuation">,</span> <span class="token number">51</span><span class="token punctuation">]</span><span class="token punctuation">,</span>         <span class="token punctuation">[</span><span class="token number">45</span><span class="token punctuation">,</span> <span class="token number">60</span><span class="token punctuation">,</span> <span class="token number">81</span><span class="token punctuation">,</span> <span class="token number">97</span><span class="token punctuation">,</span> <span class="token number">81</span><span class="token punctuation">,</span> <span class="token number">97</span><span class="token punctuation">,</span> <span class="token number">46</span><span class="token punctuation">]</span><span class="token punctuation">,</span>         <span class="token punctuation">[</span> <span class="token number">5</span><span class="token punctuation">,</span> <span class="token number">26</span><span class="token punctuation">,</span> <span class="token number">84</span><span class="token punctuation">,</span> <span class="token number">49</span><span class="token punctuation">,</span> <span class="token number">25</span><span class="token punctuation">,</span> <span class="token number">11</span><span class="token punctuation">,</span>  <span class="token number">3</span><span class="token punctuation">]</span><span class="token punctuation">,</span>         <span class="token punctuation">[</span> <span class="token number">7</span><span class="token punctuation">,</span> <span class="token number">39</span><span class="token punctuation">,</span> <span class="token number">77</span><span class="token punctuation">,</span> <span class="token number">77</span><span class="token punctuation">,</span>  <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">81</span><span class="token punctuation">,</span> <span class="token number">10</span><span class="token punctuation">]</span><span class="token punctuation">,</span>         <span class="token punctuation">[</span><span class="token number">39</span><span class="token punctuation">,</span> <span class="token number">29</span><span class="token punctuation">,</span> <span class="token number">40</span><span class="token punctuation">,</span> <span class="token number">40</span><span class="token punctuation">,</span>  <span class="token number">5</span><span class="token punctuation">,</span>  <span class="token number">6</span><span class="token punctuation">,</span> <span class="token number">42</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">,</span>        <span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">50</span><span class="token punctuation">,</span> <span class="token number">27</span><span class="token punctuation">,</span> <span class="token number">68</span><span class="token punctuation">,</span>  <span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">46</span><span class="token punctuation">,</span> <span class="token number">93</span><span class="token punctuation">,</span> <span class="token number">29</span><span class="token punctuation">]</span><span class="token punctuation">,</span>         <span class="token punctuation">[</span><span class="token number">95</span><span class="token punctuation">,</span> <span class="token number">68</span><span class="token punctuation">,</span>  <span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">81</span><span class="token punctuation">,</span> <span class="token number">44</span><span class="token punctuation">,</span> <span class="token number">27</span><span class="token punctuation">,</span> <span class="token number">89</span><span class="token punctuation">]</span><span class="token punctuation">,</span>         <span class="token punctuation">[</span> <span class="token number">9</span><span class="token punctuation">,</span> <span class="token number">55</span><span class="token punctuation">,</span> <span class="token number">39</span><span class="token punctuation">,</span> <span class="token number">85</span><span class="token punctuation">,</span> <span class="token number">63</span><span class="token punctuation">,</span> <span class="token number">74</span><span class="token punctuation">,</span> <span class="token number">67</span><span class="token punctuation">]</span><span class="token punctuation">,</span>         <span class="token punctuation">[</span><span class="token number">37</span><span class="token punctuation">,</span> <span class="token number">39</span><span class="token punctuation">,</span>  <span class="token number">8</span><span class="token punctuation">,</span> <span class="token number">77</span><span class="token punctuation">,</span> <span class="token number">89</span><span class="token punctuation">,</span> <span class="token number">84</span><span class="token punctuation">,</span> <span class="token number">14</span><span class="token punctuation">]</span><span class="token punctuation">,</span>         <span class="token punctuation">[</span><span class="token number">52</span><span class="token punctuation">,</span> <span class="token number">14</span><span class="token punctuation">,</span> <span class="token number">22</span><span class="token punctuation">,</span> <span class="token number">20</span><span class="token punctuation">,</span> <span class="token number">67</span><span class="token punctuation">,</span> <span class="token number">20</span><span class="token punctuation">,</span> <span class="token number">48</span><span class="token punctuation">]</span><span class="token punctuation">,</span>         <span class="token punctuation">[</span><span class="token number">52</span><span class="token punctuation">,</span> <span class="token number">82</span><span class="token punctuation">,</span> <span class="token number">12</span><span class="token punctuation">,</span> <span class="token number">15</span><span class="token punctuation">,</span> <span class="token number">20</span><span class="token punctuation">,</span> <span class="token number">84</span><span class="token punctuation">,</span> <span class="token number">32</span><span class="token punctuation">]</span><span class="token punctuation">,</span>         <span class="token punctuation">[</span><span class="token number">92</span><span class="token punctuation">,</span> <span class="token number">68</span><span class="token punctuation">,</span> <span class="token number">56</span><span class="token punctuation">,</span> <span class="token number">49</span><span class="token punctuation">,</span> <span class="token number">40</span><span class="token punctuation">,</span> <span class="token number">56</span><span class="token punctuation">,</span> <span class="token number">38</span><span class="token punctuation">]</span><span class="token punctuation">,</span>         <span class="token punctuation">[</span><span class="token number">49</span><span class="token punctuation">,</span> <span class="token number">56</span><span class="token punctuation">,</span> <span class="token number">10</span><span class="token punctuation">,</span> <span class="token number">23</span><span class="token punctuation">,</span> <span class="token number">90</span><span class="token punctuation">,</span>  <span class="token number">9</span><span class="token punctuation">,</span> <span class="token number">46</span><span class="token punctuation">]</span><span class="token punctuation">,</span>         <span class="token punctuation">[</span><span class="token number">99</span><span class="token punctuation">,</span> <span class="token number">68</span><span class="token punctuation">,</span> <span class="token number">51</span><span class="token punctuation">,</span>  <span class="token number">6</span><span class="token punctuation">,</span> <span class="token number">74</span><span class="token punctuation">,</span> <span class="token number">14</span><span class="token punctuation">,</span> <span class="token number">35</span><span class="token punctuation">]</span><span class="token punctuation">,</span>         <span class="token punctuation">[</span><span class="token number">33</span><span class="token punctuation">,</span> <span class="token number">42</span><span class="token punctuation">,</span> <span class="token number">50</span><span class="token punctuation">,</span> <span class="token number">91</span><span class="token punctuation">,</span> <span class="token number">56</span><span class="token punctuation">,</span> <span class="token number">94</span><span class="token punctuation">,</span> <span class="token number">80</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">,</span>        <span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">18</span><span class="token punctuation">,</span> <span class="token number">72</span><span class="token punctuation">,</span> <span class="token number">14</span><span class="token punctuation">,</span> <span class="token number">28</span><span class="token punctuation">,</span> <span class="token number">64</span><span class="token punctuation">,</span> <span class="token number">66</span><span class="token punctuation">,</span> <span class="token number">87</span><span class="token punctuation">]</span><span class="token punctuation">,</span>         <span class="token punctuation">[</span><span class="token number">33</span><span class="token punctuation">,</span> <span class="token number">50</span><span class="token punctuation">,</span> <span class="token number">75</span><span class="token punctuation">,</span>  <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">86</span><span class="token punctuation">,</span>  <span class="token number">8</span><span class="token punctuation">,</span> <span class="token number">50</span><span class="token punctuation">]</span><span class="token punctuation">,</span>         <span class="token punctuation">[</span><span class="token number">41</span><span class="token punctuation">,</span> <span class="token number">23</span><span class="token punctuation">,</span> <span class="token number">56</span><span class="token punctuation">,</span> <span class="token number">91</span><span class="token punctuation">,</span> <span class="token number">35</span><span class="token punctuation">,</span> <span class="token number">20</span><span class="token punctuation">,</span> <span class="token number">31</span><span class="token punctuation">]</span><span class="token punctuation">,</span>         <span class="token punctuation">[</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">72</span><span class="token punctuation">,</span> <span class="token number">25</span><span class="token punctuation">,</span> <span class="token number">16</span><span class="token punctuation">,</span> <span class="token number">21</span><span class="token punctuation">,</span> <span class="token number">78</span><span class="token punctuation">,</span> <span class="token number">76</span><span class="token punctuation">]</span><span class="token punctuation">,</span>         <span class="token punctuation">[</span><span class="token number">88</span><span class="token punctuation">,</span> <span class="token number">68</span><span class="token punctuation">,</span> <span class="token number">33</span><span class="token punctuation">,</span> <span class="token number">36</span><span class="token punctuation">,</span> <span class="token number">64</span><span class="token punctuation">,</span> <span class="token number">91</span><span class="token punctuation">,</span> <span class="token number">63</span><span class="token punctuation">]</span><span class="token punctuation">,</span>         <span class="token punctuation">[</span><span class="token number">26</span><span class="token punctuation">,</span> <span class="token number">26</span><span class="token punctuation">,</span>  <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">60</span><span class="token punctuation">,</span> <span class="token number">21</span><span class="token punctuation">,</span>  <span class="token number">5</span><span class="token punctuation">,</span> <span class="token number">93</span><span class="token punctuation">]</span><span class="token punctuation">,</span>         <span class="token punctuation">[</span><span class="token number">17</span><span class="token punctuation">,</span> <span class="token number">44</span><span class="token punctuation">,</span> <span class="token number">64</span><span class="token punctuation">,</span> <span class="token number">51</span><span class="token punctuation">,</span> <span class="token number">16</span><span class="token punctuation">,</span>  <span class="token number">9</span><span class="token punctuation">,</span> <span class="token number">89</span><span class="token punctuation">]</span><span class="token punctuation">,</span>         <span class="token punctuation">[</span><span class="token number">58</span><span class="token punctuation">,</span> <span class="token number">91</span><span class="token punctuation">,</span> <span class="token number">33</span><span class="token punctuation">,</span> <span class="token number">64</span><span class="token punctuation">,</span> <span class="token number">38</span><span class="token punctuation">,</span> <span class="token number">47</span><span class="token punctuation">,</span> <span class="token number">19</span><span class="token punctuation">]</span><span class="token punctuation">,</span>         <span class="token punctuation">[</span><span class="token number">66</span><span class="token punctuation">,</span> <span class="token number">65</span><span class="token punctuation">,</span> <span class="token number">48</span><span class="token punctuation">,</span> <span class="token number">38</span><span class="token punctuation">,</span> <span class="token number">19</span><span class="token punctuation">,</span> <span class="token number">84</span><span class="token punctuation">,</span> <span class="token number">12</span><span class="token punctuation">]</span><span class="token punctuation">,</span>         <span class="token punctuation">[</span><span class="token number">70</span><span class="token punctuation">,</span> <span class="token number">33</span><span class="token punctuation">,</span> <span class="token number">25</span><span class="token punctuation">,</span> <span class="token number">58</span><span class="token punctuation">,</span> <span class="token number">24</span><span class="token punctuation">,</span> <span class="token number">61</span><span class="token punctuation">,</span> <span class="token number">59</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">,</span> dtype<span class="token operator">=</span>torch<span class="token punctuation">.</span>int32<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>抽取每个班级第0个学生，第5个学生，第9个学生的全部成绩</p><pre class="line-numbers language-python"><code class="language-python">torch<span class="token punctuation">.</span>index_select<span class="token punctuation">(</span>scores<span class="token punctuation">,</span> dim <span class="token operator">=</span> <span class="token number">1</span><span class="token punctuation">,</span> index <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span><span class="token number">5</span><span class="token punctuation">,</span><span class="token number">9</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token operator">>></span><span class="token operator">></span>Output<span class="token punctuation">:</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">55</span><span class="token punctuation">,</span> <span class="token number">95</span><span class="token punctuation">,</span>  <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">18</span><span class="token punctuation">,</span> <span class="token number">37</span><span class="token punctuation">,</span> <span class="token number">30</span><span class="token punctuation">,</span> <span class="token number">93</span><span class="token punctuation">]</span><span class="token punctuation">,</span>         <span class="token punctuation">[</span><span class="token number">72</span><span class="token punctuation">,</span> <span class="token number">70</span><span class="token punctuation">,</span> <span class="token number">20</span><span class="token punctuation">,</span> <span class="token number">65</span><span class="token punctuation">,</span> <span class="token number">77</span><span class="token punctuation">,</span> <span class="token number">43</span><span class="token punctuation">,</span> <span class="token number">51</span><span class="token punctuation">]</span><span class="token punctuation">,</span>         <span class="token punctuation">[</span><span class="token number">69</span><span class="token punctuation">,</span> <span class="token number">20</span><span class="token punctuation">,</span> <span class="token number">68</span><span class="token punctuation">,</span> <span class="token number">75</span><span class="token punctuation">,</span> <span class="token number">85</span><span class="token punctuation">,</span> <span class="token number">68</span><span class="token punctuation">,</span>  <span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">,</span>        <span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">17</span><span class="token punctuation">,</span> <span class="token number">74</span><span class="token punctuation">,</span> <span class="token number">60</span><span class="token punctuation">,</span> <span class="token number">10</span><span class="token punctuation">,</span> <span class="token number">21</span><span class="token punctuation">,</span> <span class="token number">97</span><span class="token punctuation">,</span> <span class="token number">83</span><span class="token punctuation">]</span><span class="token punctuation">,</span>         <span class="token punctuation">[</span><span class="token number">39</span><span class="token punctuation">,</span> <span class="token number">60</span><span class="token punctuation">,</span> <span class="token number">17</span><span class="token punctuation">,</span> <span class="token number">47</span><span class="token punctuation">,</span> <span class="token number">85</span><span class="token punctuation">,</span> <span class="token number">44</span><span class="token punctuation">,</span> <span class="token number">51</span><span class="token punctuation">]</span><span class="token punctuation">,</span>         <span class="token punctuation">[</span><span class="token number">39</span><span class="token punctuation">,</span> <span class="token number">29</span><span class="token punctuation">,</span> <span class="token number">40</span><span class="token punctuation">,</span> <span class="token number">40</span><span class="token punctuation">,</span>  <span class="token number">5</span><span class="token punctuation">,</span>  <span class="token number">6</span><span class="token punctuation">,</span> <span class="token number">42</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">,</span>        <span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">50</span><span class="token punctuation">,</span> <span class="token number">27</span><span class="token punctuation">,</span> <span class="token number">68</span><span class="token punctuation">,</span>  <span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">46</span><span class="token punctuation">,</span> <span class="token number">93</span><span class="token punctuation">,</span> <span class="token number">29</span><span class="token punctuation">]</span><span class="token punctuation">,</span>         <span class="token punctuation">[</span><span class="token number">52</span><span class="token punctuation">,</span> <span class="token number">82</span><span class="token punctuation">,</span> <span class="token number">12</span><span class="token punctuation">,</span> <span class="token number">15</span><span class="token punctuation">,</span> <span class="token number">20</span><span class="token punctuation">,</span> <span class="token number">84</span><span class="token punctuation">,</span> <span class="token number">32</span><span class="token punctuation">]</span><span class="token punctuation">,</span>         <span class="token punctuation">[</span><span class="token number">33</span><span class="token punctuation">,</span> <span class="token number">42</span><span class="token punctuation">,</span> <span class="token number">50</span><span class="token punctuation">,</span> <span class="token number">91</span><span class="token punctuation">,</span> <span class="token number">56</span><span class="token punctuation">,</span> <span class="token number">94</span><span class="token punctuation">,</span> <span class="token number">80</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">,</span>        <span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">18</span><span class="token punctuation">,</span> <span class="token number">72</span><span class="token punctuation">,</span> <span class="token number">14</span><span class="token punctuation">,</span> <span class="token number">28</span><span class="token punctuation">,</span> <span class="token number">64</span><span class="token punctuation">,</span> <span class="token number">66</span><span class="token punctuation">,</span> <span class="token number">87</span><span class="token punctuation">]</span><span class="token punctuation">,</span>         <span class="token punctuation">[</span><span class="token number">26</span><span class="token punctuation">,</span> <span class="token number">26</span><span class="token punctuation">,</span>  <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">60</span><span class="token punctuation">,</span> <span class="token number">21</span><span class="token punctuation">,</span>  <span class="token number">5</span><span class="token punctuation">,</span> <span class="token number">93</span><span class="token punctuation">]</span><span class="token punctuation">,</span>         <span class="token punctuation">[</span><span class="token number">70</span><span class="token punctuation">,</span> <span class="token number">33</span><span class="token punctuation">,</span> <span class="token number">25</span><span class="token punctuation">,</span> <span class="token number">58</span><span class="token punctuation">,</span> <span class="token number">24</span><span class="token punctuation">,</span> <span class="token number">61</span><span class="token punctuation">,</span> <span class="token number">59</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">,</span> dtype<span class="token operator">=</span>torch<span class="token punctuation">.</span>int32<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>抽取每个班级第0个学生，第5个学生，第9个学生的第1门课程，第3门课程，第6门课程成绩</p><pre class="line-numbers language-python"><code class="language-python">q <span class="token operator">=</span> torch<span class="token punctuation">.</span>index_select<span class="token punctuation">(</span>        torch<span class="token punctuation">.</span>index_select<span class="token punctuation">(</span>scores<span class="token punctuation">,</span>dim <span class="token operator">=</span> <span class="token number">1</span><span class="token punctuation">,</span>index <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span><span class="token number">5</span><span class="token punctuation">,</span><span class="token number">9</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span>        dim<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span> index <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">3</span><span class="token punctuation">,</span><span class="token number">6</span><span class="token punctuation">]</span><span class="token punctuation">)</span>    <span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><p>抽取第0个班级第0个学生的第0门课程，第2个班级的第4个学生的第1门课程，第3个班级的第9个学生第6门课程成绩。</p><pre class="line-numbers language-python"><code class="language-python"><span class="token comment" spellcheck="true"># take将输入看成一维数组，输出和index同形状</span>s <span class="token operator">=</span> torch<span class="token punctuation">.</span>take<span class="token punctuation">(</span>scores<span class="token punctuation">,</span>torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token operator">*</span><span class="token number">10</span><span class="token operator">*</span><span class="token number">7</span><span class="token operator">+</span><span class="token number">0</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token operator">*</span><span class="token number">10</span><span class="token operator">*</span><span class="token number">7</span><span class="token operator">+</span><span class="token number">4</span><span class="token operator">*</span><span class="token number">7</span><span class="token operator">+</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">3</span><span class="token operator">*</span><span class="token number">10</span><span class="token operator">*</span><span class="token number">7</span><span class="token operator">+</span><span class="token number">9</span><span class="token operator">*</span><span class="token number">7</span><span class="token operator">+</span><span class="token number">6</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>s<span class="token punctuation">)</span><span class="token operator">>></span><span class="token operator">></span>Output<span class="token punctuation">:</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">55</span><span class="token punctuation">,</span> <span class="token number">14</span><span class="token punctuation">,</span> <span class="token number">59</span><span class="token punctuation">]</span><span class="token punctuation">,</span> dtype<span class="token operator">=</span>torch<span class="token punctuation">.</span>int32<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>抽取分数大于等于80分的分数（布尔索引）</p><pre class="line-numbers language-python"><code class="language-python">g <span class="token operator">=</span> torch<span class="token punctuation">.</span>masked_select<span class="token punctuation">(</span>scores<span class="token punctuation">,</span> scores<span class="token operator">>=</span><span class="token number">80</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>g<span class="token punctuation">)</span> <span class="token comment" spellcheck="true"># 结果是1维张量</span><span class="token operator">>></span><span class="token operator">></span>Output<span class="token punctuation">:</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">95</span><span class="token punctuation">,</span> <span class="token number">93</span><span class="token punctuation">,</span> <span class="token number">92</span><span class="token punctuation">,</span> <span class="token number">81</span><span class="token punctuation">,</span> <span class="token number">81</span><span class="token punctuation">,</span> <span class="token number">99</span><span class="token punctuation">,</span> <span class="token number">83</span><span class="token punctuation">,</span> <span class="token number">81</span><span class="token punctuation">,</span> <span class="token number">98</span><span class="token punctuation">,</span> <span class="token number">91</span><span class="token punctuation">,</span> <span class="token number">93</span><span class="token punctuation">,</span> <span class="token number">94</span><span class="token punctuation">,</span> <span class="token number">94</span><span class="token punctuation">,</span> <span class="token number">88</span><span class="token punctuation">,</span> <span class="token number">85</span><span class="token punctuation">,</span> <span class="token number">97</span><span class="token punctuation">,</span> <span class="token number">83</span><span class="token punctuation">,</span> <span class="token number">95</span><span class="token punctuation">,</span>        <span class="token number">84</span><span class="token punctuation">,</span> <span class="token number">85</span><span class="token punctuation">,</span> <span class="token number">81</span><span class="token punctuation">,</span> <span class="token number">97</span><span class="token punctuation">,</span> <span class="token number">81</span><span class="token punctuation">,</span> <span class="token number">97</span><span class="token punctuation">,</span> <span class="token number">84</span><span class="token punctuation">,</span> <span class="token number">81</span><span class="token punctuation">,</span> <span class="token number">93</span><span class="token punctuation">,</span> <span class="token number">95</span><span class="token punctuation">,</span> <span class="token number">81</span><span class="token punctuation">,</span> <span class="token number">89</span><span class="token punctuation">,</span> <span class="token number">85</span><span class="token punctuation">,</span> <span class="token number">89</span><span class="token punctuation">,</span> <span class="token number">84</span><span class="token punctuation">,</span> <span class="token number">82</span><span class="token punctuation">,</span> <span class="token number">84</span><span class="token punctuation">,</span> <span class="token number">92</span><span class="token punctuation">,</span>        <span class="token number">90</span><span class="token punctuation">,</span> <span class="token number">99</span><span class="token punctuation">,</span> <span class="token number">91</span><span class="token punctuation">,</span> <span class="token number">94</span><span class="token punctuation">,</span> <span class="token number">80</span><span class="token punctuation">,</span> <span class="token number">87</span><span class="token punctuation">,</span> <span class="token number">86</span><span class="token punctuation">,</span> <span class="token number">91</span><span class="token punctuation">,</span> <span class="token number">88</span><span class="token punctuation">,</span> <span class="token number">91</span><span class="token punctuation">,</span> <span class="token number">93</span><span class="token punctuation">,</span> <span class="token number">89</span><span class="token punctuation">,</span> <span class="token number">91</span><span class="token punctuation">,</span> <span class="token number">84</span><span class="token punctuation">]</span><span class="token punctuation">,</span>       dtype<span class="token operator">=</span>torch<span class="token punctuation">.</span>int32<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>以上这些方法仅能提取张量的部分元素值，但不能更改张量的部分元素值得到新的张量。<br>如果要通过修改张量的部分元素值得到新的张量，可以使用<code>torch.where</code>，<code>torch.index_fill</code>和 <code>torch.masked_fill</code>。<br><code>torch.where</code>可以理解为if的张量版本。<br><code>torch.index_fill</code>的选取元素逻辑和<code>torch.index_select</code>相同。<br><code>torch.masked_fill</code>的选取元素逻辑和<code>torch.masked_select</code>相同。</p><pre class="line-numbers language-python"><code class="language-python"><span class="token comment" spellcheck="true">#如果分数大于60分，赋值成1，否则赋值成0</span>ifpass <span class="token operator">=</span> torch<span class="token punctuation">.</span>where<span class="token punctuation">(</span>scores<span class="token operator">></span><span class="token number">60</span><span class="token punctuation">,</span>torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span>torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true"># 将每个班级第0个学生，第5个学生，第9个学生的全部成绩赋值成满分</span>full <span class="token operator">=</span> torch<span class="token punctuation">.</span>index_fill<span class="token punctuation">(</span>scores<span class="token punctuation">,</span>dim <span class="token operator">=</span> <span class="token number">1</span><span class="token punctuation">,</span>index <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span><span class="token number">5</span><span class="token punctuation">,</span><span class="token number">9</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">,</span>value <span class="token operator">=</span> <span class="token number">100</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true">#将分数小于60分的分数赋值成60分</span>b <span class="token operator">=</span> torch<span class="token punctuation">.</span>masked_fill<span class="token punctuation">(</span>scores<span class="token punctuation">,</span>scores<span class="token operator">&lt;</span><span class="token number">60</span><span class="token punctuation">,</span><span class="token number">60</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="维度变换"><a href="#维度变换" class="headerlink" title="维度变换"></a>维度变换</h3><p>维度变换相关函数主要有<code>torch.reshape</code>(或者调用张量的<code>view()</code>方法), <code>torch.squeeze()</code>, <code>torch.unsqueeze()</code>, <code>torch.transpose()</code>。</p><ul><li>torch.reshape 可以改变张量的形状。</li><li>torch.squeeze 可以减少维度。</li><li>torch.unsqueeze 可以增加维度。</li><li>torch.transpose 可以交换维度。</li></ul><p>张量的view方法有时候会调用失败，可以使用reshape方法。</p><pre class="line-numbers language-python"><code class="language-python">minval<span class="token punctuation">,</span>maxval <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">,</span><span class="token number">255</span>a <span class="token operator">=</span> <span class="token punctuation">(</span>minval <span class="token operator">+</span> <span class="token punctuation">(</span>maxval<span class="token operator">-</span>minval<span class="token punctuation">)</span><span class="token operator">*</span>torch<span class="token punctuation">.</span>rand<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">3</span><span class="token punctuation">,</span><span class="token number">3</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">.</span>int<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>a<span class="token punctuation">.</span>shape<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>a<span class="token punctuation">)</span><span class="token comment" spellcheck="true"># 改成（3,6）形状的张量</span>b <span class="token operator">=</span> a<span class="token punctuation">.</span>view<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">3</span><span class="token punctuation">,</span><span class="token number">6</span><span class="token punctuation">]</span><span class="token punctuation">)</span> <span class="token comment" spellcheck="true"># torch.reshape(a,[3,6])</span><span class="token keyword">print</span><span class="token punctuation">(</span>b<span class="token punctuation">.</span>shape<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>b<span class="token punctuation">)</span><span class="token comment" spellcheck="true"># 改回成 [1,3,3,2] 形状的张量</span>c <span class="token operator">=</span> torch<span class="token punctuation">.</span>reshape<span class="token punctuation">(</span>b<span class="token punctuation">,</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">3</span><span class="token punctuation">,</span><span class="token number">3</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">)</span> <span class="token comment" spellcheck="true"># b.view([1,3,3,2]) </span><span class="token operator">>></span><span class="token operator">></span>Output<span class="token punctuation">:</span>torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">)</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">126</span><span class="token punctuation">,</span> <span class="token number">195</span><span class="token punctuation">]</span><span class="token punctuation">,</span>          <span class="token punctuation">[</span> <span class="token number">22</span><span class="token punctuation">,</span>  <span class="token number">33</span><span class="token punctuation">]</span><span class="token punctuation">,</span>          <span class="token punctuation">[</span> <span class="token number">78</span><span class="token punctuation">,</span> <span class="token number">161</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">,</span>         <span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">124</span><span class="token punctuation">,</span> <span class="token number">228</span><span class="token punctuation">]</span><span class="token punctuation">,</span>          <span class="token punctuation">[</span><span class="token number">116</span><span class="token punctuation">,</span> <span class="token number">161</span><span class="token punctuation">]</span><span class="token punctuation">,</span>          <span class="token punctuation">[</span> <span class="token number">88</span><span class="token punctuation">,</span> <span class="token number">102</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">,</span>         <span class="token punctuation">[</span><span class="token punctuation">[</span>  <span class="token number">5</span><span class="token punctuation">,</span>  <span class="token number">43</span><span class="token punctuation">]</span><span class="token punctuation">,</span>          <span class="token punctuation">[</span> <span class="token number">74</span><span class="token punctuation">,</span> <span class="token number">132</span><span class="token punctuation">]</span><span class="token punctuation">,</span>          <span class="token punctuation">[</span><span class="token number">177</span><span class="token punctuation">,</span> <span class="token number">204</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">,</span> dtype<span class="token operator">=</span>torch<span class="token punctuation">.</span>int32<span class="token punctuation">)</span>torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">6</span><span class="token punctuation">]</span><span class="token punctuation">)</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">126</span><span class="token punctuation">,</span> <span class="token number">195</span><span class="token punctuation">,</span>  <span class="token number">22</span><span class="token punctuation">,</span>  <span class="token number">33</span><span class="token punctuation">,</span>  <span class="token number">78</span><span class="token punctuation">,</span> <span class="token number">161</span><span class="token punctuation">]</span><span class="token punctuation">,</span>        <span class="token punctuation">[</span><span class="token number">124</span><span class="token punctuation">,</span> <span class="token number">228</span><span class="token punctuation">,</span> <span class="token number">116</span><span class="token punctuation">,</span> <span class="token number">161</span><span class="token punctuation">,</span>  <span class="token number">88</span><span class="token punctuation">,</span> <span class="token number">102</span><span class="token punctuation">]</span><span class="token punctuation">,</span>        <span class="token punctuation">[</span>  <span class="token number">5</span><span class="token punctuation">,</span>  <span class="token number">43</span><span class="token punctuation">,</span>  <span class="token number">74</span><span class="token punctuation">,</span> <span class="token number">132</span><span class="token punctuation">,</span> <span class="token number">177</span><span class="token punctuation">,</span> <span class="token number">204</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">,</span> dtype<span class="token operator">=</span>torch<span class="token punctuation">.</span>int32<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>如果张量在某个维度上的维度是1，利用<code>torch.squeeze()</code>可以消除这个维度。<code>torch.unsqueeze()</code>的作用和<code>torch.squeeze()</code>的作用相反。</p><pre class="line-numbers language-python"><code class="language-python">a <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">1.0</span><span class="token punctuation">,</span> <span class="token number">2.0</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span>s <span class="token operator">=</span> torch<span class="token punctuation">.</span>squeeze<span class="token punctuation">(</span>a<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>a<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>a<span class="token punctuation">.</span>shape<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>s<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>s<span class="token punctuation">.</span>shape<span class="token punctuation">)</span><span class="token operator">>></span><span class="token operator">></span>Output<span class="token punctuation">:</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">.</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span>torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">)</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">.</span><span class="token punctuation">]</span><span class="token punctuation">)</span>torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>在张量的第0维插入一个维度</p><pre class="line-numbers language-python"><code class="language-python">d <span class="token operator">=</span> torch<span class="token punctuation">.</span>unsqueeze<span class="token punctuation">(</span>s<span class="token punctuation">,</span>axis<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span>  <span class="token keyword">print</span><span class="token punctuation">(</span>d<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>d<span class="token punctuation">.</span>shape<span class="token punctuation">)</span><span class="token operator">>></span><span class="token operator">></span>Output<span class="token punctuation">:</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">.</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span>torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p><code>torch.transpose()</code>可以交换张量的维度，<code>torch.transpose()</code>常用于图片存储格式的变换上。<br>如果是二维的矩阵，通常会调用矩阵的转置方法 <code>matrix.t()</code>，等价于 <code>torch.transpose(matrix, 0, 1)</code>。</p><pre class="line-numbers language-python"><code class="language-python">minval<span class="token operator">=</span><span class="token number">0</span>maxval<span class="token operator">=</span><span class="token number">255</span><span class="token comment" spellcheck="true"># Batch,Height,Width,Channel</span>data <span class="token operator">=</span> torch<span class="token punctuation">.</span>floor<span class="token punctuation">(</span>minval <span class="token operator">+</span> <span class="token punctuation">(</span>maxval<span class="token operator">-</span>minval<span class="token punctuation">)</span><span class="token operator">*</span>torch<span class="token punctuation">.</span>rand<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">100</span><span class="token punctuation">,</span><span class="token number">256</span><span class="token punctuation">,</span><span class="token number">256</span><span class="token punctuation">,</span><span class="token number">4</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">.</span>int<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>data<span class="token punctuation">.</span>shape<span class="token punctuation">)</span><span class="token comment" spellcheck="true"># 转换成 Pytorch默认的图片格式 Batch,Channel,Height,Width </span><span class="token comment" spellcheck="true"># 需要交换两次</span>data_t <span class="token operator">=</span> torch<span class="token punctuation">.</span>transpose<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>transpose<span class="token punctuation">(</span>data<span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">3</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>data_t<span class="token punctuation">.</span>shape<span class="token punctuation">)</span>matrix <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">3</span><span class="token punctuation">]</span><span class="token punctuation">,</span><span class="token punctuation">[</span><span class="token number">4</span><span class="token punctuation">,</span><span class="token number">5</span><span class="token punctuation">,</span><span class="token number">6</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>matrix<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>matrix<span class="token punctuation">.</span>t<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token comment" spellcheck="true"># 等价于torch.transpose(matrix,0,1)</span><span class="token operator">>></span><span class="token operator">></span>Output<span class="token punctuation">:</span>torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">100</span><span class="token punctuation">,</span> <span class="token number">256</span><span class="token punctuation">,</span> <span class="token number">256</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">]</span><span class="token punctuation">)</span>torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">100</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">256</span><span class="token punctuation">,</span> <span class="token number">256</span><span class="token punctuation">]</span><span class="token punctuation">)</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">]</span><span class="token punctuation">,</span>        <span class="token punctuation">[</span><span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">,</span> <span class="token number">6</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">]</span><span class="token punctuation">,</span>        <span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">]</span><span class="token punctuation">,</span>        <span class="token punctuation">[</span><span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">6</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="合并分割"><a href="#合并分割" class="headerlink" title="合并分割"></a>合并分割</h3><p>可以用<code>torch.cat()</code>方法和<code>torch.stack()</code>方法将多个张量合并，可以用<code>torch.split()</code>方法把一个张量分割成多个张量。<br><code>torch.cat()</code>和<code>torch.stack()</code>有略微的区别：</p><ul><li>torch.cat是连接，不会增加维度</li><li>torch.stack是堆叠，会增加维度。</li></ul><p><strong>torch中dim和axis参数名可以混用</strong>。</p><pre class="line-numbers language-python"><code class="language-python">a <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">1.0</span><span class="token punctuation">,</span><span class="token number">2.0</span><span class="token punctuation">]</span><span class="token punctuation">,</span><span class="token punctuation">[</span><span class="token number">3.0</span><span class="token punctuation">,</span><span class="token number">4.0</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span>b <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">5.0</span><span class="token punctuation">,</span><span class="token number">6.0</span><span class="token punctuation">]</span><span class="token punctuation">,</span><span class="token punctuation">[</span><span class="token number">7.0</span><span class="token punctuation">,</span><span class="token number">8.0</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span>c <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">9.0</span><span class="token punctuation">,</span><span class="token number">10.0</span><span class="token punctuation">]</span><span class="token punctuation">,</span><span class="token punctuation">[</span><span class="token number">11.0</span><span class="token punctuation">,</span><span class="token number">12.0</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span>abc_cat <span class="token operator">=</span> torch<span class="token punctuation">.</span>cat<span class="token punctuation">(</span><span class="token punctuation">[</span>a<span class="token punctuation">,</span>b<span class="token punctuation">,</span>c<span class="token punctuation">]</span><span class="token punctuation">,</span> dim <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>abc_cat<span class="token punctuation">.</span>shape<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>abc_cat<span class="token punctuation">)</span><span class="token comment" spellcheck="true"># torch中dim和axis参数名可以混用</span>abc_stack <span class="token operator">=</span> torch<span class="token punctuation">.</span>stack<span class="token punctuation">(</span><span class="token punctuation">[</span>a<span class="token punctuation">,</span>b<span class="token punctuation">,</span>c<span class="token punctuation">]</span><span class="token punctuation">,</span> axis <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">)</span> <span class="token keyword">print</span><span class="token punctuation">(</span>abc_stack<span class="token punctuation">.</span>shape<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>abc_stack<span class="token punctuation">)</span><span class="token operator">>></span><span class="token operator">></span>Output<span class="token punctuation">:</span>torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">6</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">)</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span> <span class="token number">1</span><span class="token punctuation">.</span><span class="token punctuation">,</span>  <span class="token number">2</span><span class="token punctuation">.</span><span class="token punctuation">]</span><span class="token punctuation">,</span>        <span class="token punctuation">[</span> <span class="token number">3</span><span class="token punctuation">.</span><span class="token punctuation">,</span>  <span class="token number">4</span><span class="token punctuation">.</span><span class="token punctuation">]</span><span class="token punctuation">,</span>        <span class="token punctuation">[</span> <span class="token number">5</span><span class="token punctuation">.</span><span class="token punctuation">,</span>  <span class="token number">6</span><span class="token punctuation">.</span><span class="token punctuation">]</span><span class="token punctuation">,</span>        <span class="token punctuation">[</span> <span class="token number">7</span><span class="token punctuation">.</span><span class="token punctuation">,</span>  <span class="token number">8</span><span class="token punctuation">.</span><span class="token punctuation">]</span><span class="token punctuation">,</span>        <span class="token punctuation">[</span> <span class="token number">9</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">10</span><span class="token punctuation">.</span><span class="token punctuation">]</span><span class="token punctuation">,</span>        <span class="token punctuation">[</span><span class="token number">11</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">12</span><span class="token punctuation">.</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span>torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">)</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token punctuation">[</span> <span class="token number">1</span><span class="token punctuation">.</span><span class="token punctuation">,</span>  <span class="token number">2</span><span class="token punctuation">.</span><span class="token punctuation">]</span><span class="token punctuation">,</span>         <span class="token punctuation">[</span> <span class="token number">3</span><span class="token punctuation">.</span><span class="token punctuation">,</span>  <span class="token number">4</span><span class="token punctuation">.</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">,</span>        <span class="token punctuation">[</span><span class="token punctuation">[</span> <span class="token number">5</span><span class="token punctuation">.</span><span class="token punctuation">,</span>  <span class="token number">6</span><span class="token punctuation">.</span><span class="token punctuation">]</span><span class="token punctuation">,</span>         <span class="token punctuation">[</span> <span class="token number">7</span><span class="token punctuation">.</span><span class="token punctuation">,</span>  <span class="token number">8</span><span class="token punctuation">.</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">,</span>        <span class="token punctuation">[</span><span class="token punctuation">[</span> <span class="token number">9</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">10</span><span class="token punctuation">.</span><span class="token punctuation">]</span><span class="token punctuation">,</span>         <span class="token punctuation">[</span><span class="token number">11</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">12</span><span class="token punctuation">.</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><pre class="line-numbers language-python"><code class="language-python">cat <span class="token operator">=</span> torch<span class="token punctuation">.</span>cat<span class="token punctuation">(</span><span class="token punctuation">[</span>a<span class="token punctuation">,</span>b<span class="token punctuation">,</span>c<span class="token punctuation">]</span><span class="token punctuation">,</span>axis <span class="token operator">=</span> <span class="token number">1</span><span class="token punctuation">)</span>stack <span class="token operator">=</span> torch<span class="token punctuation">.</span>stack<span class="token punctuation">(</span><span class="token punctuation">[</span>a<span class="token punctuation">,</span>b<span class="token punctuation">,</span>c<span class="token punctuation">]</span><span class="token punctuation">,</span>axis <span class="token operator">=</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token operator">>></span><span class="token operator">></span>Output<span class="token punctuation">:</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span> <span class="token number">1</span><span class="token punctuation">.</span><span class="token punctuation">,</span>  <span class="token number">2</span><span class="token punctuation">.</span><span class="token punctuation">,</span>  <span class="token number">5</span><span class="token punctuation">.</span><span class="token punctuation">,</span>  <span class="token number">6</span><span class="token punctuation">.</span><span class="token punctuation">,</span>  <span class="token number">9</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">10</span><span class="token punctuation">.</span><span class="token punctuation">]</span><span class="token punctuation">,</span>        <span class="token punctuation">[</span> <span class="token number">3</span><span class="token punctuation">.</span><span class="token punctuation">,</span>  <span class="token number">4</span><span class="token punctuation">.</span><span class="token punctuation">,</span>  <span class="token number">7</span><span class="token punctuation">.</span><span class="token punctuation">,</span>  <span class="token number">8</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">11</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">12</span><span class="token punctuation">.</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token punctuation">[</span> <span class="token number">1</span><span class="token punctuation">.</span><span class="token punctuation">,</span>  <span class="token number">2</span><span class="token punctuation">.</span><span class="token punctuation">]</span><span class="token punctuation">,</span>         <span class="token punctuation">[</span> <span class="token number">5</span><span class="token punctuation">.</span><span class="token punctuation">,</span>  <span class="token number">6</span><span class="token punctuation">.</span><span class="token punctuation">]</span><span class="token punctuation">,</span>         <span class="token punctuation">[</span> <span class="token number">9</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">10</span><span class="token punctuation">.</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">,</span>        <span class="token punctuation">[</span><span class="token punctuation">[</span> <span class="token number">3</span><span class="token punctuation">.</span><span class="token punctuation">,</span>  <span class="token number">4</span><span class="token punctuation">.</span><span class="token punctuation">]</span><span class="token punctuation">,</span>         <span class="token punctuation">[</span> <span class="token number">7</span><span class="token punctuation">.</span><span class="token punctuation">,</span>  <span class="token number">8</span><span class="token punctuation">.</span><span class="token punctuation">]</span><span class="token punctuation">,</span>         <span class="token punctuation">[</span><span class="token number">11</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">12</span><span class="token punctuation">.</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p><code>torch.split()</code>是<code>torch.cat()</code>的逆运算，可以指定分割份数平均分割，也可以通过指定每份的记录数量进行分割。</p><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">print</span><span class="token punctuation">(</span>abc_cat<span class="token punctuation">)</span>a<span class="token punctuation">,</span>b<span class="token punctuation">,</span>c <span class="token operator">=</span> torch<span class="token punctuation">.</span>split<span class="token punctuation">(</span>abc_cat<span class="token punctuation">,</span>split_size_or_sections <span class="token operator">=</span> <span class="token number">2</span><span class="token punctuation">,</span>dim <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">)</span> <span class="token comment" spellcheck="true">#每份2个进行分割</span><span class="token keyword">print</span><span class="token punctuation">(</span>a<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>b<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>c<span class="token punctuation">)</span><span class="token comment" spellcheck="true"># 每份分别为[4,1,1]</span>p<span class="token punctuation">,</span>q<span class="token punctuation">,</span>r <span class="token operator">=</span> torch<span class="token punctuation">.</span>split<span class="token punctuation">(</span>abc_cat<span class="token punctuation">,</span>split_size_or_sections <span class="token operator">=</span><span class="token punctuation">[</span><span class="token number">4</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span>dim <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">)</span> <span class="token keyword">print</span><span class="token punctuation">(</span>p<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>q<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>r<span class="token punctuation">)</span><span class="token operator">>></span><span class="token operator">></span>Output<span class="token punctuation">:</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span> <span class="token number">1</span><span class="token punctuation">.</span><span class="token punctuation">,</span>  <span class="token number">2</span><span class="token punctuation">.</span><span class="token punctuation">]</span><span class="token punctuation">,</span>        <span class="token punctuation">[</span> <span class="token number">3</span><span class="token punctuation">.</span><span class="token punctuation">,</span>  <span class="token number">4</span><span class="token punctuation">.</span><span class="token punctuation">]</span><span class="token punctuation">,</span>        <span class="token punctuation">[</span> <span class="token number">5</span><span class="token punctuation">.</span><span class="token punctuation">,</span>  <span class="token number">6</span><span class="token punctuation">.</span><span class="token punctuation">]</span><span class="token punctuation">,</span>        <span class="token punctuation">[</span> <span class="token number">7</span><span class="token punctuation">.</span><span class="token punctuation">,</span>  <span class="token number">8</span><span class="token punctuation">.</span><span class="token punctuation">]</span><span class="token punctuation">,</span>        <span class="token punctuation">[</span> <span class="token number">9</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">10</span><span class="token punctuation">.</span><span class="token punctuation">]</span><span class="token punctuation">,</span>        <span class="token punctuation">[</span><span class="token number">11</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">12</span><span class="token punctuation">.</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">.</span><span class="token punctuation">]</span><span class="token punctuation">,</span>        <span class="token punctuation">[</span><span class="token number">3</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">.</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">5</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">6</span><span class="token punctuation">.</span><span class="token punctuation">]</span><span class="token punctuation">,</span>        <span class="token punctuation">[</span><span class="token number">7</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">8</span><span class="token punctuation">.</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span> <span class="token number">9</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">10</span><span class="token punctuation">.</span><span class="token punctuation">]</span><span class="token punctuation">,</span>        <span class="token punctuation">[</span><span class="token number">11</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">12</span><span class="token punctuation">.</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">.</span><span class="token punctuation">]</span><span class="token punctuation">,</span>        <span class="token punctuation">[</span><span class="token number">3</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">.</span><span class="token punctuation">]</span><span class="token punctuation">,</span>        <span class="token punctuation">[</span><span class="token number">5</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">6</span><span class="token punctuation">.</span><span class="token punctuation">]</span><span class="token punctuation">,</span>        <span class="token punctuation">[</span><span class="token number">7</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">8</span><span class="token punctuation">.</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span> <span class="token number">9</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">10</span><span class="token punctuation">.</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">11</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">12</span><span class="token punctuation">.</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="张量的数学运算"><a href="#张量的数学运算" class="headerlink" title="张量的数学运算"></a>张量的数学运算</h2><p>张量的数学运算符可以分为标量运算符、向量运算符、以及矩阵运算符。</p><h3 id="标量运算"><a href="#标量运算" class="headerlink" title="标量运算"></a>标量运算</h3><p>加减乘除乘方，以及三角函数，指数，对数等常见函数，逻辑比较运算符等都是标量运算符。<br><strong>标量运算符的特点是对张量实施逐元素运算</strong>。有些标量运算符对常用的数学运算符进行了重载。并且支持类似numpy的广播特性。</p><pre class="line-numbers language-python"><code class="language-python">a <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">1.0</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">,</span><span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">3</span><span class="token punctuation">,</span><span class="token number">4.0</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span>b <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">5.0</span><span class="token punctuation">,</span><span class="token number">6</span><span class="token punctuation">]</span><span class="token punctuation">,</span><span class="token punctuation">[</span><span class="token number">7.0</span><span class="token punctuation">,</span><span class="token number">8.0</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true"># 逐元素运算</span>a<span class="token operator">+</span>b  <span class="token comment" spellcheck="true">#运算符重载</span>a<span class="token operator">-</span>b a<span class="token operator">*</span>b a<span class="token operator">/</span>ba<span class="token operator">**</span><span class="token number">2</span>a<span class="token operator">**</span><span class="token number">0.5</span>a<span class="token operator">%</span><span class="token number">3</span>a<span class="token operator">//</span><span class="token number">3</span>torch<span class="token punctuation">.</span>sqrt<span class="token punctuation">(</span>a<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>a<span class="token operator">>=</span><span class="token number">2</span><span class="token punctuation">)</span> <span class="token comment" spellcheck="true"># torch.ge(a,2)  #ge: greater_equal缩写</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token punctuation">(</span>a<span class="token operator">>=</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token operator">&amp;</span><span class="token punctuation">(</span>a<span class="token operator">&lt;=</span><span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token punctuation">(</span>a<span class="token operator">>=</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token operator">|</span><span class="token punctuation">(</span>a<span class="token operator">&lt;=</span><span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>a<span class="token operator">==</span><span class="token number">5</span><span class="token punctuation">)</span> <span class="token comment" spellcheck="true"># torch.eq(a,5)  # eq: equal缩写</span><span class="token operator">>></span><span class="token operator">></span>Output<span class="token punctuation">:</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token boolean">False</span><span class="token punctuation">,</span>  <span class="token boolean">True</span><span class="token punctuation">]</span><span class="token punctuation">,</span>        <span class="token punctuation">[</span><span class="token boolean">False</span><span class="token punctuation">,</span>  <span class="token boolean">True</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token boolean">False</span><span class="token punctuation">,</span>  <span class="token boolean">True</span><span class="token punctuation">]</span><span class="token punctuation">,</span>        <span class="token punctuation">[</span><span class="token boolean">False</span><span class="token punctuation">,</span> <span class="token boolean">False</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token boolean">True</span><span class="token punctuation">,</span> <span class="token boolean">True</span><span class="token punctuation">]</span><span class="token punctuation">,</span>        <span class="token punctuation">[</span><span class="token boolean">True</span><span class="token punctuation">,</span> <span class="token boolean">True</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token boolean">False</span><span class="token punctuation">,</span> <span class="token boolean">False</span><span class="token punctuation">]</span><span class="token punctuation">,</span>        <span class="token punctuation">[</span><span class="token boolean">False</span><span class="token punctuation">,</span> <span class="token boolean">False</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><pre class="line-numbers language-python"><code class="language-python">a <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">1.0</span><span class="token punctuation">,</span><span class="token number">8.0</span><span class="token punctuation">]</span><span class="token punctuation">)</span>b <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">5.0</span><span class="token punctuation">,</span><span class="token number">6.0</span><span class="token punctuation">]</span><span class="token punctuation">)</span>c <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">6.0</span><span class="token punctuation">,</span><span class="token number">7.0</span><span class="token punctuation">]</span><span class="token punctuation">)</span>d <span class="token operator">=</span> a<span class="token operator">+</span>b<span class="token operator">+</span>c<span class="token keyword">print</span><span class="token punctuation">(</span>d<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>torch<span class="token punctuation">.</span>max<span class="token punctuation">(</span>a<span class="token punctuation">,</span>b<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>torch<span class="token punctuation">.</span>min<span class="token punctuation">(</span>a<span class="token punctuation">,</span>b<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token operator">>></span><span class="token operator">></span>Output<span class="token punctuation">:</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">12</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">21</span><span class="token punctuation">.</span><span class="token punctuation">]</span><span class="token punctuation">)</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">5</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">8</span><span class="token punctuation">.</span><span class="token punctuation">]</span><span class="token punctuation">)</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">6</span><span class="token punctuation">.</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p><code>torch.round()</code>：保留整数部分，四舍五入。<br><code>torch.floor()</code>：保留整数部分，向下取整。<br><code>torch.ceil()</code>：保留整数部分，向上取整。<br><code>torch.trunc()</code>：保留整数部分，向0归整。<br><code>torch.fmod()</code>：作除法取余数。<br><code>torch.remainder()</code>：作除法取剩余的部分，结果恒正。</p><pre class="line-numbers language-python"><code class="language-python">x <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">2.6</span><span class="token punctuation">,</span><span class="token operator">-</span><span class="token number">2.7</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>torch<span class="token punctuation">.</span>round<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token keyword">print</span><span class="token punctuation">(</span>torch<span class="token punctuation">.</span>floor<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token keyword">print</span><span class="token punctuation">(</span>torch<span class="token punctuation">.</span>ceil<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">)</span>  <span class="token keyword">print</span><span class="token punctuation">(</span>torch<span class="token punctuation">.</span>trunc<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token keyword">print</span><span class="token punctuation">(</span>torch<span class="token punctuation">.</span>fmod<span class="token punctuation">(</span>x<span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token keyword">print</span><span class="token punctuation">(</span>torch<span class="token punctuation">.</span>remainder<span class="token punctuation">(</span>x<span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token operator">>></span><span class="token operator">></span>Output<span class="token punctuation">:</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span> <span class="token number">3</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">3</span><span class="token punctuation">.</span><span class="token punctuation">]</span><span class="token punctuation">)</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span> <span class="token number">2</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">3</span><span class="token punctuation">.</span><span class="token punctuation">]</span><span class="token punctuation">)</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span> <span class="token number">3</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">2</span><span class="token punctuation">.</span><span class="token punctuation">]</span><span class="token punctuation">)</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span> <span class="token number">2</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">2</span><span class="token punctuation">.</span><span class="token punctuation">]</span><span class="token punctuation">)</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span> <span class="token number">0.6000</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">0.7000</span><span class="token punctuation">]</span><span class="token punctuation">)</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">0.6000</span><span class="token punctuation">,</span> <span class="token number">1.3000</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><pre class="line-numbers language-python"><code class="language-python">x <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">0.9</span><span class="token punctuation">,</span><span class="token operator">-</span><span class="token number">0.8</span><span class="token punctuation">,</span><span class="token number">100.0</span><span class="token punctuation">,</span><span class="token operator">-</span><span class="token number">20.0</span><span class="token punctuation">,</span><span class="token number">0.7</span><span class="token punctuation">]</span><span class="token punctuation">)</span>y <span class="token operator">=</span> torch<span class="token punctuation">.</span>clamp<span class="token punctuation">(</span>x<span class="token punctuation">,</span>min<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span>max <span class="token operator">=</span> <span class="token number">1</span><span class="token punctuation">)</span>z <span class="token operator">=</span> torch<span class="token punctuation">.</span>clamp<span class="token punctuation">(</span>x<span class="token punctuation">,</span>max <span class="token operator">=</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>y<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>z<span class="token punctuation">)</span><span class="token operator">>></span><span class="token operator">></span>Output<span class="token punctuation">:</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span> <span class="token number">0.9000</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">0.8000</span><span class="token punctuation">,</span>  <span class="token number">1.0000</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1.0000</span><span class="token punctuation">,</span>  <span class="token number">0.7000</span><span class="token punctuation">]</span><span class="token punctuation">)</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span>  <span class="token number">0.9000</span><span class="token punctuation">,</span>  <span class="token operator">-</span><span class="token number">0.8000</span><span class="token punctuation">,</span>   <span class="token number">1.0000</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">20.0000</span><span class="token punctuation">,</span>   <span class="token number">0.7000</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="向量运算"><a href="#向量运算" class="headerlink" title="向量运算"></a>向量运算</h3><p>向量运算符只在一个特定轴上运算，将一个向量映射到一个标量或者另外一个向量。</p><pre class="line-numbers language-python"><code class="language-python">a <span class="token operator">=</span> torch<span class="token punctuation">.</span>arange<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">10</span><span class="token punctuation">)</span><span class="token punctuation">.</span>float<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>torch<span class="token punctuation">.</span>sum<span class="token punctuation">(</span>a<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>torch<span class="token punctuation">.</span>mean<span class="token punctuation">(</span>a<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>torch<span class="token punctuation">.</span>max<span class="token punctuation">(</span>a<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>torch<span class="token punctuation">.</span>min<span class="token punctuation">(</span>a<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>torch<span class="token punctuation">.</span>prod<span class="token punctuation">(</span>a<span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token comment" spellcheck="true">#累乘</span><span class="token keyword">print</span><span class="token punctuation">(</span>torch<span class="token punctuation">.</span>std<span class="token punctuation">(</span>a<span class="token punctuation">)</span><span class="token punctuation">)</span>  <span class="token comment" spellcheck="true">#标准差</span><span class="token keyword">print</span><span class="token punctuation">(</span>torch<span class="token punctuation">.</span>var<span class="token punctuation">(</span>a<span class="token punctuation">)</span><span class="token punctuation">)</span>  <span class="token comment" spellcheck="true">#方差</span><span class="token keyword">print</span><span class="token punctuation">(</span>torch<span class="token punctuation">.</span>median<span class="token punctuation">(</span>a<span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token comment" spellcheck="true">#中位数</span><span class="token operator">>></span><span class="token operator">></span>Output<span class="token punctuation">:</span>tensor<span class="token punctuation">(</span><span class="token number">45</span><span class="token punctuation">.</span><span class="token punctuation">)</span>tensor<span class="token punctuation">(</span><span class="token number">5</span><span class="token punctuation">.</span><span class="token punctuation">)</span>tensor<span class="token punctuation">(</span><span class="token number">9</span><span class="token punctuation">.</span><span class="token punctuation">)</span>tensor<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">.</span><span class="token punctuation">)</span>tensor<span class="token punctuation">(</span><span class="token number">362880</span><span class="token punctuation">.</span><span class="token punctuation">)</span>tensor<span class="token punctuation">(</span><span class="token number">2.7386</span><span class="token punctuation">)</span>tensor<span class="token punctuation">(</span><span class="token number">7.5000</span><span class="token punctuation">)</span>tensor<span class="token punctuation">(</span><span class="token number">5</span><span class="token punctuation">.</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><pre class="line-numbers language-python"><code class="language-python">b <span class="token operator">=</span> a<span class="token punctuation">.</span>view<span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span><span class="token number">3</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>b<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>torch<span class="token punctuation">.</span>max<span class="token punctuation">(</span>b<span class="token punctuation">,</span>dim <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>torch<span class="token punctuation">.</span>max<span class="token punctuation">(</span>b<span class="token punctuation">,</span>dim <span class="token operator">=</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token operator">>></span><span class="token operator">></span>Output<span class="token punctuation">:</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">.</span><span class="token punctuation">]</span><span class="token punctuation">,</span>        <span class="token punctuation">[</span><span class="token number">4</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">6</span><span class="token punctuation">.</span><span class="token punctuation">]</span><span class="token punctuation">,</span>        <span class="token punctuation">[</span><span class="token number">7</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">8</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">9</span><span class="token punctuation">.</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span>torch<span class="token punctuation">.</span>return_types<span class="token punctuation">.</span>max<span class="token punctuation">(</span>values<span class="token operator">=</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">7</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">8</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">9</span><span class="token punctuation">.</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">,</span>indices<span class="token operator">=</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span>torch<span class="token punctuation">.</span>return_types<span class="token punctuation">.</span>max<span class="token punctuation">(</span>values<span class="token operator">=</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">3</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">6</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">9</span><span class="token punctuation">.</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">,</span>indices<span class="token operator">=</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><pre class="line-numbers language-python"><code class="language-python">a <span class="token operator">=</span> torch<span class="token punctuation">.</span>arange<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">10</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>torch<span class="token punctuation">.</span>cumsum<span class="token punctuation">(</span>a<span class="token punctuation">,</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>torch<span class="token punctuation">.</span>cumprod<span class="token punctuation">(</span>a<span class="token punctuation">,</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>torch<span class="token punctuation">.</span>cummax<span class="token punctuation">(</span>a<span class="token punctuation">,</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">.</span>values<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>torch<span class="token punctuation">.</span>cummax<span class="token punctuation">(</span>a<span class="token punctuation">,</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">.</span>indices<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>torch<span class="token punctuation">.</span>cummin<span class="token punctuation">(</span>a<span class="token punctuation">,</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token operator">>></span><span class="token operator">></span>Output<span class="token punctuation">:</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span> <span class="token number">1</span><span class="token punctuation">,</span>  <span class="token number">3</span><span class="token punctuation">,</span>  <span class="token number">6</span><span class="token punctuation">,</span> <span class="token number">10</span><span class="token punctuation">,</span> <span class="token number">15</span><span class="token punctuation">,</span> <span class="token number">21</span><span class="token punctuation">,</span> <span class="token number">28</span><span class="token punctuation">,</span> <span class="token number">36</span><span class="token punctuation">,</span> <span class="token number">45</span><span class="token punctuation">]</span><span class="token punctuation">)</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span>     <span class="token number">1</span><span class="token punctuation">,</span>      <span class="token number">2</span><span class="token punctuation">,</span>      <span class="token number">6</span><span class="token punctuation">,</span>     <span class="token number">24</span><span class="token punctuation">,</span>    <span class="token number">120</span><span class="token punctuation">,</span>    <span class="token number">720</span><span class="token punctuation">,</span>   <span class="token number">5040</span><span class="token punctuation">,</span>  <span class="token number">40320</span><span class="token punctuation">,</span> <span class="token number">362880</span><span class="token punctuation">]</span><span class="token punctuation">)</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">,</span> <span class="token number">6</span><span class="token punctuation">,</span> <span class="token number">7</span><span class="token punctuation">,</span> <span class="token number">8</span><span class="token punctuation">,</span> <span class="token number">9</span><span class="token punctuation">]</span><span class="token punctuation">)</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">,</span> <span class="token number">6</span><span class="token punctuation">,</span> <span class="token number">7</span><span class="token punctuation">,</span> <span class="token number">8</span><span class="token punctuation">]</span><span class="token punctuation">)</span>torch<span class="token punctuation">.</span>return_types<span class="token punctuation">.</span>cummin<span class="token punctuation">(</span>values<span class="token operator">=</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">,</span>indices<span class="token operator">=</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p><code>torch.sort()</code>和<code>torch.topk()</code>可以对张量排序。利用<code>torch.topk()</code>可以在Pytorch中实现KNN算法。</p><pre class="line-numbers language-python"><code class="language-python">a <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">9</span><span class="token punctuation">,</span><span class="token number">7</span><span class="token punctuation">,</span><span class="token number">8</span><span class="token punctuation">]</span><span class="token punctuation">,</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">3</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">,</span><span class="token punctuation">[</span><span class="token number">5</span><span class="token punctuation">,</span><span class="token number">6</span><span class="token punctuation">,</span><span class="token number">4</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">.</span>float<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>torch<span class="token punctuation">.</span>topk<span class="token punctuation">(</span>a<span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">,</span>dim <span class="token operator">=</span> <span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">,</span><span class="token string">"\n"</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>torch<span class="token punctuation">.</span>topk<span class="token punctuation">(</span>a<span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">,</span>dim <span class="token operator">=</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span><span class="token string">"\n"</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>torch<span class="token punctuation">.</span>sort<span class="token punctuation">(</span>a<span class="token punctuation">,</span>dim <span class="token operator">=</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span><span class="token string">"\n"</span><span class="token punctuation">)</span><span class="token operator">>></span><span class="token operator">></span>Output<span class="token punctuation">:</span>torch<span class="token punctuation">.</span>return_types<span class="token punctuation">.</span>topk<span class="token punctuation">(</span>values<span class="token operator">=</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">9</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">7</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">8</span><span class="token punctuation">.</span><span class="token punctuation">]</span><span class="token punctuation">,</span>        <span class="token punctuation">[</span><span class="token number">5</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">6</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">.</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">,</span>indices<span class="token operator">=</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span>        <span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>return_types<span class="token punctuation">.</span>topk<span class="token punctuation">(</span>values<span class="token operator">=</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">9</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">8</span><span class="token punctuation">.</span><span class="token punctuation">]</span><span class="token punctuation">,</span>        <span class="token punctuation">[</span><span class="token number">3</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">.</span><span class="token punctuation">]</span><span class="token punctuation">,</span>        <span class="token punctuation">[</span><span class="token number">6</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">.</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">,</span>indices<span class="token operator">=</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">,</span>        <span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">,</span>        <span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span> torch<span class="token punctuation">.</span>return_types<span class="token punctuation">.</span>sort<span class="token punctuation">(</span>values<span class="token operator">=</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">7</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">8</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">9</span><span class="token punctuation">.</span><span class="token punctuation">]</span><span class="token punctuation">,</span>        <span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">.</span><span class="token punctuation">]</span><span class="token punctuation">,</span>        <span class="token punctuation">[</span><span class="token number">4</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">6</span><span class="token punctuation">.</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">,</span>indices<span class="token operator">=</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span>        <span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span>        <span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="矩阵运算"><a href="#矩阵运算" class="headerlink" title="矩阵运算"></a>矩阵运算</h3><p>矩阵必须是二维的。<br>矩阵运算包括：矩阵乘法，矩阵转置，矩阵逆，矩阵求迹，矩阵范数，矩阵行列式，矩阵求特征值，矩阵分解等运算。</p><pre class="line-numbers language-python"><code class="language-python">a <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">,</span><span class="token punctuation">[</span><span class="token number">3</span><span class="token punctuation">,</span><span class="token number">4</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span>b <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true"># 下列均等价</span>torch<span class="token punctuation">.</span>matmul<span class="token punctuation">(</span>a<span class="token punctuation">,</span>b<span class="token punctuation">)</span>torch<span class="token punctuation">.</span>mm<span class="token punctuation">(</span>a<span class="token punctuation">,</span>b<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>a@b<span class="token punctuation">)</span><span class="token operator">>></span><span class="token operator">></span>Output<span class="token punctuation">:</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">]</span><span class="token punctuation">,</span>        <span class="token punctuation">[</span><span class="token number">6</span><span class="token punctuation">,</span> <span class="token number">8</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><pre class="line-numbers language-python"><code class="language-python">a <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">1.0</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">,</span><span class="token punctuation">[</span><span class="token number">3</span><span class="token punctuation">,</span><span class="token number">4</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>a<span class="token punctuation">.</span>t<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token operator">>></span><span class="token operator">></span>Output<span class="token punctuation">:</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">.</span><span class="token punctuation">]</span><span class="token punctuation">,</span>        <span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">.</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">.</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>矩阵求逆必须是浮点数类型。</p><pre class="line-numbers language-python"><code class="language-python"><span class="token comment" spellcheck="true"># 必须为浮点类型</span>a <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">1.0</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">,</span><span class="token punctuation">[</span><span class="token number">3</span><span class="token punctuation">,</span><span class="token number">4</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>torch<span class="token punctuation">.</span>inverse<span class="token punctuation">(</span>a<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token operator">>></span><span class="token operator">></span>Output<span class="token punctuation">:</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">2.0000</span><span class="token punctuation">,</span>  <span class="token number">1.0000</span><span class="token punctuation">]</span><span class="token punctuation">,</span>        <span class="token punctuation">[</span> <span class="token number">1.5000</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">0.5000</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><pre class="line-numbers language-python"><code class="language-python">a <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">1.0</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">,</span><span class="token punctuation">[</span><span class="token number">3</span><span class="token punctuation">,</span><span class="token number">4</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>torch<span class="token punctuation">.</span>trace<span class="token punctuation">(</span>a<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token operator">>></span><span class="token operator">></span>Output<span class="token punctuation">:</span>tensor<span class="token punctuation">(</span><span class="token number">5</span><span class="token punctuation">.</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><pre class="line-numbers language-python"><code class="language-python">a <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">1.0</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">,</span><span class="token punctuation">[</span><span class="token number">3</span><span class="token punctuation">,</span><span class="token number">4</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>torch<span class="token punctuation">.</span>norm<span class="token punctuation">(</span>a<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token operator">>></span><span class="token operator">></span>Output<span class="token punctuation">:</span>tensor<span class="token punctuation">(</span><span class="token number">5.4772</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><pre class="line-numbers language-python"><code class="language-python">a <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">1.0</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">,</span><span class="token punctuation">[</span><span class="token number">3</span><span class="token punctuation">,</span><span class="token number">4</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>torch<span class="token punctuation">.</span>det<span class="token punctuation">(</span>a<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token operator">>></span><span class="token operator">></span>Output<span class="token punctuation">:</span>tensor<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">2.0000</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><pre class="line-numbers language-python"><code class="language-python">a <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">1.0</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">,</span><span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">5</span><span class="token punctuation">,</span><span class="token number">4</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">,</span>dtype <span class="token operator">=</span> torch<span class="token punctuation">.</span>float<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>torch<span class="token punctuation">.</span>eig<span class="token punctuation">(</span>a<span class="token punctuation">,</span>eigenvectors<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true"># 两个特征值分别是 -2.5+2.7839j, 2.5-2.7839j </span><span class="token operator">>></span><span class="token operator">></span>Output<span class="token punctuation">:</span>torch<span class="token punctuation">.</span>return_types<span class="token punctuation">.</span>eig<span class="token punctuation">(</span>eigenvalues<span class="token operator">=</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span> <span class="token number">2.5000</span><span class="token punctuation">,</span>  <span class="token number">2.7839</span><span class="token punctuation">]</span><span class="token punctuation">,</span>        <span class="token punctuation">[</span> <span class="token number">2.5000</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">2.7839</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">,</span>eigenvectors<span class="token operator">=</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span> <span class="token number">0.2535</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">0.4706</span><span class="token punctuation">]</span><span class="token punctuation">,</span>        <span class="token punctuation">[</span> <span class="token number">0.8452</span><span class="token punctuation">,</span>  <span class="token number">0.0000</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>矩阵QR分解，将一个方阵分解为一个正交矩阵q和上三角矩阵r。<br>QR分解实际上是对矩阵a实施Schmidt正交化得到q。</p><pre class="line-numbers language-python"><code class="language-python">a <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">1.0</span><span class="token punctuation">,</span><span class="token number">2.0</span><span class="token punctuation">]</span><span class="token punctuation">,</span><span class="token punctuation">[</span><span class="token number">3.0</span><span class="token punctuation">,</span><span class="token number">4.0</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span>q<span class="token punctuation">,</span>r <span class="token operator">=</span> torch<span class="token punctuation">.</span>qr<span class="token punctuation">(</span>a<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>q<span class="token punctuation">,</span><span class="token string">"\n"</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>r<span class="token punctuation">,</span><span class="token string">"\n"</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>q@r<span class="token punctuation">)</span><span class="token operator">>></span><span class="token operator">></span>Output<span class="token punctuation">:</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">0.3162</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">0.9487</span><span class="token punctuation">]</span><span class="token punctuation">,</span>        <span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">0.9487</span><span class="token punctuation">,</span>  <span class="token number">0.3162</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span> tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">3.1623</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">4.4272</span><span class="token punctuation">]</span><span class="token punctuation">,</span>        <span class="token punctuation">[</span> <span class="token number">0.0000</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">0.6325</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span> tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">1.0000</span><span class="token punctuation">,</span> <span class="token number">2.0000</span><span class="token punctuation">]</span><span class="token punctuation">,</span>        <span class="token punctuation">[</span><span class="token number">3.0000</span><span class="token punctuation">,</span> <span class="token number">4.0000</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>矩阵svd分解，svd分解可以将任意一个矩阵分解为一个正交矩阵u，一个对角阵s和一个正交矩阵<code>v.t()</code>的乘积。svd常用于矩阵压缩和降维，利用svd分解可以在Pytorch中实现主成分分析降维。</p><pre class="line-numbers language-python"><code class="language-python">a<span class="token operator">=</span>torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">1.0</span><span class="token punctuation">,</span><span class="token number">2.0</span><span class="token punctuation">]</span><span class="token punctuation">,</span><span class="token punctuation">[</span><span class="token number">3.0</span><span class="token punctuation">,</span><span class="token number">4.0</span><span class="token punctuation">]</span><span class="token punctuation">,</span><span class="token punctuation">[</span><span class="token number">5.0</span><span class="token punctuation">,</span><span class="token number">6.0</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span>u<span class="token punctuation">,</span>s<span class="token punctuation">,</span>v <span class="token operator">=</span> torch<span class="token punctuation">.</span>svd<span class="token punctuation">(</span>a<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>u<span class="token punctuation">,</span><span class="token string">"\n"</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>s<span class="token punctuation">,</span><span class="token string">"\n"</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>v<span class="token punctuation">,</span><span class="token string">"\n"</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>u@torch<span class="token punctuation">.</span>diag<span class="token punctuation">(</span>s<span class="token punctuation">)</span>@v<span class="token punctuation">.</span>t<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token operator">>></span><span class="token operator">></span>Output<span class="token punctuation">:</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">0.2298</span><span class="token punctuation">,</span>  <span class="token number">0.8835</span><span class="token punctuation">]</span><span class="token punctuation">,</span>        <span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">0.5247</span><span class="token punctuation">,</span>  <span class="token number">0.2408</span><span class="token punctuation">]</span><span class="token punctuation">,</span>        <span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">0.8196</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">0.4019</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span> tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">9.5255</span><span class="token punctuation">,</span> <span class="token number">0.5143</span><span class="token punctuation">]</span><span class="token punctuation">)</span> tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">0.6196</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">0.7849</span><span class="token punctuation">]</span><span class="token punctuation">,</span>        <span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">0.7849</span><span class="token punctuation">,</span>  <span class="token number">0.6196</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span> tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">1.0000</span><span class="token punctuation">,</span> <span class="token number">2.0000</span><span class="token punctuation">]</span><span class="token punctuation">,</span>        <span class="token punctuation">[</span><span class="token number">3.0000</span><span class="token punctuation">,</span> <span class="token number">4.0000</span><span class="token punctuation">]</span><span class="token punctuation">,</span>        <span class="token punctuation">[</span><span class="token number">5.0000</span><span class="token punctuation">,</span> <span class="token number">6.0000</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="广播机制"><a href="#广播机制" class="headerlink" title="广播机制"></a>广播机制</h3><p>Pytorch的广播规则和numpy是一样的:</p><ol><li>如果张量的维度不同，将维度较小的张量进行扩展，直到两个张量的维度都一样。</li><li>如果两个张量在某个维度上的长度是相同的，或者其中一个张量在该维度上的长度为1，那么我们就说这两个张量在该维度上是相容的。</li><li>如果两个张量在所有维度上都是相容的，它们就能使用广播。</li><li>广播之后，每个维度的长度将取两个张量在该维度长度的较大值。</li><li>在任何一个维度上，如果一个张量的长度为1，另一个张量长度大于1，那么在该维度上，就好像是对第一个张量进行了复制。</li></ol><p><code>torch.broadcast_tensors()</code>可以将多个张量根据广播规则转换成相同的维度。<br></p><pre class="line-numbers language-python"><code class="language-python">a <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">3</span><span class="token punctuation">]</span><span class="token punctuation">)</span>b <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span><span class="token number">0</span><span class="token punctuation">,</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span><span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>b <span class="token operator">+</span> a<span class="token punctuation">)</span> a_broad<span class="token punctuation">,</span>b_broad <span class="token operator">=</span> torch<span class="token punctuation">.</span>broadcast_tensors<span class="token punctuation">(</span>a<span class="token punctuation">,</span>b<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>a_broad<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>b_broad<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>a_broad <span class="token operator">+</span> b_broad<span class="token punctuation">)</span> <span class="token operator">>></span><span class="token operator">></span>Output<span class="token punctuation">:</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">]</span><span class="token punctuation">,</span>        <span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">]</span><span class="token punctuation">,</span>        <span class="token punctuation">[</span><span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">]</span><span class="token punctuation">,</span>        <span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">]</span><span class="token punctuation">,</span>        <span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span> tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span>        <span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span>        <span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span> tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">]</span><span class="token punctuation">,</span>        <span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">]</span><span class="token punctuation">,</span>        <span class="token punctuation">[</span><span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">]</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p></p>]]></content>
      
      
      <categories>
          
          <category> Pytorch </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 基础知识 </tag>
            
            <tag> pytorch </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Pytorch训练代码模板</title>
      <link href="/2023/03/23/shen-du-xue-xi-xun-lian-dai-ma-mo-ban/"/>
      <url>/2023/03/23/shen-du-xue-xi-xun-lian-dai-ma-mo-ban/</url>
      
        <content type="html"><![CDATA[<p>从参数定义，到网络模型定义，再到训练步骤，验证步骤，测试步骤，总结了一套较为直观的模板。目录如下：</p><ol><li>导入包以及设置随机种子</li><li>以类的方式定义超参数</li><li>定义自己的模型</li><li>定义早停类(此步骤可以省略)</li><li>定义自己的数据集Dataset,DataLoader</li><li>实例化模型，设置loss，优化器等</li><li>开始训练以及调整lr</li><li>绘图</li><li>预测<h1 id="导入包以及设置随机种子"><a href="#导入包以及设置随机种子" class="headerlink" title="导入包以及设置随机种子"></a>导入包以及设置随机种子</h1><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">import</span> numpy <span class="token keyword">as</span> np<span class="token keyword">import</span> torch<span class="token keyword">import</span> torch<span class="token punctuation">.</span>nn <span class="token keyword">as</span> nn<span class="token keyword">import</span> pandas <span class="token keyword">as</span> pd<span class="token keyword">from</span> torch<span class="token punctuation">.</span>utils<span class="token punctuation">.</span>data <span class="token keyword">import</span> DataLoader<span class="token punctuation">,</span> Dataset<span class="token keyword">from</span> sklearn<span class="token punctuation">.</span>model_selection <span class="token keyword">import</span> train_test_split<span class="token keyword">import</span> matplotlib<span class="token punctuation">.</span>pyplot <span class="token keyword">as</span> plt<span class="token keyword">import</span> random<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></li></ol><p>seed = 42<br>random.seed(seed)  # seed for module random<br>np.random.seed(seed)  # seed for numpy<br>torch.manual_seed(seed)  # seed for PyTorch CPU<br>torch.cuda.manual_seed(seed)  # seed for current PyTorch GPU<br>torch.cuda.manual_seed_all(seed)  # seed for all PyTorch GPUs</p><pre><code># **以类的方式定义超参数**```pythonclass argparse():    passargs = argparse()args.epochs, args.learning_rate, args.patience = [30, 0.001, 4]args.hidden_size, args.input_size= [40, 30]args.device, = [torch.device(&quot;cuda:0&quot; if torch.cuda.is_available() else &quot;cpu&quot;),]</code></pre><h1 id="定义模型"><a href="#定义模型" class="headerlink" title="定义模型"></a><strong>定义模型</strong></h1><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">Your_model</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>        super<span class="token punctuation">(</span>Your_model<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>        <span class="token keyword">pass</span>    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span>x<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token keyword">pass</span>        <span class="token keyword">return</span> x<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h1 id="定义早停类-此步骤可以省略"><a href="#定义早停类-此步骤可以省略" class="headerlink" title="定义早停类(此步骤可以省略)"></a><strong>定义早停类(此步骤可以省略)</strong></h1><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">EarlyStopping</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> patience<span class="token operator">=</span><span class="token number">7</span><span class="token punctuation">,</span> verbose<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span> delta<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">:</span>        self<span class="token punctuation">.</span>patience <span class="token operator">=</span> patience        self<span class="token punctuation">.</span>verbose <span class="token operator">=</span> verbose        self<span class="token punctuation">.</span>counter <span class="token operator">=</span> <span class="token number">0</span>        self<span class="token punctuation">.</span>best_score <span class="token operator">=</span> None        self<span class="token punctuation">.</span>early_stop <span class="token operator">=</span> <span class="token boolean">False</span>        self<span class="token punctuation">.</span>val_loss_min <span class="token operator">=</span> np<span class="token punctuation">.</span>Inf        self<span class="token punctuation">.</span>delta <span class="token operator">=</span> delta    <span class="token keyword">def</span> <span class="token function">__call__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> val_loss<span class="token punctuation">,</span> model<span class="token punctuation">,</span> path<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"val_loss={}"</span><span class="token punctuation">.</span>format<span class="token punctuation">(</span>val_loss<span class="token punctuation">)</span><span class="token punctuation">)</span>        score <span class="token operator">=</span> <span class="token operator">-</span>val_loss        <span class="token keyword">if</span> self<span class="token punctuation">.</span>best_score <span class="token keyword">is</span> None<span class="token punctuation">:</span>            self<span class="token punctuation">.</span>best_score <span class="token operator">=</span> score            self<span class="token punctuation">.</span>save_checkpoint<span class="token punctuation">(</span>val_loss<span class="token punctuation">,</span> model<span class="token punctuation">,</span>path<span class="token punctuation">)</span>        <span class="token keyword">elif</span> score <span class="token operator">&lt;</span> self<span class="token punctuation">.</span>best_score<span class="token operator">+</span>self<span class="token punctuation">.</span>delta<span class="token punctuation">:</span>            self<span class="token punctuation">.</span>counter<span class="token operator">+=</span><span class="token number">1</span>            <span class="token keyword">print</span><span class="token punctuation">(</span>f<span class="token string">'EarlyStopping counter: {self.counter} out of {self.patience}'</span><span class="token punctuation">)</span>            <span class="token keyword">if</span> self<span class="token punctuation">.</span>counter<span class="token operator">>=</span>self<span class="token punctuation">.</span>patience<span class="token punctuation">:</span>                self<span class="token punctuation">.</span>early_stop <span class="token operator">=</span> <span class="token boolean">True</span>        <span class="token keyword">else</span><span class="token punctuation">:</span>            self<span class="token punctuation">.</span>best_score <span class="token operator">=</span> score            self<span class="token punctuation">.</span>save_checkpoint<span class="token punctuation">(</span>val_loss<span class="token punctuation">,</span> model<span class="token punctuation">,</span>path<span class="token punctuation">)</span>            self<span class="token punctuation">.</span>counter <span class="token operator">=</span> <span class="token number">0</span>    <span class="token keyword">def</span> <span class="token function">save_checkpoint</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> val_loss<span class="token punctuation">,</span>model<span class="token punctuation">,</span> path<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token keyword">if</span> self<span class="token punctuation">.</span>verbose<span class="token punctuation">:</span>            <span class="token keyword">print</span><span class="token punctuation">(</span>f<span class="token string">'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...'</span><span class="token punctuation">)</span>        torch<span class="token punctuation">.</span>save<span class="token punctuation">(</span>model<span class="token punctuation">.</span>state_dict<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> path<span class="token operator">+</span><span class="token string">'/'</span><span class="token operator">+</span><span class="token string">'model_checkpoint.pth'</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>val_loss_min <span class="token operator">=</span> val_loss<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h1 id="定义数据集Dataset-DataLoader"><a href="#定义数据集Dataset-DataLoader" class="headerlink" title="定义数据集Dataset,DataLoader"></a><strong>定义数据集Dataset,DataLoader</strong></h1><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">Dataset_name</span><span class="token punctuation">(</span>Dataset<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> flag<span class="token operator">=</span><span class="token string">'train'</span><span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token keyword">assert</span> flag <span class="token keyword">in</span> <span class="token punctuation">[</span><span class="token string">'train'</span><span class="token punctuation">,</span> <span class="token string">'test'</span><span class="token punctuation">,</span> <span class="token string">'valid'</span><span class="token punctuation">]</span>        self<span class="token punctuation">.</span>flag <span class="token operator">=</span> flag        self<span class="token punctuation">.</span>__load_data__<span class="token punctuation">(</span><span class="token punctuation">)</span>    <span class="token keyword">def</span> <span class="token function">__getitem__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> index<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token keyword">pass</span>    <span class="token keyword">def</span> <span class="token function">__len__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token keyword">pass</span>    <span class="token keyword">def</span> <span class="token function">__load_data__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> csv_paths<span class="token punctuation">:</span> list<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token keyword">pass</span>        <span class="token keyword">print</span><span class="token punctuation">(</span>            <span class="token string">"train_X.shape:{}\ntrain_Y.shape:{}\nvalid_X.shape:{}\nvalid_Y.shape:{}\n"</span>            <span class="token punctuation">.</span>format<span class="token punctuation">(</span>self<span class="token punctuation">.</span>train_X<span class="token punctuation">.</span>shape<span class="token punctuation">,</span> self<span class="token punctuation">.</span>train_Y<span class="token punctuation">.</span>shape<span class="token punctuation">,</span> self<span class="token punctuation">.</span>valid_X<span class="token punctuation">.</span>shape<span class="token punctuation">,</span> self<span class="token punctuation">.</span>valid_Y<span class="token punctuation">.</span>shape<span class="token punctuation">)</span><span class="token punctuation">)</span>train_dataset <span class="token operator">=</span> Dataset_name<span class="token punctuation">(</span>flag<span class="token operator">=</span><span class="token string">'train'</span><span class="token punctuation">)</span>train_dataloader <span class="token operator">=</span> DataLoader<span class="token punctuation">(</span>dataset<span class="token operator">=</span>train_dataset<span class="token punctuation">,</span> batch_size<span class="token operator">=</span><span class="token number">64</span><span class="token punctuation">,</span> shuffle<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>valid_dataset <span class="token operator">=</span> Dataset_name<span class="token punctuation">(</span>flag<span class="token operator">=</span><span class="token string">'valid'</span><span class="token punctuation">)</span>valid_dataloader <span class="token operator">=</span> DataLoader<span class="token punctuation">(</span>dataset<span class="token operator">=</span>valid_dataset<span class="token punctuation">,</span> batch_size<span class="token operator">=</span><span class="token number">64</span><span class="token punctuation">,</span> shuffle<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h1 id="实例化模型，设置loss，优化器等"><a href="#实例化模型，设置loss，优化器等" class="headerlink" title="实例化模型，设置loss，优化器等"></a><strong>实例化模型，设置loss，优化器等</strong></h1><pre class="line-numbers language-python"><code class="language-python">model <span class="token operator">=</span> Your_model<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>to<span class="token punctuation">(</span>args<span class="token punctuation">.</span>device<span class="token punctuation">)</span>criterion <span class="token operator">=</span> torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>MSELoss<span class="token punctuation">(</span><span class="token punctuation">)</span>optimizer <span class="token operator">=</span> torch<span class="token punctuation">.</span>optim<span class="token punctuation">.</span>Adam<span class="token punctuation">(</span>Your_model<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> lr<span class="token operator">=</span>args<span class="token punctuation">.</span>learning_rate<span class="token punctuation">)</span>train_loss <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>valid_loss <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>train_epochs_loss <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>valid_epochs_loss <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>early_stopping <span class="token operator">=</span> EarlyStopping<span class="token punctuation">(</span>patience<span class="token operator">=</span>args<span class="token punctuation">.</span>patience<span class="token punctuation">,</span> verbose<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h1 id="开始训练以及调整lr"><a href="#开始训练以及调整lr" class="headerlink" title="开始训练以及调整lr"></a><strong>开始训练以及调整lr</strong></h1><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">for</span> epoch <span class="token keyword">in</span> range<span class="token punctuation">(</span>args<span class="token punctuation">.</span>epochs<span class="token punctuation">)</span><span class="token punctuation">:</span>    Your_model<span class="token punctuation">.</span>train<span class="token punctuation">(</span><span class="token punctuation">)</span>    train_epoch_loss <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>    <span class="token keyword">for</span> idx<span class="token punctuation">,</span><span class="token punctuation">(</span>data_x<span class="token punctuation">,</span>data_y<span class="token punctuation">)</span> <span class="token keyword">in</span> enumerate<span class="token punctuation">(</span>train_dataloader<span class="token punctuation">,</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">:</span>        data_x <span class="token operator">=</span> data_x<span class="token punctuation">.</span>to<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>float32<span class="token punctuation">)</span><span class="token punctuation">.</span>to<span class="token punctuation">(</span>args<span class="token punctuation">.</span>device<span class="token punctuation">)</span>        data_y <span class="token operator">=</span> data_y<span class="token punctuation">.</span>to<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>float32<span class="token punctuation">)</span><span class="token punctuation">.</span>to<span class="token punctuation">(</span>args<span class="token punctuation">.</span>device<span class="token punctuation">)</span>        outputs <span class="token operator">=</span> Your_model<span class="token punctuation">(</span>data_x<span class="token punctuation">)</span>        optimizer<span class="token punctuation">.</span>zero_grad<span class="token punctuation">(</span><span class="token punctuation">)</span>        loss <span class="token operator">=</span> criterion<span class="token punctuation">(</span>data_y<span class="token punctuation">,</span> outputs<span class="token punctuation">)</span>        loss<span class="token punctuation">.</span>backward<span class="token punctuation">(</span><span class="token punctuation">)</span>        optimizer<span class="token punctuation">.</span>step<span class="token punctuation">(</span><span class="token punctuation">)</span>        train_epoch_loss<span class="token punctuation">.</span>append<span class="token punctuation">(</span>loss<span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>        train_loss<span class="token punctuation">.</span>append<span class="token punctuation">(</span>loss<span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>        <span class="token keyword">if</span> idx<span class="token operator">%</span><span class="token punctuation">(</span>len<span class="token punctuation">(</span>train_dataloader<span class="token punctuation">)</span><span class="token operator">//</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token operator">==</span><span class="token number">0</span><span class="token punctuation">:</span>            <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"epoch={}/{},{}/{}of train, loss={}"</span><span class="token punctuation">.</span>format<span class="token punctuation">(</span>                epoch<span class="token punctuation">,</span> args<span class="token punctuation">.</span>epochs<span class="token punctuation">,</span> idx<span class="token punctuation">,</span> len<span class="token punctuation">(</span>train_dataloader<span class="token punctuation">)</span><span class="token punctuation">,</span> loss<span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>    train_epochs_loss<span class="token punctuation">.</span>append<span class="token punctuation">(</span>np<span class="token punctuation">.</span>average<span class="token punctuation">(</span>train_epoch_loss<span class="token punctuation">)</span><span class="token punctuation">)</span>    <span class="token comment" spellcheck="true">#=====================valid============================</span>    Your_model<span class="token punctuation">.</span>eval<span class="token punctuation">(</span><span class="token punctuation">)</span>    valid_epoch_loss <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>    <span class="token keyword">for</span> idx<span class="token punctuation">,</span><span class="token punctuation">(</span>data_x<span class="token punctuation">,</span>data_y<span class="token punctuation">)</span> <span class="token keyword">in</span> enumerate<span class="token punctuation">(</span>valid_dataloader<span class="token punctuation">,</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">:</span>        data_x <span class="token operator">=</span> data_x<span class="token punctuation">.</span>to<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>float32<span class="token punctuation">)</span><span class="token punctuation">.</span>to<span class="token punctuation">(</span>args<span class="token punctuation">.</span>device<span class="token punctuation">)</span>        data_y <span class="token operator">=</span> data_y<span class="token punctuation">.</span>to<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>float32<span class="token punctuation">)</span><span class="token punctuation">.</span>to<span class="token punctuation">(</span>args<span class="token punctuation">.</span>device<span class="token punctuation">)</span>        outputs <span class="token operator">=</span> Your_model<span class="token punctuation">(</span>data_x<span class="token punctuation">)</span>        loss <span class="token operator">=</span> criterion<span class="token punctuation">(</span>outputs<span class="token punctuation">,</span>data_y<span class="token punctuation">)</span>        valid_epoch_loss<span class="token punctuation">.</span>append<span class="token punctuation">(</span>loss<span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>        valid_loss<span class="token punctuation">.</span>append<span class="token punctuation">(</span>loss<span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>    valid_epochs_loss<span class="token punctuation">.</span>append<span class="token punctuation">(</span>np<span class="token punctuation">.</span>average<span class="token punctuation">(</span>valid_epoch_loss<span class="token punctuation">)</span><span class="token punctuation">)</span>    <span class="token comment" spellcheck="true">#==================early stopping======================</span>    early_stopping<span class="token punctuation">(</span>valid_epochs_loss<span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span> model<span class="token operator">=</span>Your_model<span class="token punctuation">,</span> path<span class="token operator">=</span>r<span class="token string">'c:\\your_model_to_save'</span><span class="token punctuation">)</span>    <span class="token keyword">if</span> early_stopping<span class="token punctuation">.</span>early_stop<span class="token punctuation">:</span>        <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"Early stopping"</span><span class="token punctuation">)</span>        <span class="token keyword">break</span>    <span class="token comment" spellcheck="true">#====================adjust lr========================</span>    lr_adjust <span class="token operator">=</span> <span class="token punctuation">{</span>            <span class="token number">2</span><span class="token punctuation">:</span> <span class="token number">5e</span><span class="token operator">-</span><span class="token number">5</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">:</span> <span class="token number">1e</span><span class="token operator">-</span><span class="token number">5</span><span class="token punctuation">,</span> <span class="token number">6</span><span class="token punctuation">:</span> <span class="token number">5e</span><span class="token operator">-</span><span class="token number">6</span><span class="token punctuation">,</span> <span class="token number">8</span><span class="token punctuation">:</span> <span class="token number">1e</span><span class="token operator">-</span><span class="token number">6</span><span class="token punctuation">,</span>            <span class="token number">10</span><span class="token punctuation">:</span> <span class="token number">5e</span><span class="token operator">-</span><span class="token number">7</span><span class="token punctuation">,</span> <span class="token number">15</span><span class="token punctuation">:</span> <span class="token number">1e</span><span class="token operator">-</span><span class="token number">7</span><span class="token punctuation">,</span> <span class="token number">20</span><span class="token punctuation">:</span> <span class="token number">5e</span><span class="token operator">-</span><span class="token number">8</span>        <span class="token punctuation">}</span>    <span class="token keyword">if</span> epoch <span class="token keyword">in</span> lr_adjust<span class="token punctuation">.</span>keys<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>        lr <span class="token operator">=</span> lr_adjust<span class="token punctuation">[</span>epoch<span class="token punctuation">]</span>        <span class="token keyword">for</span> param_group <span class="token keyword">in</span> optimizer<span class="token punctuation">.</span>param_groups<span class="token punctuation">:</span>            param_group<span class="token punctuation">[</span><span class="token string">'lr'</span><span class="token punctuation">]</span> <span class="token operator">=</span> lr        <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'Updating learning rate to {}'</span><span class="token punctuation">.</span>format<span class="token punctuation">(</span>lr<span class="token punctuation">)</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h1 id="绘图"><a href="#绘图" class="headerlink" title="绘图"></a><strong>绘图</strong></h1><pre class="line-numbers language-python"><code class="language-python">plt<span class="token punctuation">.</span>figure<span class="token punctuation">(</span>figsize<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">12</span><span class="token punctuation">,</span><span class="token number">4</span><span class="token punctuation">)</span><span class="token punctuation">)</span>plt<span class="token punctuation">.</span>subplot<span class="token punctuation">(</span><span class="token number">121</span><span class="token punctuation">)</span>plt<span class="token punctuation">.</span>plot<span class="token punctuation">(</span>train_loss<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">]</span><span class="token punctuation">)</span>plt<span class="token punctuation">.</span>title<span class="token punctuation">(</span><span class="token string">"train_loss"</span><span class="token punctuation">)</span>plt<span class="token punctuation">.</span>subplot<span class="token punctuation">(</span><span class="token number">122</span><span class="token punctuation">)</span>plt<span class="token punctuation">.</span>plot<span class="token punctuation">(</span>train_epochs_loss<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">:</span><span class="token punctuation">]</span><span class="token punctuation">,</span><span class="token string">'-o'</span><span class="token punctuation">,</span>label<span class="token operator">=</span><span class="token string">"train_loss"</span><span class="token punctuation">)</span>plt<span class="token punctuation">.</span>plot<span class="token punctuation">(</span>valid_epochs_loss<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">:</span><span class="token punctuation">]</span><span class="token punctuation">,</span><span class="token string">'-o'</span><span class="token punctuation">,</span>label<span class="token operator">=</span><span class="token string">"valid_loss"</span><span class="token punctuation">)</span>plt<span class="token punctuation">.</span>title<span class="token punctuation">(</span><span class="token string">"epochs_loss"</span><span class="token punctuation">)</span>plt<span class="token punctuation">.</span>legend<span class="token punctuation">(</span><span class="token punctuation">)</span>plt<span class="token punctuation">.</span>show<span class="token punctuation">(</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h1 id="预测"><a href="#预测" class="headerlink" title="预测"></a>预测</h1><pre class="line-numbers language-python"><code class="language-python"><span class="token comment" spellcheck="true"># 此处可定义一个预测集的Dataloader。也可以直接将你的预测数据reshape,添加batch_size=1</span>Your_model<span class="token punctuation">.</span>eval<span class="token punctuation">(</span><span class="token punctuation">)</span>predict <span class="token operator">=</span> Your_model<span class="token punctuation">(</span>data<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre>]]></content>
      
      
      <categories>
          
          <category> Pytorch </category>
          
      </categories>
      
      
        <tags>
            
            <tag> pytorch </tag>
            
            <tag> 训练代码模板 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Pytorch保存和加载模型&amp;断点训练</title>
      <link href="/2023/03/23/bao-cun-he-jia-zai-mo-xing-duan-dian-ji-xu-xun-lian/"/>
      <url>/2023/03/23/bao-cun-he-jia-zai-mo-xing-duan-dian-ji-xu-xun-lian/</url>
      
        <content type="html"><![CDATA[<h1 id="三个重要函数"><a href="#三个重要函数" class="headerlink" title="三个重要函数"></a>三个重要函数</h1><p>Pytorch保存和加载模型需要掌握3个重要的函数：</p><ol><li><strong>torch.save：</strong> 将一个序列化的对象保存到磁盘。这个函数使用 Python 的 pickle 工具进行序列化。用这个函数可以保存<strong>模型 (model)</strong>、<strong>张量 (tensor)</strong> 和<strong>各种对象的字典 (dict)</strong> 。</li><li><strong>torch.load：</strong> 将 pickle 对象文件反序列化到内存，也便于将数据加载到设备中。</li><li><strong>torch.nn.Module.load_state_dict()：</strong> 加载模型的参数。<h1 id="state-dict"><a href="#state-dict" class="headerlink" title="state_dict"></a>state_dict</h1><h2 id="state-dict-介绍"><a href="#state-dict-介绍" class="headerlink" title="state_dict 介绍"></a>state_dict 介绍</h2>PyTorch 中，<code>torch.nn.Module</code>里面的可学习的参数 (weights 和 biases) 都放在<code>model.parameters()</code>里面。而<code>state_dict</code>是一个Python dictionary object，将每一层映射到它的 parameter tensor 上。</li></ol><p><strong>注意</strong>：只有含有可学习参数的层 (convolutional layers, linear layers)，或者含有<code>registered buffers</code>的层 (batchnorm’s running_mean) 才有 state_dict。优化器的对象 (torch.optim) 也有 state_dict，存储了优化器的状态和它的超参数。<br>因为<code>state_dict</code>是一个 Python字典对象，所以保存，加载，更新它比较容易。<br>下面我们通过一个例子直观感受下 state_dict 的用法：</p><pre class="line-numbers language-python"><code class="language-python"><span class="token comment" spellcheck="true"># Define model</span><span class="token keyword">class</span> <span class="token class-name">TheModelClass</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>        super<span class="token punctuation">(</span>TheModelClass<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>conv1 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">6</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>pool <span class="token operator">=</span> nn<span class="token punctuation">.</span>MaxPool2d<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>conv2 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span><span class="token number">6</span><span class="token punctuation">,</span> <span class="token number">16</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>fc1 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">16</span> <span class="token operator">*</span> <span class="token number">5</span> <span class="token operator">*</span> <span class="token number">5</span><span class="token punctuation">,</span> <span class="token number">120</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>fc2 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">120</span><span class="token punctuation">,</span> <span class="token number">84</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>fc3 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">84</span><span class="token punctuation">,</span> <span class="token number">10</span><span class="token punctuation">)</span>    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>        x <span class="token operator">=</span> self<span class="token punctuation">.</span>pool<span class="token punctuation">(</span>F<span class="token punctuation">.</span>relu<span class="token punctuation">(</span>self<span class="token punctuation">.</span>conv1<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>        x <span class="token operator">=</span> self<span class="token punctuation">.</span>pool<span class="token punctuation">(</span>F<span class="token punctuation">.</span>relu<span class="token punctuation">(</span>self<span class="token punctuation">.</span>conv2<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>        x <span class="token operator">=</span> x<span class="token punctuation">.</span>view<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">16</span> <span class="token operator">*</span> <span class="token number">5</span> <span class="token operator">*</span> <span class="token number">5</span><span class="token punctuation">)</span>        x <span class="token operator">=</span> F<span class="token punctuation">.</span>relu<span class="token punctuation">(</span>self<span class="token punctuation">.</span>fc1<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">)</span>        x <span class="token operator">=</span> F<span class="token punctuation">.</span>relu<span class="token punctuation">(</span>self<span class="token punctuation">.</span>fc2<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">)</span>        x <span class="token operator">=</span> self<span class="token punctuation">.</span>fc3<span class="token punctuation">(</span>x<span class="token punctuation">)</span>        <span class="token keyword">return</span> x<span class="token comment" spellcheck="true"># Initialize model</span>model <span class="token operator">=</span> TheModelClass<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true"># Initialize optimizer</span>optimizer <span class="token operator">=</span> optim<span class="token punctuation">.</span>SGD<span class="token punctuation">(</span>model<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> lr<span class="token operator">=</span><span class="token number">0.001</span><span class="token punctuation">,</span> momentum<span class="token operator">=</span><span class="token number">0.9</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true"># Print model's state_dict</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"Model's state_dict:"</span><span class="token punctuation">)</span><span class="token keyword">for</span> param_tensor <span class="token keyword">in</span> model<span class="token punctuation">.</span>state_dict<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">print</span><span class="token punctuation">(</span>param_tensor<span class="token punctuation">,</span> <span class="token string">"\t"</span><span class="token punctuation">,</span> model<span class="token punctuation">.</span>state_dict<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">[</span>param_tensor<span class="token punctuation">]</span><span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true"># Print optimizer's state_dict</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"Optimizer's state_dict:"</span><span class="token punctuation">)</span><span class="token keyword">for</span> var_name <span class="token keyword">in</span> optimizer<span class="token punctuation">.</span>state_dict<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">print</span><span class="token punctuation">(</span>var_name<span class="token punctuation">,</span> <span class="token string">"\t"</span><span class="token punctuation">,</span> optimizer<span class="token punctuation">.</span>state_dict<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">[</span>var_name<span class="token punctuation">]</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>输出</p><pre class="line-numbers language-python"><code class="language-python">Model's state_dict<span class="token punctuation">:</span>conv1<span class="token punctuation">.</span>weight     torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">6</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">]</span><span class="token punctuation">)</span>conv1<span class="token punctuation">.</span>bias   torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">6</span><span class="token punctuation">]</span><span class="token punctuation">)</span>conv2<span class="token punctuation">.</span>weight     torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">16</span><span class="token punctuation">,</span> <span class="token number">6</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">]</span><span class="token punctuation">)</span>conv2<span class="token punctuation">.</span>bias   torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">16</span><span class="token punctuation">]</span><span class="token punctuation">)</span>fc1<span class="token punctuation">.</span>weight   torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">120</span><span class="token punctuation">,</span> <span class="token number">400</span><span class="token punctuation">]</span><span class="token punctuation">)</span>fc1<span class="token punctuation">.</span>bias     torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">120</span><span class="token punctuation">]</span><span class="token punctuation">)</span>fc2<span class="token punctuation">.</span>weight   torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">84</span><span class="token punctuation">,</span> <span class="token number">120</span><span class="token punctuation">]</span><span class="token punctuation">)</span>fc2<span class="token punctuation">.</span>bias     torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">84</span><span class="token punctuation">]</span><span class="token punctuation">)</span>fc3<span class="token punctuation">.</span>weight   torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">10</span><span class="token punctuation">,</span> <span class="token number">84</span><span class="token punctuation">]</span><span class="token punctuation">)</span>fc3<span class="token punctuation">.</span>bias     torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">10</span><span class="token punctuation">]</span><span class="token punctuation">)</span>Optimizer's state_dict<span class="token punctuation">:</span>state    <span class="token punctuation">{</span><span class="token punctuation">}</span>param_groups     <span class="token punctuation">[</span><span class="token punctuation">{</span><span class="token string">'lr'</span><span class="token punctuation">:</span> <span class="token number">0.001</span><span class="token punctuation">,</span> <span class="token string">'momentum'</span><span class="token punctuation">:</span> <span class="token number">0.9</span><span class="token punctuation">,</span> <span class="token string">'dampening'</span><span class="token punctuation">:</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token string">'weight_decay'</span><span class="token punctuation">:</span> <span class="token number">0</span><span class="token punctuation">,</span>                    <span class="token string">'nesterov'</span><span class="token punctuation">:</span> <span class="token boolean">False</span><span class="token punctuation">,</span> <span class="token string">'params'</span><span class="token punctuation">:</span> <span class="token punctuation">[</span><span class="token number">4675713712</span><span class="token punctuation">,</span> <span class="token number">4675713784</span><span class="token punctuation">,</span> <span class="token number">4675714000</span><span class="token punctuation">,</span>                                                  <span class="token number">4675714072</span><span class="token punctuation">,</span> <span class="token number">4675714216</span><span class="token punctuation">,</span> <span class="token number">4675714288</span><span class="token punctuation">,</span>                                                  <span class="token number">4675714432</span><span class="token punctuation">,</span> <span class="token number">4675714504</span><span class="token punctuation">,</span> <span class="token number">4675714648</span><span class="token punctuation">,</span>                                                  <span class="token number">4675714720</span><span class="token punctuation">]</span><span class="token punctuation">}</span><span class="token punctuation">]</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h1 id="保存和加载模型"><a href="#保存和加载模型" class="headerlink" title="保存和加载模型"></a>保存和加载模型</h1><h2 id="保存和加载-state-dict-已经训练完，无需继续训练"><a href="#保存和加载-state-dict-已经训练完，无需继续训练" class="headerlink" title="保存和加载 state_dict (已经训练完，无需继续训练)"></a>保存和加载 state_dict (已经训练完，无需继续训练)</h2><p>保存</p><pre class="line-numbers language-python"><code class="language-python">torch<span class="token punctuation">.</span>save<span class="token punctuation">(</span>model<span class="token punctuation">.</span>state_dict<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> PATH<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>加载</p><pre class="line-numbers language-python"><code class="language-python">model <span class="token operator">=</span> TheModelClass<span class="token punctuation">(</span><span class="token operator">*</span>args<span class="token punctuation">,</span> <span class="token operator">**</span>kwargs<span class="token punctuation">)</span>model<span class="token punctuation">.</span>load_state_dict<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>load<span class="token punctuation">(</span>PATH<span class="token punctuation">)</span><span class="token punctuation">)</span>model<span class="token punctuation">.</span>eval<span class="token punctuation">(</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><p>一般保存为<code>.pt</code>或<code>.pth</code>格式的文件。<br>注意：</p><ol><li>可以使用<code>model.eval()</code>将 dropout 和 batch normalization 层设置成 evaluation 模式。</li><li><code>load_state_dict()</code>函数需要一个<strong>dict类型</strong>的输入，而不是保存模型的PATH。</li><li>如果你想<strong>保存验证集上表现最好的模型</strong>，那么这样<code>best_model_state=model.state_dict()</code>是错误的。因为这属于浅复制，也就是说此时这个best_model_state会随着后续的训练过程而不断被更新，最后保存的其实是个 overfit 的模型。所以正确的做法应该是<code>best_model_state=deepcopy(model.state_dict())</code>。<h2 id="保存和加载整个模型-已经训练完，无需继续训练"><a href="#保存和加载整个模型-已经训练完，无需继续训练" class="headerlink" title="保存和加载整个模型 (已经训练完，无需继续训练)"></a>保存和加载整个模型 (已经训练完，无需继续训练)</h2>保存<pre class="line-numbers language-python"><code class="language-python">torch<span class="token punctuation">.</span>save<span class="token punctuation">(</span>model<span class="token punctuation">,</span> PATH<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>加载。注意：<strong>必须提前定义好模型的类</strong>。<pre class="line-numbers language-python"><code class="language-python"><span class="token comment" spellcheck="true"># Model class must be defined somewhere</span>model <span class="token operator">=</span> torch<span class="token punctuation">.</span>load<span class="token punctuation">(</span>PATH<span class="token punctuation">)</span>model<span class="token punctuation">.</span>eval<span class="token punctuation">(</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre>一般保存为<code>.pt</code>或<code>.pth</code>格式的文件。<h2 id="保存和加载-state-dict-没有训练完，还会继续训练"><a href="#保存和加载-state-dict-没有训练完，还会继续训练" class="headerlink" title="保存和加载 state_dict (没有训练完，还会继续训练)"></a>保存和加载 state_dict (没有训练完，还会继续训练)</h2>保存。除了保存 model_state_dict 之外，还需要保存：optimizer_state_dict，epoch 和 loss，因为继续训练时要知道优化器的状态，epoch 等等。<pre class="line-numbers language-python"><code class="language-python">torch<span class="token punctuation">.</span>save<span class="token punctuation">(</span><span class="token punctuation">{</span>         <span class="token string">'epoch'</span><span class="token punctuation">:</span> epoch<span class="token punctuation">,</span>         <span class="token string">'model_state_dict'</span><span class="token punctuation">:</span> model<span class="token punctuation">.</span>state_dict<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>         <span class="token string">'optimizer_state_dict'</span><span class="token punctuation">:</span> optimizer<span class="token punctuation">.</span>state_dict<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>         <span class="token string">'loss'</span><span class="token punctuation">:</span> loss<span class="token punctuation">,</span>         <span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span>         <span class="token punctuation">}</span><span class="token punctuation">,</span> PATH<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>加载。除了加载 model_state_dict 之外，还需要加载：optimizer_state_dict，epoch 和 loss。<pre class="line-numbers language-python"><code class="language-python">model <span class="token operator">=</span> TheModelClass<span class="token punctuation">(</span><span class="token operator">*</span>args<span class="token punctuation">,</span> <span class="token operator">**</span>kwargs<span class="token punctuation">)</span>optimizer <span class="token operator">=</span> TheOptimizerClass<span class="token punctuation">(</span><span class="token operator">*</span>args<span class="token punctuation">,</span> <span class="token operator">**</span>kwargs<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre></li></ol><p>checkpoint = torch.load(PATH)<br>model.load_state_dict(checkpoint[‘model_state_dict’])<br>optimizer.load_state_dict(checkpoint[‘optimizer_state_dict’])<br>epoch = checkpoint[‘epoch’]<br>loss = checkpoint[‘loss’]</p><p>model.eval()</p><h1 id="or"><a href="#or" class="headerlink" title="- or -"></a>- or -</h1><p>model.train()</p><pre><code>## 把多个模型存进一个文件保存：把模型 A 和 B 的 state_dict 和 optimizer 都存进一个文件中。```pythontorch.save({            &#39;modelA_state_dict&#39;: modelA.state_dict(),            &#39;modelB_state_dict&#39;: modelB.state_dict(),            &#39;optimizerA_state_dict&#39;: optimizerA.state_dict(),            &#39;optimizerB_state_dict&#39;: optimizerB.state_dict(),            ...            }, PATH)</code></pre><p>加载：</p><pre class="line-numbers language-python"><code class="language-python">modelA <span class="token operator">=</span> TheModelAClass<span class="token punctuation">(</span><span class="token operator">*</span>args<span class="token punctuation">,</span> <span class="token operator">**</span>kwargs<span class="token punctuation">)</span>modelB <span class="token operator">=</span> TheModelBClass<span class="token punctuation">(</span><span class="token operator">*</span>args<span class="token punctuation">,</span> <span class="token operator">**</span>kwargs<span class="token punctuation">)</span>optimizerA <span class="token operator">=</span> TheOptimizerAClass<span class="token punctuation">(</span><span class="token operator">*</span>args<span class="token punctuation">,</span> <span class="token operator">**</span>kwargs<span class="token punctuation">)</span>optimizerB <span class="token operator">=</span> TheOptimizerBClass<span class="token punctuation">(</span><span class="token operator">*</span>args<span class="token punctuation">,</span> <span class="token operator">**</span>kwargs<span class="token punctuation">)</span>checkpoint <span class="token operator">=</span> torch<span class="token punctuation">.</span>load<span class="token punctuation">(</span>PATH<span class="token punctuation">)</span>modelA<span class="token punctuation">.</span>load_state_dict<span class="token punctuation">(</span>checkpoint<span class="token punctuation">[</span><span class="token string">'modelA_state_dict'</span><span class="token punctuation">]</span><span class="token punctuation">)</span>modelB<span class="token punctuation">.</span>load_state_dict<span class="token punctuation">(</span>checkpoint<span class="token punctuation">[</span><span class="token string">'modelB_state_dict'</span><span class="token punctuation">]</span><span class="token punctuation">)</span>optimizerA<span class="token punctuation">.</span>load_state_dict<span class="token punctuation">(</span>checkpoint<span class="token punctuation">[</span><span class="token string">'optimizerA_state_dict'</span><span class="token punctuation">]</span><span class="token punctuation">)</span>optimizerB<span class="token punctuation">.</span>load_state_dict<span class="token punctuation">(</span>checkpoint<span class="token punctuation">[</span><span class="token string">'optimizerB_state_dict'</span><span class="token punctuation">]</span><span class="token punctuation">)</span>modelA<span class="token punctuation">.</span>eval<span class="token punctuation">(</span><span class="token punctuation">)</span>modelB<span class="token punctuation">.</span>eval<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true"># - or -</span>modelA<span class="token punctuation">.</span>train<span class="token punctuation">(</span><span class="token punctuation">)</span>modelB<span class="token punctuation">.</span>train<span class="token punctuation">(</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h1 id="其他"><a href="#其他" class="headerlink" title="其他"></a>其他</h1><h2 id="使用其他模型的参数暖启动自己的模型"><a href="#使用其他模型的参数暖启动自己的模型" class="headerlink" title="使用其他模型的参数暖启动自己的模型"></a>使用其他模型的参数暖启动自己的模型</h2><p>有时候训练一个新的复杂模型时，需要加载它的一部分预训练的权重。即使只有几个可用的参数，也会有助于 warmstart 训练过程，帮助模型更快达到收敛。<br>如果手里有的这个 state_dict 缺乏一些 keys，或者多了一些 keys，只要设置<code>strict</code>参数为 False，就能够把 state_dict 能够匹配的 keys 加载进去，而忽略掉那些不匹配的 keys。<br>保存模型 A 的 state_dict ：</p><pre class="line-numbers language-python"><code class="language-python">torch<span class="token punctuation">.</span>save<span class="token punctuation">(</span>modelA<span class="token punctuation">.</span>state_dict<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> PATH<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>加载到模型 B：</p><pre class="line-numbers language-python"><code class="language-python">modelB <span class="token operator">=</span> TheModelBClass<span class="token punctuation">(</span><span class="token operator">*</span>args<span class="token punctuation">,</span> <span class="token operator">**</span>kwargs<span class="token punctuation">)</span>modelB<span class="token punctuation">.</span>load_state_dict<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>load<span class="token punctuation">(</span>PATH<span class="token punctuation">)</span><span class="token punctuation">,</span> strict<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><h2 id="不同设备（CPU-GPU）上保存和加载"><a href="#不同设备（CPU-GPU）上保存和加载" class="headerlink" title="不同设备（CPU/GPU）上保存和加载"></a>不同设备（CPU/GPU）上保存和加载</h2><p>不同设备的保存和加载需要在<code>load()</code>函数中引入一个参数<code>map_location</code>来指定需要将不同设备上保存的模型映射到同一个设备上。如果保存模型和加载模型的是<strong>相同</strong>的设备：CPU-&gt;CPU，GPU-&gt;GPU。则加载模型时可不使用<code>map_location</code>参数。如果保存模型和加载模型的是<strong>不同</strong>的设备：CPU-&gt;GPU，GPU-&gt;CPU。则加载模型时必须使用<code>map_location</code>参数。<br>上述的两种情况下，如果加载的设备是GPU，还需要使用<code>model.to(device)</code>将模型的参数张量转化为CUDA张量。<br>从CPU加载到GPU：</p><pre class="line-numbers language-python"><code class="language-python">torch<span class="token punctuation">.</span>save<span class="token punctuation">(</span>model<span class="token punctuation">.</span>state_dict<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> PATH<span class="token punctuation">)</span>device <span class="token operator">=</span> torch<span class="token punctuation">.</span>device<span class="token punctuation">(</span><span class="token string">"cuda"</span><span class="token punctuation">)</span>model <span class="token operator">=</span> TheModelClass<span class="token punctuation">(</span><span class="token operator">*</span>args<span class="token punctuation">,</span> <span class="token operator">**</span>kwargs<span class="token punctuation">)</span>model<span class="token punctuation">.</span>load_state_dict<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>load<span class="token punctuation">(</span>PATH<span class="token punctuation">,</span> map_location<span class="token operator">=</span><span class="token string">"cuda"</span><span class="token punctuation">)</span><span class="token punctuation">)</span>  model<span class="token punctuation">.</span>to<span class="token punctuation">(</span>device<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>从GPU加载到CPU：</p><pre class="line-numbers language-python"><code class="language-python">torch<span class="token punctuation">.</span>save<span class="token punctuation">(</span>model<span class="token punctuation">.</span>state_dict<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> PATH<span class="token punctuation">)</span>device <span class="token operator">=</span> torch<span class="token punctuation">.</span>device<span class="token punctuation">(</span><span class="token string">'cpu'</span><span class="token punctuation">)</span>model <span class="token operator">=</span> TheModelClass<span class="token punctuation">(</span><span class="token operator">*</span>args<span class="token punctuation">,</span> <span class="token operator">**</span>kwargs<span class="token punctuation">)</span>model<span class="token punctuation">.</span>load_state_dict<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>load<span class="token punctuation">(</span>PATH<span class="token punctuation">,</span> map_location<span class="token operator">=</span>device<span class="token punctuation">)</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><h1 id="断点继续训练"><a href="#断点继续训练" class="headerlink" title="断点继续训练"></a>断点继续训练</h1><p>将网络训练过程中的网络的权重，优化器的权重保存，以及epoch 保存，便于继续训练恢复。如果使用了学习率衰减，在保存网络中的训练的参数的过程中，还需要保存lr_scheduler的state_dict，然后断点继续训练的时候恢复。</p><pre class="line-numbers language-python"><code class="language-python">checkpoint <span class="token operator">=</span> <span class="token punctuation">{</span>    <span class="token string">"net"</span><span class="token punctuation">:</span> model<span class="token punctuation">.</span>state_dict<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>    <span class="token string">'optimizer'</span><span class="token punctuation">:</span> optimizer<span class="token punctuation">.</span>state_dict<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>    <span class="token string">"epoch"</span><span class="token punctuation">:</span> epoch<span class="token punctuation">,</span>    <span class="token string">'lr_schedule'</span><span class="token punctuation">:</span> lr_schedule<span class="token punctuation">.</span>state_dict<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">}</span><span class="token keyword">if</span> <span class="token operator">not</span> os<span class="token punctuation">.</span>path<span class="token punctuation">.</span>isdir<span class="token punctuation">(</span><span class="token string">"./model_parameter/test"</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    os<span class="token punctuation">.</span>mkdir<span class="token punctuation">(</span><span class="token string">"./model_parameter/test"</span><span class="token punctuation">)</span>torch<span class="token punctuation">.</span>save<span class="token punctuation">(</span>checkpoint<span class="token punctuation">,</span> <span class="token string">'./model_parameter/test/ckpt_best_%s.pth'</span> <span class="token operator">%</span> <span class="token punctuation">(</span>str<span class="token punctuation">(</span>epoch<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>加载恢复训练参数</p><pre class="line-numbers language-python"><code class="language-python"><span class="token comment" spellcheck="true">#加载恢复</span><span class="token keyword">if</span> RESUME<span class="token punctuation">:</span>    path_checkpoint <span class="token operator">=</span> <span class="token string">"./model_parameter/test/ckpt_best_50.pth"</span>  <span class="token comment" spellcheck="true"># 断点路径</span>    checkpoint <span class="token operator">=</span> torch<span class="token punctuation">.</span>load<span class="token punctuation">(</span>path_checkpoint<span class="token punctuation">)</span>  <span class="token comment" spellcheck="true"># 加载断点</span>    model<span class="token punctuation">.</span>load_state_dict<span class="token punctuation">(</span>checkpoint<span class="token punctuation">[</span><span class="token string">'net'</span><span class="token punctuation">]</span><span class="token punctuation">)</span>  <span class="token comment" spellcheck="true"># 加载模型可学习参数</span>    optimizer<span class="token punctuation">.</span>load_state_dict<span class="token punctuation">(</span>checkpoint<span class="token punctuation">[</span><span class="token string">'optimizer'</span><span class="token punctuation">]</span><span class="token punctuation">)</span>  <span class="token comment" spellcheck="true"># 加载优化器参数</span>    start_epoch <span class="token operator">=</span> checkpoint<span class="token punctuation">[</span><span class="token string">'epoch'</span><span class="token punctuation">]</span>  <span class="token comment" spellcheck="true"># 设置开始的epoch</span>    lr_schedule<span class="token punctuation">.</span>load_state_dict<span class="token punctuation">(</span>checkpoint<span class="token punctuation">[</span><span class="token string">'lr_schedule'</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true">#加载lr_scheduler</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>]]></content>
      
      
      <categories>
          
          <category> Pytorch </category>
          
      </categories>
      
      
        <tags>
            
            <tag> pytorch </tag>
            
            <tag> 保存和加载/断点训练 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>深度学习中的学习率衰减</title>
      <link href="/2023/02/21/xue-xi-lu-shuai-jian/"/>
      <url>/2023/02/21/xue-xi-lu-shuai-jian/</url>
      
        <content type="html"><![CDATA[<h1 id="学习率衰减原理"><a href="#学习率衰减原理" class="headerlink" title="学习率衰减原理"></a>学习率衰减原理</h1><p>在梯度下降算法中，学习率用来控制权重更新的步幅。学习率越大，则权重更新的步子迈得大一些，学习率越小，则权重更新的步子迈得小一些。<br>$w=w- \alpha * \frac{\partial Loss}{\partial w}$<br>如果在训练过程中，学习率保持不变，则可能会出现下面两种情况。</p><ul><li>左侧是学习率较小的情况，这时权重更新步幅小，导致模型收敛很慢。</li><li>右侧是学习率较大的情况，这时权重更新步幅大，模型刚开始收敛很快，但是最终在接近目标函数极值时由于步子迈的太大，会越过极值的位置，导致模型在目标函数的极值两侧来回震荡，不能收敛到最优解。</li></ul><p><img src="./imgs/lr1.png" alt><img src="./imgs/lr3.png" alt><br>最好的状态是在训练初期学习率设置大一些，使模型收敛得快一些，训练后期学习率设置小一些，使模型能收敛到最优解。<br><img src="./imgs/lr2.png" alt><br>因此在训练时若模型的精度出现震荡或是Loss不再下降时，适当进行学习率衰减是一个有效的训练方法。Pytorch有两种学习率衰减的方法：</p><ol><li>一种是手动设置，</li><li>另一种使用<code>lr_scheduler()</code>提供的几种衰减函数进行设置。</li></ol><p>使用<code>lr_scheduler()</code>提供的函数进行设置时，方法都是类似的，如下面的代码模板。<code>decay_method()</code>方法就是<code>lr_scheduler()</code>提供的衰减函数。</p><pre class="line-numbers language-python"><code class="language-python">scheduler <span class="token operator">=</span> torch<span class="token punctuation">.</span>optim<span class="token punctuation">.</span>lr_scheduler<span class="token punctuation">.</span>decay_method<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token keyword">for</span> epoch <span class="token keyword">in</span> range<span class="token punctuation">(</span>epochs<span class="token punctuation">)</span><span class="token punctuation">:</span>    train<span class="token punctuation">(</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">)</span>    validate<span class="token punctuation">(</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">.</span><span class="token punctuation">)</span>    scheduler<span class="token punctuation">.</span>step<span class="token punctuation">(</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h1 id="lr-scheduler-的学习率衰减策略"><a href="#lr-scheduler-的学习率衰减策略" class="headerlink" title="lr_scheduler()的学习率衰减策略"></a>lr_scheduler()的学习率衰减策略</h1><h2 id="等间隔调整学习率"><a href="#等间隔调整学习率" class="headerlink" title="等间隔调整学习率"></a>等间隔调整学习率</h2><pre class="line-numbers language-python"><code class="language-python">scheduler <span class="token operator">=</span> torch<span class="token punctuation">.</span>optim<span class="token punctuation">.</span>lr_scheduler<span class="token punctuation">.</span>StepLR<span class="token punctuation">(</span>optimizer<span class="token punctuation">,</span> step_size<span class="token punctuation">,</span> gamma<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>该方法有3个参数：</p><ol><li>optimizer是优化器；</li><li>step_size表示间隔多少个epoch后调整学习率；</li><li>gamma表示学习率调整的倍数。</li></ol><p>该方法是每训练step_size个epoch，就将学习率调整为 lr<em>gamma。如下图step_size = 10，gamma = 0.8表示每间隔10个epoch将学习率调整为 lr</em>0.8。<br><img src="./imgs/step_LR.png" alt></p><h2 id="多间隔调整学习率"><a href="#多间隔调整学习率" class="headerlink" title="多间隔调整学习率"></a>多间隔调整学习率</h2><pre class="line-numbers language-python"><code class="language-python">scheduler <span class="token operator">=</span> torch<span class="token punctuation">.</span>optim<span class="token punctuation">.</span>lr_scheduler<span class="token punctuation">.</span>MultiStepLR<span class="token punctuation">(</span>optimizer<span class="token punctuation">,</span> milestones<span class="token punctuation">,</span> gamma<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>该方法与StepLR()类似也是间隔调整学习率，但是<strong>间隔并不相等</strong>。它有3个参数：</p><ol><li>optimizer是优化器；</li><li>milestones是一个列表，列表中每个元素表示要调整学习率的epoch；</li><li>gamma表示学习率调整的倍数。</li></ol><p>如下图所示，milestones= [10, 30, 90]，gamma = 0.8表示在epoch = 10、30、90时将学习率调整为 lr*0.8。<br><img src="./imgs/MultiStepLR.png" alt></p><h2 id="指数衰减调整学习率"><a href="#指数衰减调整学习率" class="headerlink" title="指数衰减调整学习率"></a>指数衰减调整学习率</h2><pre class="line-numbers language-python"><code class="language-python">scheduler <span class="token operator">=</span> torch<span class="token punctuation">.</span>optim<span class="token punctuation">.</span>lr_scheduler<span class="token punctuation">.</span>ExponentialLR<span class="token punctuation">(</span>optimizer<span class="token punctuation">,</span> gamma<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>该方法与前两种不同，是<strong>每个epoch都调整学习率</strong>，学习率调整为<br>$lr* gamma^{epoch}$。<br>它有两个参数：</p><ol><li>optimizer是优化器；</li><li>gamma是调整学习率的底数，指数为epoch。</li></ol><p>如下图，gamma = 0.8。<br><img src="./imgs/ELR.png" alt></p><h2 id="余弦退火函数调整学习率"><a href="#余弦退火函数调整学习率" class="headerlink" title="余弦退火函数调整学习率"></a>余弦退火函数调整学习率</h2><pre class="line-numbers language-python"><code class="language-python">scheduler <span class="token operator">=</span> torch<span class="token punctuation">.</span>optim<span class="token punctuation">.</span>lr_scheduler<span class="token punctuation">.</span>CosineAnnealingLR<span class="token punctuation">(</span>optimizer<span class="token punctuation">,</span> T_max<span class="token punctuation">,</span> eta_min<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>该方法使得<strong>学习率呈Cos型衰减</strong>，T_max表示0.5个余弦函数周期，也<strong>表示学习率调整为最小时的epoch数值，eta_min表示学习率调整的最小值</strong>，默认为0。如下所示，T_max = 100, eta_min=0表示学习率呈Cos衰减，在epoch = 100时学习率降到最低，即学习率降到0。<br><img src="./imgs/Cos.png" alt></p><h2 id="根据指标调整学习率"><a href="#根据指标调整学习率" class="headerlink" title="根据指标调整学习率"></a>根据指标调整学习率</h2><pre class="line-numbers language-python"><code class="language-python">scheduler <span class="token operator">=</span> torch<span class="token punctuation">.</span>optim<span class="token punctuation">.</span>lr_sheduler<span class="token punctuation">.</span>ReduceLROnPlateau<span class="token punctuation">(</span>optimizer<span class="token punctuation">,</span> mode<span class="token operator">=</span><span class="token string">'min'</span><span class="token punctuation">,</span> factor<span class="token operator">=</span><span class="token number">0.1</span><span class="token punctuation">,</span>                     patience<span class="token operator">=</span><span class="token number">10</span><span class="token punctuation">,</span> verbose<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span> threshold<span class="token operator">=</span><span class="token number">0.0001</span><span class="token punctuation">,</span> threshold_mode<span class="token operator">=</span><span class="token string">'rel'</span><span class="token punctuation">,</span>                     cooldown<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">,</span> min_lr<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">,</span> eps<span class="token operator">=</span><span class="token number">1e</span><span class="token operator">-</span><span class="token number">08</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><p>该方法<strong>根据监测指标来调整学习率，如训练过程中监测Loss不再降低或是监测Acc不再上升，则调整学习率。</strong>它有如下参数：</p><ol><li><strong>mod</strong>是模式选择，其参数有两种值”min”和”max”，min表示监测指标不再降低就调整学习率，”max”表示检测指标不再上升就调整学习率。</li><li><strong>factor</strong>表示学习率调整的倍数，学习率会调整为lr*factor。</li><li><strong>patience</strong>表示监测指标经过多少个epoch没有降低或上升了，其含义是经过patience个epoch，监测指标仍然没有变化，则调整学习率。</li><li><strong>verbose</strong>表示是否打印学习率。</li><li><strong>cooldown</strong>表示调整过一次学习率后，等待一定的epoch再进行监测。</li><li><strong>min_lr</strong>表示学习率的最小值。</li><li><strong>eps</strong>表示学习率的最小变化，若新旧学习率之间的差值小于1e-8则学习率不变。<h2 id="自定义调整学习率"><a href="#自定义调整学习率" class="headerlink" title="自定义调整学习率"></a>自定义调整学习率</h2><pre class="line-numbers language-python"><code class="language-python">scheduler <span class="token operator">=</span> lr_scheduler<span class="token punctuation">.</span>LambdaLR<span class="token punctuation">(</span>optimizer<span class="token punctuation">,</span> lr_lambda<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>该方法允许<strong>自定义学习率衰减函数</strong>，它是每个epoch调整一次学习率。<br>该方法的<code>lr_lambda</code>参数表示<strong>自定义的学习率调整函数</strong>，可以是自定义函数，也可以是lambda表达式，自定义的函数须要<strong>接收一个int参数：epoch</strong>，根据epoch定义出学习率的调整倍数。学习率调整为<br>$lr * LR_{lambda}(epoch)$<br>如下所示，自定义学习率调整函数：下面的函数是<strong>自定义的指数衰减</strong>，gamma = 0.8。<pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">lr_decay</span><span class="token punctuation">(</span>epoch<span class="token punctuation">)</span><span class="token punctuation">:</span> <span class="token keyword">return</span> <span class="token number">0.8</span><span class="token operator">**</span>epochscheduler <span class="token operator">=</span> lr_scheduler<span class="token punctuation">.</span>LambdaLR<span class="token punctuation">(</span>optimizer<span class="token punctuation">,</span>lr_lambda <span class="token operator">=</span> lr_decay<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre>自定义lambda表达式：下面是<strong>自定义的等间隔学习率调整</strong>，间隔为10epoch。<pre class="line-numbers language-python"><code class="language-python">lambda1 <span class="token operator">=</span> <span class="token keyword">lambda</span> epoch<span class="token punctuation">:</span><span class="token punctuation">(</span><span class="token number">0.8</span> <span class="token operator">**</span> <span class="token punctuation">(</span>epoch <span class="token operator">//</span> <span class="token number">10</span><span class="token punctuation">)</span><span class="token punctuation">)</span>scheduler <span class="token operator">=</span> lr_scheduler<span class="token punctuation">.</span>LambdaLR<span class="token punctuation">(</span>optimizer<span class="token punctuation">,</span>lr_lambda <span class="token operator">=</span> lambda1<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><h1 id="手动调整学习率"><a href="#手动调整学习率" class="headerlink" title="手动调整学习率"></a>手动调整学习率</h1>我们可以通过下面两种方式来访问模型中的学习率。optimizer通过<code>param_groups</code>来管理参数组，<code>param_groups</code>中保存了模型的参数组及其对应的学习率，通过<code>param_group[&#39;lr&#39;]</code>就可以访问到对应参数组的学习率。<pre class="line-numbers language-python"><code class="language-python"><span class="token comment" spellcheck="true"># 下面两种方式等价</span><span class="token keyword">print</span> <span class="token punctuation">(</span>optimizer<span class="token punctuation">.</span>param_groups<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token string">'lr'</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token keyword">print</span> <span class="token punctuation">(</span>optimizer<span class="token punctuation">.</span>state_dict<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token string">'param_groups'</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token string">'lr'</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre>所以<strong>通过</strong><code>**param_groups**</code><strong>来修改对应参数组的学习率</strong>。下面的代码是每10个epoch修改一次学习率，调整的学习率倍数为0.9。<pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">for</span> epoch <span class="token keyword">in</span> range<span class="token punctuation">(</span><span class="token number">100</span><span class="token punctuation">)</span><span class="token punctuation">:</span> <span class="token keyword">if</span> epoch <span class="token operator">%</span> <span class="token number">10</span> <span class="token operator">==</span> <span class="token number">0</span><span class="token punctuation">:</span>     <span class="token keyword">for</span> param_group <span class="token keyword">in</span> optimizer<span class="token punctuation">.</span>param_groups<span class="token punctuation">:</span>         param_group<span class="token punctuation">[</span><span class="token string">'lr'</span><span class="token punctuation">]</span> <span class="token operator">*=</span> <span class="token number">0.9</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre>除了这种方式外，还可以<strong>通过重新定义优化器的方式修改学习率</strong>。下面的代码是每20个epoch修改一次学习率，调整的学习率倍数为0.8。<pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">lr_decay</span><span class="token punctuation">(</span>lr<span class="token punctuation">,</span> epoch<span class="token punctuation">)</span><span class="token punctuation">:</span> <span class="token keyword">return</span> lr<span class="token operator">*</span><span class="token punctuation">(</span><span class="token number">0.8</span> <span class="token operator">**</span> <span class="token punctuation">(</span>epoch <span class="token operator">//</span> <span class="token number">20</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre></li></ol><p>for epoch in range(epochs):<br>    # 调整学习率<br>    lr = lr_decay(lr, epoch)<br>    optimizer = optim.SGD(net.parameters(), lr=lr)<br>    ……<br>    ……<br>    # 采用新的学习率进行参数更新<br>    optimizer.step()<br>    ……</p><pre><code></code></pre>]]></content>
      
      
      <categories>
          
          <category> Pytorch </category>
          
      </categories>
      
      
        <tags>
            
            <tag> pytorch </tag>
            
            <tag> 学习率衰减 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Python的@staticmethod和@classmethod</title>
      <link href="/2023/02/17/python-zhong-staticmethod-classmethod-fang-fa/"/>
      <url>/2023/02/17/python-zhong-staticmethod-classmethod-fang-fa/</url>
      
        <content type="html"><![CDATA[<h1 id="原理"><a href="#原理" class="headerlink" title="原理"></a>原理</h1><p>一般来说，要使用某个类的方法，需要先实例化一个对象再调用方法。而使用<code>@staticmethod</code>或<code>@classmethod</code>，就可以不需要实例化，直接<code>类名.方法名()</code>来调用。<br>Python面向对象编程中，类中定义的方法可以是<code>@classmethod</code>装饰的<strong>类方法</strong>，也可以是<code>@staticmethod</code>装饰的<strong>静态方法</strong>，用的最多的还是不带装饰器的<strong>实例方法。</strong><br>先来看一个简单示例：</p><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">A</span><span class="token punctuation">(</span>object<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">def</span> <span class="token function">m1</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> n<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"self:"</span><span class="token punctuation">,</span> self<span class="token punctuation">)</span>    @classmethod    <span class="token keyword">def</span> <span class="token function">m2</span><span class="token punctuation">(</span>cls<span class="token punctuation">,</span> n<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"cls:"</span><span class="token punctuation">,</span> cls<span class="token punctuation">)</span>    @staticmethod    <span class="token keyword">def</span> <span class="token function">m3</span><span class="token punctuation">(</span>n<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token keyword">pass</span>a <span class="token operator">=</span> A<span class="token punctuation">(</span><span class="token punctuation">)</span>a<span class="token punctuation">.</span>m1<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span> <span class="token comment" spellcheck="true"># self: &lt;__main__.A object at 0x000001E596E41A90></span>A<span class="token punctuation">.</span>m2<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span> <span class="token comment" spellcheck="true"># cls: &lt;class '__main__.A'></span>A<span class="token punctuation">.</span>m3<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>在类中一共定义了3个方法，m1 是<strong>实例方法</strong>，第一个参数必须是<code>self</code>（约定俗成的）。m2 是<strong>类方法</strong>，第一个参数必须是<code>cls</code>（同样是约定俗成），m3 是<strong>静态方法</strong>，参数根据业务需求定，可有可无。<br>当程序运行时，大概发生了这么几件事（结合下面的图来看）：</p><ul><li>第一步：代码从第一行开始执行 class 命令，此时会创建一个类 A 对象（<mark>类也是对象</mark>，一切皆对象嘛）同时初始化类里面的属性和方法；记住，此刻实例对象还没创建出来。</li><li>第二、三步：接着执行 a=A()，系统自动调用类的构造器，构造出实例对象 a。</li><li>第四步：接着调用<code>a.m1(1)</code>，m1 是实例方法，<strong>内部会自动把</strong><mark>实例对象</mark>传递给 self 参数进行绑定**，也就是说， self 和 a 指向的都是同一个实例对象。</li><li>第五步：调用A.m2(1)时，<strong>python内部隐式地把</strong><mark>类对象</mark><strong>传递给 cls 参数</strong>，cls 和 A 都指向类对象。</li></ul><p><img src="./imgs/python_class.png" alt><br>严格意义上来说，左边的都是变量名，是对象的引用，右边才是真正的对像</p><h1 id="几个方法的介绍"><a href="#几个方法的介绍" class="headerlink" title="几个方法的介绍"></a>几个方法的介绍</h1><h2 id="实例方法"><a href="#实例方法" class="headerlink" title="实例方法"></a>实例方法</h2><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">print</span><span class="token punctuation">(</span>A<span class="token punctuation">.</span>m1<span class="token punctuation">)</span><span class="token comment" spellcheck="true"># A.m1在py2中显示为&lt;unbound method A.m1></span><span class="token operator">&lt;</span>function A<span class="token punctuation">.</span>m1 at <span class="token number">0x000002BF7FF9A488</span><span class="token operator">></span><span class="token keyword">print</span><span class="token punctuation">(</span>a<span class="token punctuation">.</span>m1<span class="token punctuation">)</span><span class="token operator">&lt;</span>bound method A<span class="token punctuation">.</span>m1 of <span class="token operator">&lt;</span>__main__<span class="token punctuation">.</span>A object at <span class="token number">0x000002BF7FFA2BE0</span><span class="token operator">>></span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p><code>A.m1</code>是一个还没有绑定实例对象的方法，对于未绑定方法，调用<code>A.m1</code>时必须显示地传入一个实例对象进去，而<code>a.m1</code>是已经绑定了实例的方法，python隐式地把对象传递给了self参数，所以不再手动传递参数，这是调用实例方法的过程。</p><pre class="line-numbers language-python"><code class="language-python">A<span class="token punctuation">.</span>m1<span class="token punctuation">(</span>a<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true"># 等价于</span>a<span class="token punctuation">.</span>m1<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><p><strong>如果未绑定的方法</strong><code>**A.m1**</code><strong>不传实例对象给 self 时，就会报参数缺失错误</strong></p><h2 id="类方法"><a href="#类方法" class="headerlink" title="类方法"></a>类方法</h2><p>m2是<strong>类方法</strong>，不管是<code>A.m2</code>还是<code>a.m2</code>，都是已经自动绑定了类对象A的方法，对于<code>a.m2</code>，因为python可以通过实例对象a找到它所属的类是A，找到A之后自动绑定到 cls。</p><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">print</span><span class="token punctuation">(</span>A<span class="token punctuation">.</span>m2<span class="token punctuation">)</span><span class="token operator">&lt;</span>bound method A<span class="token punctuation">.</span>m2 of <span class="token operator">&lt;</span><span class="token keyword">class</span> <span class="token string">'__main__.A'</span><span class="token operator">>></span><span class="token keyword">print</span><span class="token punctuation">(</span>a<span class="token punctuation">.</span>m2<span class="token punctuation">)</span><span class="token operator">&lt;</span>bound method A<span class="token punctuation">.</span>m2 of <span class="token operator">&lt;</span><span class="token keyword">class</span> <span class="token string">'__main__.A'</span><span class="token operator">>></span>A<span class="token punctuation">.</span>m2<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span> <span class="token comment" spellcheck="true"># 等价于</span>a<span class="token punctuation">.</span>m2<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>这使得我们可以在实例方法中通过使用<code>self.m2()</code>这种方式来调用类方法和静态方法。<br>类方法只能访问类变量，不能访问实例变量，也就是跟类有关，跟实例无关。</p><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">m1</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> n<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">"self:"</span><span class="token punctuation">,</span> self<span class="token punctuation">)</span>    self<span class="token punctuation">.</span>m2<span class="token punctuation">(</span>n<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><h2 id="静态方法"><a href="#静态方法" class="headerlink" title="静态方法"></a>静态方法</h2><p>m3是类里面的一个<strong>静态方法</strong>，跟普通函数没什么区别，<strong>与类和实例都没有所谓的绑定关系</strong>，它只不过是碰巧存在类中的一个函数而已。<strong>不论是通过类还是实例都可以引用该方法</strong>。</p><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">print</span><span class="token punctuation">(</span>A<span class="token punctuation">.</span>m3<span class="token punctuation">)</span><span class="token operator">&lt;</span>function A<span class="token punctuation">.</span>m3 at <span class="token number">0x000002BF7FF9A840</span><span class="token operator">></span><span class="token keyword">print</span><span class="token punctuation">(</span>a<span class="token punctuation">.</span>m3<span class="token punctuation">)</span><span class="token operator">&lt;</span>function A<span class="token punctuation">.</span>m3 at <span class="token number">0x000002BF7FF9A840</span><span class="token operator">></span>A<span class="token punctuation">.</span>m3<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span> <span class="token comment" spellcheck="true"># 等价于</span>a<span class="token punctuation">.</span>m3<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><ul><li>可以看成是静态方法已经跟这个类没关系了，相当于已经脱离了这个类，是一个完全独立的函数，只是调用的时候必须通过这个类，或者为了规范代码而将函数放到类中</li><li>当类中的该方法不涉及对该类属性的操作，建议声明为@staticmethod，面向对象思想体现<h1 id="应用场景"><a href="#应用场景" class="headerlink" title="应用场景"></a>应用场景</h1>静态方法的使用场景： 如果在方法中不需要访问任何实例方法和属性，纯粹地通过传入参数并返回数据的功能性方法，那么它就适合用静态方法来定义，它节省了实例化对象的开销成本，往往这种方法放在类外面的模块层作为一个函数存在也是没问题的，而放在类中，仅为这个类服务。例如下面是微信公众号开发中验证微信签名的一个例子，它没有引用任何类或者实例相关的属性和方法。<pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">from</span> hashlib <span class="token keyword">import</span> sha1<span class="token keyword">import</span> tornado<span class="token punctuation">.</span>web<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre></li></ul><p>class SignatureHandler(tornado.web.RequestHandler):<br>    def get(self):<br>        “””<br>         根据签名判断请求是否来自微信<br>        “””<br>        signature = self.get_query_argument(“signature”, None)<br>        echostr = self.get_query_argument(“echostr”, None)<br>        timestamp = self.get_query_argument(“timestamp”, None)<br>        nonce = self.get_query_argument(“nonce”, None)<br>        if self._check_sign(TOKEN, timestamp, nonce, signature):<br>            logger.info(“微信签名校验成功”)<br>            self.write(echostr)<br>        else:<br>            self.write(“你不是微信发过来的请求”)</p><pre><code>@staticmethoddef _check_sign(token, timestamp, nonce, signature):    sign = [token, timestamp, nonce]    sign.sort()    sign = &quot;&quot;.join(sign)    sign = sha1(sign).hexdigest()    return sign == signature</code></pre><pre><code>类方法的使用场景有：作为工厂方法创建实例对象，例如内置模块 datetime.date 类中就有大量使用类方法作为工厂方法，以此来创建date对象。```pythonclass date:    def __new__(cls, year, month=None, day=None):        self = object.__new__(cls)        self._year = year        self._month = month        self._day = day        return self    @classmethod    def fromtimestamp(cls, t):        y, m, d, hh, mm, ss, weekday, jday, dst = _time.localtime(t)        return cls(y, m, d)    @classmethod    def today(cls):        t = _time.time()        return cls.fromtimestamp(t)</code></pre><p>如果希望在方法裡面调用静态类，那么把方法定义成类方法是合适的，因为要是定义成静态方法，那么你就要显示地引用类A，这对继承来说可不是一件好事情。<br>如果在@staticmethod中要调用到这个类的一些属性方法，只能直接<code>类名.属性名</code>或<code>类名.方法名</code>。而@classmethod因为持有cls参数，可以来调用类的属性，类的方法，实例化对象等，避免硬编码。</p><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">A</span><span class="token punctuation">:</span>    @staticmethod    <span class="token keyword">def</span> <span class="token function">m1</span><span class="token punctuation">(</span><span class="token punctuation">)</span>        <span class="token keyword">pass</span>    @staticmethod    <span class="token keyword">def</span> <span class="token function">m2</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>        A<span class="token punctuation">.</span>m1<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token comment" spellcheck="true"># bad</span>    @classmethod    <span class="token keyword">def</span> <span class="token function">m3</span><span class="token punctuation">(</span>cls<span class="token punctuation">)</span><span class="token punctuation">:</span>        cls<span class="token punctuation">.</span>m1<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token comment" spellcheck="true"># good</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>在上述的例子中，有的返回的是 function 类型，有的返回的是 method 类型，function类型是函数，method类型是方法。他们的主要区别在于，函数的传参都是显式传递的；而方法传参往往都会有隐式传递的，具体根据于调用方。隐式传递 self 或者cls数据。</p><ul><li><strong>@staticmethod 的效果是让类对象与实例对象的调用都返回函数。</strong></li><li><strong>@staticmethod不需要表示自身对象的self和自身类的cls参数，就跟使用函数一样。</strong></li><li><strong>@classmethod 则是要让类对象与实例对象的调用都返回方法，并且传递隐式参数 cls</strong></li><li><strong>@classmethod的第一个参数需要是表示自身类的cls参数。</strong></li></ul><p>classmethod主要用途是作为构造函数：</p><ul><li>Python只有一个构造函数<strong>new</strong>，如果想要多种构造函数就很不方便。只能在new里面写一堆if isinstance 。有classmethod之后就可以用classmethod来写不同的构造函数。</li></ul><p>staticmethod主要用途是限定Namespace：</p><ul><li>也就是说这个函数虽然是个普通的function，但是它只有这个class会用到，不适合作为module level的function，这时候就把它作为staticmethod。如果不考虑namespace的问题的话直接在module里面def function就行了。</li></ul><p>staticmethod 和 classmethod 都运用了描述符的机制，学习描述符不仅能提供接触到更多工具集的方法，还能更深地理解 Python 工作的原理并更加体会到其设计的优雅性。</p>]]></content>
      
      
      <categories>
          
          <category> Python </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 基础知识 </tag>
            
            <tag> Python </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Python的装饰器</title>
      <link href="/2023/02/17/python-zhuang-shi-qi/"/>
      <url>/2023/02/17/python-zhuang-shi-qi/</url>
      
        <content type="html"><![CDATA[<h1 id="装饰器是什么"><a href="#装饰器是什么" class="headerlink" title="装饰器是什么"></a>装饰器是什么</h1><p>装饰器是Python语法糖，装饰器是可调用的对象，可以像常规的可调用对象那样调用，特殊的地方是装饰器的参数是一个函数。<br>装饰器的使用场景：</p><ol><li>增强被装饰函数的行为</li><li>提高代码复用</li></ol><p>一个良好的装饰器必须要遵守<strong>两个原则</strong>：</p><ul><li><p>不能修改被装饰函数的代码</p></li><li><p>不能修改被装饰函数的调用方式</p><h1 id="装饰器理解基础"><a href="#装饰器理解基础" class="headerlink" title="装饰器理解基础"></a>装饰器理解基础</h1><p>如果想要很好的理解装饰器，那下面的两个内容需要先有所认知。</p></li><li><p>函数名可以赋值给变量</p></li><li><p>高阶函数</p><h2 id="函数名可以赋值给变量"><a href="#函数名可以赋值给变量" class="headerlink" title="函数名可以赋值给变量"></a>函数名可以赋值给变量</h2><p>看下这个例子：</p><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">func</span><span class="token punctuation">(</span>name<span class="token punctuation">)</span><span class="token punctuation">:</span>  <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'我是{}！慌的一逼！'</span><span class="token punctuation">.</span>format<span class="token punctuation">(</span>name<span class="token punctuation">)</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre></li></ul><p>func(‘梅西’)<br>y = func<br>y(‘勒夫’)</p><blockquote><blockquote><blockquote><p>Output:</p></blockquote></blockquote></blockquote><h1 id="我是梅西！慌的一逼！"><a href="#我是梅西！慌的一逼！" class="headerlink" title="我是梅西！慌的一逼！"></a>我是梅西！慌的一逼！</h1><h1 id="我是勒夫！慌的一逼！"><a href="#我是勒夫！慌的一逼！" class="headerlink" title="我是勒夫！慌的一逼！"></a>我是勒夫！慌的一逼！</h1><pre><code>代码中我们首先定义了函数`func`，并调用了`func`函数，并且把`func`赋值给y。`y = func`表明了：**函数名可以赋值给变量，并且不影响调用**。## 高阶函数高阶函数满足如下两个条件中的任意一个：1. 可以接收函数名作为实参；2. 返回值中可以包含函数名。在 Python 标准库中的 map 和 filter 等函数就是高阶函数---函数名作为参数。```pythonl = [1, 2, 4]r = map(lambda x: x*3, l)for i in r:    print(&#39;当前天台人数：&#39;, i)&gt;&gt;&gt; Output:# 当前天台人数： 3# 当前天台人数： 6# 当前天台人数： 12</code></pre><p>自定义一个能返回函数的函数，也是高阶函数：</p><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">f</span><span class="token punctuation">(</span>l<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">return</span> map<span class="token punctuation">(</span><span class="token keyword">lambda</span> x<span class="token punctuation">:</span> x<span class="token operator">*</span><span class="token number">5</span><span class="token punctuation">,</span> l<span class="token punctuation">)</span>a <span class="token operator">=</span> f<span class="token punctuation">(</span>l<span class="token punctuation">)</span><span class="token keyword">for</span> i <span class="token keyword">in</span> a<span class="token punctuation">:</span>    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'当前天台人数：'</span><span class="token punctuation">,</span> i<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><h1 id="实现一个类似的装饰器"><a href="#实现一个类似的装饰器" class="headerlink" title="实现一个类似的装饰器"></a>实现一个类似的装饰器</h1><p>现在已经知道了<strong>函数名赋值</strong>和<strong>高阶函数</strong>，有了这两个基础，就可以尝试实现一个类似的装饰器。</p><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">status</span><span class="token punctuation">(</span>func<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'慌的一逼！'</span><span class="token punctuation">)</span>    <span class="token keyword">return</span> func<span class="token keyword">def</span> <span class="token function">name</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'我是梅西！'</span><span class="token punctuation">)</span>temp <span class="token operator">=</span> status<span class="token punctuation">(</span>name<span class="token punctuation">)</span>temp<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token operator">>></span><span class="token operator">></span> Output<span class="token punctuation">:</span><span class="token comment" spellcheck="true"># 慌的一逼！</span><span class="token comment" spellcheck="true"># 我是梅西！</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>在这个例子中我们定义了一个 status 函数，status 接收一个函数名然后直接返回该函数名。这样我们实现了不修改原函数 name，并且添加了一个新功能的需求。但是这里有个<strong>缺陷就是函数的调用方式改变了</strong>。即不是原本的 name，而是 temp。<br>要解决这个问题很简单，相信 a = a<em>3 这样的表达式大家都见过，那么上述代码中的 temp = status(name) 同样可以修改为 name = status(name)，这样我们就完美的解决了问题：*</em>既添加新功能又没有修改原函数和其调用方式**。修改后的代码如下：</p><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">status</span><span class="token punctuation">(</span>func<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'慌的一逼！'</span><span class="token punctuation">)</span>    <span class="token keyword">return</span> func<span class="token keyword">def</span> <span class="token function">name</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'我是梅西！'</span><span class="token punctuation">)</span>name <span class="token operator">=</span> status<span class="token punctuation">(</span>name<span class="token punctuation">)</span>name<span class="token punctuation">(</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>但这样的代码却有个不便之处，即每次使用这样的装饰器，我们都要写类似 name = status(name) 的代码。在 python 中为了简化这种情况，提供了一个语法糖 @，<strong>在每个被装饰的函数上方使用这个语法糖</strong>就可以省掉这一句代码 name = status(name)，最后的代码如下：</p><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">status</span><span class="token punctuation">(</span>func<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'慌的一逼！'</span><span class="token punctuation">)</span>    <span class="token keyword">return</span> func@status<span class="token keyword">def</span> <span class="token function">name</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'我是梅西！'</span><span class="token punctuation">)</span>name<span class="token punctuation">(</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>这样我们就弄清楚了装饰器的工作原理：</p><ul><li><strong>写一个高阶函数</strong>，即参数是函数，返回的也是函数。</li><li>在<strong>利用语法糖@，简化赋值操作</strong>。<h1 id="装饰器进阶"><a href="#装饰器进阶" class="headerlink" title="装饰器进阶"></a>装饰器进阶</h1>能够处理返回值的装饰器：<pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">guess_win</span><span class="token punctuation">(</span>func<span class="token punctuation">)</span><span class="token punctuation">:</span>  <span class="token keyword">def</span> <span class="token function">rooftop_status</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>      result <span class="token operator">=</span> func<span class="token punctuation">(</span><span class="token punctuation">)</span>      <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'天台已满，请排队！'</span><span class="token punctuation">)</span>      <span class="token keyword">return</span> result  <span class="token keyword">return</span> rooftop_status<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></li></ul><p>@guess_win<br>def german_team():<br>    print(‘德国必胜！’)<br>    return ‘赢了会所嫩模！输了下海干活！’</p><p>x = german_team()<br>print(x)</p><blockquote><blockquote><blockquote><p>Output:</p></blockquote></blockquote></blockquote><h1 id="德国必胜！"><a href="#德国必胜！" class="headerlink" title="德国必胜！"></a>德国必胜！</h1><h1 id="天台已满，请排队！"><a href="#天台已满，请排队！" class="headerlink" title="天台已满，请排队！"></a>天台已满，请排队！</h1><h1 id="赢了会所嫩模！输了下海干活！"><a href="#赢了会所嫩模！输了下海干活！" class="headerlink" title="赢了会所嫩模！输了下海干活！"></a>赢了会所嫩模！输了下海干活！</h1><pre><code>能够处理参数的装饰器：```pythondef guess_win(func):    def rooftop_status(*args, **kwargs):        result = func(*args, **kwargs)        print(&#39;天台已满，请排队！&#39;)        return result    return rooftop_status@guess_windef german_team(arg):    print(&#39;{}必胜！&#39;.format(arg))    return &#39;赢了会所嫩模！输了下海干活！&#39;x = german_team(&#39;德国&#39;)y = german_team(&#39;西班牙&#39;)print(x)&gt;&gt;&gt; Output:# 德国必胜！# 天台已满，请排队！# 西班牙必胜！# 天台已满，请排队！# 赢了会所嫩模！输了下海干活！</code></pre><h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>装饰器的<strong>本质是函数</strong>，<strong>其参数是另一个函数（被装饰的函数）</strong>。装饰器通常会额外处理被装饰的函数，然后把它返回，或者将其替换成另一个函数或可调用对象。行为良好的装饰器可以重用，以减少代码量。<br>代码运行的理解通式：</p><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">xxx</span><span class="token punctuation">(</span>func<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">return</span> func@xxx<span class="token keyword">def</span> <span class="token function">yyy</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">pass</span>yyy<span class="token punctuation">(</span><span class="token punctuation">)</span>等价于下面的方式yyy <span class="token operator">=</span> xxx<span class="token punctuation">(</span>yyy<span class="token punctuation">)</span>yyy<span class="token punctuation">(</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>]]></content>
      
      
      <categories>
          
          <category> Python </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 基础知识 </tag>
            
            <tag> Python </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>深度学习的优化算法</title>
      <link href="/2023/02/15/shen-du-xue-xi-you-hua-suan-fa/"/>
      <url>/2023/02/15/shen-du-xue-xi-you-hua-suan-fa/</url>
      
        <content type="html"><![CDATA[<p>炼丹师的日常：拿来药材（数据），架起八卦炉（模型），点着六味真火（优化算法），就摇着蒲扇等着丹药出炉了。不过，同样的食材，同样的菜谱，但火候不一样了，这出来的口味可是千差万别。火小了夹生，火大了易糊，火不匀则半生半糊。</p><h1 id="回顾优化算法"><a href="#回顾优化算法" class="headerlink" title="回顾优化算法"></a>回顾优化算法</h1><p>深度学习优化算法经历了 SGD -&gt; SGDM -&gt; NAG -&gt;AdaGrad -&gt; AdaDelta -&gt; Adam -&gt; Nadam 这样的发展历程。<br>首先定义待优化参数$w$，目标函数$f(w)$，初始学习率$\alpha$。<br>开始进行迭代优化，在每个 epoch $t$ 中：</p><ol><li>计算目标函数关于当前参数的梯度：$g_t=\frac{\partial f(w)}{\partial w}$</li><li>根据历史梯度计算一阶动量和二阶动量：$m_t=\phi(g_1, g_2, g_3, … , g_t)$；$V_t=\psi(g_1, g_2, g_3, … , g_t)$</li><li>计算当前时刻的下降梯度：$\eta_t=\alpha \times m_t / \sqrt[2]{V_t}$</li><li>根据下降梯度进行更新：$w_{t+1}=w_t- \eta_t$</li></ol><p>上面4个步骤就是优化算法的通式。</p><h2 id="SGD算法"><a href="#SGD算法" class="headerlink" title="SGD算法"></a>SGD算法</h2><p>SGD没有动量的概念，因此：<br>$m_t=g_t; V_t=I^2$<br>代入步骤3中，可以看到下降梯度就是最简单的<br>$\eta_t=\alpha \cdot g_t$<br>SGD最大的缺点是下降速度慢，而且可能会在沟壑的两边持续震荡，停留在一个局部最优点。</p><h2 id="SGDM算法"><a href="#SGDM算法" class="headerlink" title="SGDM算法"></a>SGDM算法</h2><p>SGDM全称是SGD with Momentum，在SGD基础上引入了一阶动量。为了抑制SGD的震荡，SGDM认为梯度下降过程可以加入惯性。下坡的时候，如果发现是陡坡，那就利用惯性跑的快一些。引入的一阶动量为：<br>$m_t=\beta_1 \cdot m_{t-1} + (1 - \beta_1) \cdot g_t$<br>一阶动量是各个时刻梯度方向的指数移动平均值，也即梯度的动量更新。约等于最近$1 / (1-\beta_1)$个时刻的梯度向量和的平均值。<br>也就是说，$t$时刻的下降方向，不仅由当前点的梯度方向决定，而且由此前累积的下降方向决定。$\beta_1$的经验值为0.9，这就意味着下降方向主要是此前累积的下降方向，并略微偏向当前时刻的下降方向。想象高速公路上汽车转弯，在高速向前的同时略微偏向，急转弯可是要出事的。</p><h2 id="NGA算法"><a href="#NGA算法" class="headerlink" title="NGA算法"></a>NGA算法</h2><p>SGD 还有一个问题是困在局部最优的沟壑里面震荡。想象一下你走到一个盆地，四周都是略高的小山，你觉得没有下坡的方向，那就只能待在这里了。可是如果你爬上高地，就会发现外面的世界还很广阔。因此，我们不能停留在当前位置去观察未来的方向，而要向前一步、多看一步、看远一些。<br>NAG全称Nesterov Accelerated Gradient，是在SGD、SGDM的基础上的进一步改进，改进点在于步骤1。我们知道在时刻$t$的主要下降方向是由累积动量决定的，自己的梯度方向说了也不算，那与其看当前梯度方向，不如先看看如果跟着累积动量走了一步，那个时候再怎么走。因此，NAG在步骤1，<strong>不计算当前位置的梯度方向，而是计算如果按照累积动量走了一步，那个时候的下降方向</strong>：<br>$g_t=\nabla f(w_t - \alpha \cdot m_{t-1} / \sqrt{V_{t-1}})$<br>然后用下一个点的梯度方向，与历史累积动量相结合，计算步骤2中当前时刻的累积动量。</p><h2 id="AdaGrad算法"><a href="#AdaGrad算法" class="headerlink" title="AdaGrad算法"></a>AdaGrad算法</h2><p>二阶动量的出现，才意味着自适应学习优化算法时代的到来。<br>SGD及其变种以同样的学习率更新每个参数，但深度神经网络往往包含大量的参数，这些参数并不是总会用得到（想想大规模的embedding）。<strong>对于经常更新的参数，我们已经积累了大量关于它的知识，不希望被单个样本影响太大，希望学习速率慢一些；对于偶尔更新的参数，我们了解的信息太少，希望能从每个偶然出现的样本身上多学一些，即学习速率大一些</strong>。<br>怎么样去度量历史更新频率呢？那就是二阶动量——该维度上，迄今为止所有梯度值的平方和：<br>$V_t=\sum_{\tau =1}^{t} g_{\tau}^2$<br>步骤3中下降的梯度为：<br>$\eta_t=\alpha \times m_t / \sqrt{V_t}$<br>可以看出，此时实质上的学习率由$\alpha$变成了$\alpha / \sqrt{V_t}$。一般为了避免分母为0，会在分母上加一个小的平滑项。因此$\sqrt{V_t}$是恒大于0的，而且参数更新越频繁，二阶动量越大，学习率就越小。<br>这一方法在稀疏数据场景下表现非常好。但也存在一些问题：因为$\sqrt{V_t}$是单调递增的，会使得学习率单调递减至0，可能会使得训练过程提前结束，即便后续还有数据也无法学到必要的知识。</p><h2 id="AdaDelta-RMSProp算法"><a href="#AdaDelta-RMSProp算法" class="headerlink" title="AdaDelta/RMSProp算法"></a>AdaDelta/RMSProp算法</h2><p>由于AdaGrad单调递减的学习率变化过于激进，我们考虑一个改变二阶动量计算方法的策略：不累积全部历史梯度，而只关注过去一段时间窗口的下降梯度。这也就是AdaDelta名称中Delta的来历。<br>修改的思路很简单。前面我们讲到，指数移动平均值大约就是过去一段时间的平均值，因此我们用这一方法来计算二阶累积动量：<br>$V_t=\beta_2 \cdot V_{t-1} + (1- \beta_2)g_t^2$<br>这就避免了二阶动量持续累积、导致训练过程提前结束的问题了。</p><h2 id="Adam算法"><a href="#Adam算法" class="headerlink" title="Adam算法"></a>Adam算法</h2><p>Adam是前述方法的集大成者。我们看到，SGDM在SGD基础上增加了一阶动量，AdaGrad和AdaDelta在SGD基础上增加了二阶动量。把一阶动量和二阶动量都用起来，就是Adam了——Adaptive + Momentum。<br>SGD的一阶动量：<br>$$m_t=\beta_1 \cdot m_{t-1} + (1 - \beta_1) \cdot g_t$$<br>加上AdaDelta的二阶动量：<br>$V_t=\beta_2 \cdot V_{t-1} + (1- \beta_2)g_t^2$<br>优化算法里最常见的两个超参数$\beta_1$和$\beta_2$都在这里了，前者控制一阶动量，后者控制二阶动量。</p><h2 id="Nadam算法"><a href="#Nadam算法" class="headerlink" title="Nadam算法"></a>Nadam算法</h2><p>我们说Adam是集大成者，但它居然遗漏了Nesterov，这还能忍？必须给它加上，按照NAG的步骤1：<br>$g_t=\nabla f(w_t - \alpha \cdot m_{t-1} / \sqrt{V_{t-1}})$<br>这就是Nesterov + Adam = Nadam了。</p><h1 id="指数移动平均值的偏差修正"><a href="#指数移动平均值的偏差修正" class="headerlink" title="指数移动平均值的偏差修正"></a>指数移动平均值的偏差修正</h1><p>一阶动量和二阶动量都是按照指数移动平均值进行计算的：<br>$m_t=\beta_1 \cdot m_{t-1} + (1 - \beta_1) \cdot g_t$<br>$V_t=\beta_2 \cdot V_{t-1} + (1- \beta_2)g_t^2$<br>参数的经验值是$\beta_1=0.9$；$\beta_2=0.999$。初始化：$m_0=0$；$V_0=0$。<br>这个时候我们看到，在初期$m_t$和$V_t$都会接近于0，这个估计是有问题的。因此我们常常根据下式进行误差修正：<br>$\widetilde{m_t}=m_t/(1-\beta_1^t)$<br>$\widetilde{V_t}=V_t/(1-\beta_2^t)$</p><h1 id="Adam-SGD-组合策略"><a href="#Adam-SGD-组合策略" class="headerlink" title="Adam+SGD 组合策略"></a>Adam+SGD 组合策略</h1><p>不同优化算法的优劣依然是未有定论的争议话题。主流的观点认为：Adam等自适应学习率算法对于稀疏数据具有优势，且收敛速度很快；但精调参数的SGD（+Momentum）往往能够取得更好的最终结果。<br>可不可以把这两者结合起来，<strong>先用Adam快速下降，再用SGD调优</strong>，一举两得？思路简单，但里面有两个技术问题：</p><ol><li><strong>什么时候切换优化算法？</strong>——如果切换太晚，Adam可能已经跑到自己的盆地里去了，SGD再怎么好也跑不出来了。</li><li><strong>切换算法以后用什么样的学习率？</strong>——Adam用的是自适应学习率，依赖的是二阶动量的累积，SGD接着训练的话，用什么样的学习率？<h2 id="切换之后用什么样的学习率"><a href="#切换之后用什么样的学习率" class="headerlink" title="切换之后用什么样的学习率"></a><strong>切换之后用什么样的学习率</strong></h2>Adam的下降方向是$\eta_t^{Adam}=(\alpha / \sqrt{V_t}) \cdot m_t$，SGD的下降方向是$\eta_t^{SGD}=\alpha^{SGD} \cdot g_t$。<br>$\eta_t^{SGD}$必定可以分解为$\eta_t^{Adam}$所在方向及其正交方向上的两个方向之和，那么其在$\eta_t^{Adam}$方向上的投影就意味着SGD在Adam算法决定的下降方向上前进的距离，而在$\eta_t^{Adam}$的正交方向上的投影是 SGD 在自己选择的修正方向上前进的距离。这里p为Adam下降方向，g为梯度方向，γ为SGD的学习率。<br><img src="./imgs/yuque_mind.jpeg" alt><br>如果SGD要走完Adam未走完的路，那就首先要接过Adam的大旗——沿着$\eta_t^{Adam}$方向走一步，而后在沿着其正交方向走相应的一步。这样我们就知道该如何确定SGD的步长（学习率）了——<strong>SGD在Adam下降方向上的正交投影，应该正好等于Adam的下降方向（含步长）</strong>。也即：<br>$proj_{\eta_t^{SGD}}=\eta_t^{Adam}$<br>解这个方程，我们就可以得到接续进行SGD的学习率：<br>$\alpha_t^{SGD}=((\eta_t^{Adam})^T \eta_t^{Adam}) / ((\eta_t^{Adam})^T g_t)$<br>为了减少噪声影响，作者使用移动平均值来修正对学习率的估计：<br>$\lambda_t^{SGD}=\beta_2 \cdot \lambda_{t-1}^{SGD}+(1-\beta_2) \cdot \alpha_t^{SGD}$<br>$\widetilde{\lambda_t^{SGD}}=\lambda_t^{SGD}/(1-\beta_2^t)$<br>这里直接复用了Adam的$\beta_2$参数。</li></ol><h2 id="何时进行算法的切换"><a href="#何时进行算法的切换" class="headerlink" title="何时进行算法的切换"></a><strong>何时进行算法的切换</strong></h2><p>当 SGD的相应学习率的移动平均值基本不变的时候，即：<br>$|\widetilde{\lambda_t^{SGD}}-\alpha_t^{SGD}|&lt; \epsilon$<br>每次迭代完都计算一下SGD接班人的相应学习率，如果发现基本稳定了，那就SGD以$\widetilde{\lambda_t^{SGD}}$为学习率接班前进。</p><h1 id="优化算法的常用tricks"><a href="#优化算法的常用tricks" class="headerlink" title="优化算法的常用tricks"></a>优化算法的常用tricks</h1><ol><li>首先，各大算法孰优孰劣并无定论。如果是刚入门，<strong>优先考虑 SGD+Nesterov Momentum或者Adam。</strong>（Standford 231n : _The two recommended updates to use are either SGD+Nesterov Momentum or Adam_）</li><li><strong>选择你熟悉的算法</strong>——这样你可以更加熟练地利用你的经验进行调参。</li><li><strong>充分了解你的数据</strong>——如果模型是非常稀疏的，那么优先考虑自适应学习率的算法。</li><li><strong>根据你的需求来选择</strong>——在模型设计实验过程中，要快速验证新模型的效果，可以先用Adam进行快速实验优化；在模型上线或者结果发布前，可以用精调的SGD进行模型的极致优化。</li><li><strong>先用小数据集进行实验。</strong> 有论文研究指出，随机梯度下降算法的收敛速度和数据集的大小的关系不大。（_The mathematics of stochastic gradient descent are amazingly independent of the training set size. In particular, the asymptotic SGD convergence rates are independent from the sample size_）因此可以先用一个具有代表性的小数据集进行实验，测试一下最好的优化算法，并通过参数搜索来寻找最优的训练参数。</li><li><strong>考虑不同算法的组合。</strong> 先用Adam进行快速下降，而后再换到SGD进行充分的调优。切换策略可以参考本文介绍的方法。</li><li><strong>数据集一定要充分的打散（shuffle）。</strong> 这样在使用自适应学习率算法的时候，可以避免某些特征集中出现，而导致的有时学习过度、有时学习不足，使得下降方向出现偏差的问题。</li><li>训练过程中<strong>持续监控训练数据和验证数据</strong>上的目标函数值以及精度或者AUC等指标的变化情况。对训练数据的监控是要保证模型进行了充分的训练——下降方向正确，且学习率足够高；对验证数据的监控是为了避免出现过拟合。</li><li><strong>制定一个合适的学习率衰减策略。</strong> 可以使用定期衰减策略，比如每过多少个epoch就衰减一次；或者利用精度或者AUC等性能指标来监控，当测试集上的指标不变或者下跌时，就降低学习率。</li></ol>]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 基础知识 </tag>
            
            <tag> 优化算法 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hexo项目部署|PUSH到Github有各种问题</title>
      <link href="/2022/12/05/ji-lu-yi-ci-hexo-xiang-mu-bu-shu-de-debug-guo-cheng/"/>
      <url>/2022/12/05/ji-lu-yi-ci-hexo-xiang-mu-bu-shu-de-debug-guo-cheng/</url>
      
        <content type="html"><![CDATA[<blockquote><p>关注公众号【惜学塔】，每日知识干货马上就来！</p></blockquote><p><img src="/medias/contact.jpg" alt></p><h1 id="正常部署方式"><a href="#正常部署方式" class="headerlink" title="正常部署方式"></a>正常部署方式</h1><p>当github仓库、git、Hexo准备就绪，想把自己的内容推送到Github上时，通过GitHub网址 ，<a href="https://jlcxxzj.github.io/" target="_blank" rel="noopener">可见jlcxxzj</a>,就可以愉快的访问时，一切都是那么简单，只需要：</p><h2 id="一、建立本地项目和Github的连接"><a href="#一、建立本地项目和Github的连接" class="headerlink" title="一、建立本地项目和Github的连接"></a>一、建立本地项目和Github的连接</h2><ol><li>打开Hexo项目根目录，右键打开git bash，然后输入自己的github账号信息命令：<pre class="line-numbers language-bash"><code class="language-bash"> <span class="token function">git</span> config --global user.name <span class="token string">"jlcxxzj"</span> <span class="token function">git</span> config --global user.email <span class="token string">"2541597473@qq.com"</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre></li><li>没有报错就说明用户信息是正确的，如果报错去Github设置看看自己的用户名和邮箱；接下来生成密钥SSH key，首先是确定要生成密钥的账户：<pre class="line-numbers language-bash"><code class="language-bash"> ssh-keygen -t rsa -C <span class="token string">"2541597473@qq.com"</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre> 继续输入下面命令，会产生一个密钥，也可在本地的用户根目录下<strong>.ssh</strong>文件夹找到产生的密钥：<pre class="line-numbers language-bash"><code class="language-bash"> <span class="token function">cat</span> ~/.ssh/id_rsa.pub<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre></li><li>生成密钥成功以后，打开<a href="https://github.com/jlcxxzj">github</a>，在右上角头像下面点击<code>settings</code>，再点击<code>SSH and GPG keys</code>，新建一个SSH，名字任意，可按项目名称来，将第2步产生的密钥复制到框中，点击确定保存。</li><li>在第1步右键打开的git bash下输入<code>ssh -T git@github.com</code>，如果没有报错，出现你的用户名，那就成功了。<h1 id="内网穿透"><a href="#内网穿透" class="headerlink" title="内网穿透"></a>内网穿透</h1>由于本机属于内网，互联网是没有办法直接访问的，因此需要使用内网穿透来使得其他设备可以访问到<br><a href="https://cloud.tencent.com/developer/article/2126247" target="_blank" rel="noopener">https://cloud.tencent.com/developer/article/2126247</a><br>主机IP可通过CMD输入ipconfig或者ipconfig/all来查看<br>选择https方式<h2 id="二、将自己的Hexo博客上传到github页面，不用搭服务器或者内网穿透就能远程访问"><a href="#二、将自己的Hexo博客上传到github页面，不用搭服务器或者内网穿透就能远程访问" class="headerlink" title="二、将自己的Hexo博客上传到github页面，不用搭服务器或者内网穿透就能远程访问"></a>二、将自己的Hexo博客上传到github页面，不用搭服务器或者内网穿透就能远程访问</h2></li><li>首先修改一下Hexo博客更目录下的_config.yml配置文件(注意，不是主题下面的_config.yml),修改一下部署的配置信息，repository修改为自己的github项目地址：<pre class="line-numbers language-bash"><code class="language-bash"> deploy:     type: <span class="token function">git</span>     repository: https://github.com/jlcxxzj/jlcxxzj.github.io.git     branch: master<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre></li><li>正常情况下以下3步就可以部署到github页面访问1. <code>hexo clean</code>、<code>hexo g</code>、<code>hexo d</code></li><li>github访问地址 <a href="https://github.com/jlcxxzj/jlcxxzj.github.io.git，在具体的项目设置中，可以根本更改默认分支，访问的话也是访问默认的分支">https://github.com/jlcxxzj/jlcxxzj.github.io.git，在具体的项目设置中，可以根本更改默认分支，访问的话也是访问默认的分支</a></li><li>也可以本地访问， <code>hexo clean</code>、<code>hexo g</code>、<code>hexo s</code>，默认4000端口可访问，<a href="http://localhost:4000" target="_blank" rel="noopener">http://localhost:4000</a></li></ol><h1 id="各种意外情况"><a href="#各种意外情况" class="headerlink" title="各种意外情况"></a>各种意外情况</h1><h2 id="部署出现错误err-Error-Spawn-failed"><a href="#部署出现错误err-Error-Spawn-failed" class="headerlink" title="部署出现错误err: Error: Spawn failed"></a>部署出现错误err: Error: Spawn failed</h2><pre class="line-numbers language-bash"><code class="language-bash"><span class="token comment" spellcheck="true">##进入站点根目录</span><span class="token function">cd</span> /usr/local/src/hexo/hanyubolg/<span class="token comment" spellcheck="true">##删除git提交内容文件夹</span><span class="token function">rm</span> -rf .deploy_git/<span class="token comment" spellcheck="true">##执行</span><span class="token function">git</span> config --global core.autocrlf <span class="token boolean">false</span><span class="token comment" spellcheck="true">##最后</span>hexo clean <span class="token operator">&amp;&amp;</span> hexo g <span class="token operator">&amp;&amp;</span> hexo d<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="Git出错：error-RPC-failed-curl-56-OpenSSL-SSL-read-Connection-was-reset-errno-10054"><a href="#Git出错：error-RPC-failed-curl-56-OpenSSL-SSL-read-Connection-was-reset-errno-10054" class="headerlink" title="Git出错：error: RPC failed; curl 56 OpenSSL SSL_read: Connection was reset, errno 10054"></a>Git出错：error: RPC failed; curl 56 OpenSSL SSL_read: Connection was reset, errno 10054</h2><p>出现这种原因的可能有好几种</p><h3 id="fatal-The-remote-end-hung-up-unexpectedly"><a href="#fatal-The-remote-end-hung-up-unexpectedly" class="headerlink" title="fatal: The remote end hung up unexpectedly"></a>fatal: The remote end hung up unexpectedly</h3><p><img src="https://upload-images.jianshu.io/upload_images/20074990-37e315ab227b135e.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/566/format/webp" alt></p><ol><li>整体文件太大，修改文件大小的上限：<br><code>git config --global http.postBuffer 524288000</code><br>也可在项目中的.git文件夹，直接修改；通过下面命令查看是否修改成功：<br><code>git config --list</code></li><li>单个文件<strong>超过100M</strong>是不能上传的，部署的时候需要注意<h3 id="error-failed-to-push-some-refs-to-git"><a href="#error-failed-to-push-some-refs-to-git" class="headerlink" title="error: failed to push some refs to *.git"></a>error: failed to push some refs to *.git</h3>之前这种操作属于常规操作，没想到这次出了问题，感觉问题应该出在了工程创建方式上。我在 git 后台创建工程时勾选了自动添加 README.md 文件，可能导致了后续一系列问题。</li></ol><h2 id="部署的时候一直卡着不动的原因"><a href="#部署的时候一直卡着不动的原因" class="headerlink" title="部署的时候一直卡着不动的原因"></a>部署的时候一直卡着不动的原因</h2><p><a href="https://blog.csdn.net/qq_25333681/article/details/80879500" target="_blank" rel="noopener">https://blog.csdn.net/qq_25333681/article/details/80879500</a></p><h2 id="需要修改展示的branch分支页面时"><a href="#需要修改展示的branch分支页面时" class="headerlink" title="需要修改展示的branch分支页面时"></a>需要修改展示的branch分支页面时</h2><p>点击settings，在Pages的 <strong>Build and deployment</strong>修改<strong>Branch</strong>，需要等一会重新输入<a href="https://jlcxxzj.github.io/" target="_blank" rel="noopener">项目地址</a>，就能访问分支下的页面了。<br><img src="%E8%AE%B0%E5%BD%95%E4%B8%80%E6%AC%A1hexo%E9%A1%B9%E7%9B%AE%E9%83%A8%E7%BD%B2%E7%9A%84Debug%E8%BF%87%E7%A8%8B_md_files/00d68c60-745b-11ed-aa4d-fdeb71e777c3.jpeg?v=1&type=image" alt></p><h2 id="Debug链接"><a href="#Debug链接" class="headerlink" title="Debug链接"></a>Debug链接</h2><p><a href="https://blog.csdn.net/u013250071/article/details/81203900" target="_blank" rel="noopener">修改.git文件夹</a></p>]]></content>
      
      
      <categories>
          
          <category> Debug </category>
          
      </categories>
      
      
        <tags>
            
            <tag> github项目部署 </tag>
            
            <tag> hexo博客 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>华山论剑 | 论坛下畅所欲言</title>
      <link href="/2022/11/30/lun-tan-xiang-mu/"/>
      <url>/2022/11/30/lun-tan-xiang-mu/</url>
      
        <content type="html"><![CDATA[<blockquote><p>关注公众号【惜学塔】，每日知识干货马上就来！</p></blockquote><p><img src="/medias/contact.jpg" alt></p><h1 id="本地项目地址"><a href="#本地项目地址" class="headerlink" title="本地项目地址"></a>本地项目地址</h1><p>本地地址：D:\javafile\Austin\预研项目\开源论坛<br>开源论坛2还没有调试</p>]]></content>
      
      
      <categories>
          
          <category> 项目 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Java </tag>
            
            <tag> 后端 </tag>
            
            <tag> Bootstrap </tag>
            
            <tag> 论坛 </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
